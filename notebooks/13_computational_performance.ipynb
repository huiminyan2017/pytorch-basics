{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3cb551d",
   "metadata": {},
   "source": [
    "========================================Compilers and Interpreters=================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_():\n",
    "    return '''\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "'''\n",
    "\n",
    "def fancy_func_():\n",
    "    return '''\n",
    "def fancy_func(a, b, c, d):\n",
    "    e = add(a, b)\n",
    "    f = add(c, d)\n",
    "    g = add(e, f)\n",
    "    return g\n",
    "'''\n",
    "\n",
    "def evoke_():\n",
    "    return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'\n",
    "\n",
    "prog = evoke_()\n",
    "print(prog)\n",
    "y = compile(prog, '', 'exec')\n",
    "exec(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b7035",
   "metadata": {},
   "source": [
    "##### imperative (interpreted) programming and symbolic programming:\n",
    "\n",
    "Imperative programming is easier. When imperative programming is used in Python, the majority of the code is straightforward and easy to write. It is also easier to debug imperative programming code. This is because it is easier to obtain and print all relevant intermediate variable values, or use Python‚Äôs built-in debugging tools.\n",
    "\n",
    "Symbolic programming is more efficient and easier to port. Symbolic programming makes it easier to optimize the code during compilation, while also having the ability to port the program into a format independent of Python. This allows the program to be run in a non-Python environment, thus avoiding any potential performance issues related to the Python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5878a13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0645,  0.0214]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "# Factory for networks\n",
    "def get_net():\n",
    "    net = nn.Sequential(nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2))\n",
    "    return net\n",
    "\n",
    "x = torch.randn(size=(1, 512))\n",
    "net = get_net()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0874c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.jit.script(net)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f548d100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without torchscript: 0.7279 sec\n",
      "With torchscript: 0.6189 sec\n"
     ]
    }
   ],
   "source": [
    "#@save\n",
    "class Benchmark:\n",
    "    \"\"\"For measuring running time.\"\"\"\n",
    "    def __init__(self, description='Done'):\n",
    "        self.description = description\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.timer = d2l.Timer()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        print(f'{self.description}: {self.timer.stop():.4f} sec')\n",
    "\n",
    "net = get_net()\n",
    "with Benchmark('Without torchscript'):\n",
    "    for i in range(10000): net(x)\n",
    "\n",
    "net = torch.jit.script(net)\n",
    "with Benchmark('With torchscript'):\n",
    "    for i in range(10000): net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad719637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# One of the benefits of compiling the models is that we can serialize (save) the model and its parameters to disk. \n",
    "net.save('../data/my_mlp')\n",
    "!ls -lh my_mlp*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbdb5bc",
   "metadata": {},
   "source": [
    "======================================Asynchronous Computation=================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51924066",
   "metadata": {},
   "source": [
    " For PyTorch, by default, GPU operations are asynchronous. When you call a function that uses the GPU, the operations are enqueued to the particular device, but not necessarily executed until later. This allows us to execute more computations in parallel, including operations on the CPU or other GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4bd5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the first GPU operation, PyTorch triggers a bunch of one-time costs:\n",
    "# 1. CUDA context initialization\n",
    "#   Creating the CUDA context can take tens to hundreds of milliseconds\n",
    "# 2. Memory allocator setup\n",
    "#   GPU memory pools are initialized lazily\n",
    "# 3. Kernel loading / JIT compilation\n",
    "#   cuBLAS / CUDA kernels are loaded and sometimes compiled\n",
    "# 4. Driver synchronization\n",
    "#   The first call often forces a sync that later calls avoid\n",
    "# If you don‚Äôt warm up, the first timed iteration will include all of this overhead.\n",
    "\n",
    "# Below 3 lines are  Warmup for GPU computation, they ensure:\n",
    "# CUDA stream is created, Kernels are loaded and later operations are purely async compute\n",
    "device = d2l.try_gpu()\n",
    "a = torch.randn(size=(1000, 1000), device=device)\n",
    "b = torch.mm(a, a)\n",
    "\n",
    "with d2l.Benchmark('numpy'):\n",
    "    for _ in range(10):\n",
    "        a = numpy.random.normal(size=(1000, 1000))\n",
    "        b = numpy.dot(a, a)\n",
    "\n",
    "with d2l.Benchmark('torch'):\n",
    "    for _ in range(10):\n",
    "        a = torch.randn(size=(1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)\n",
    "\n",
    "with d2l.Benchmark():\n",
    "    for _ in range(10):\n",
    "        a = torch.randn(size=(1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)\n",
    "    torch.cuda.synchronize(device)  # Wait for all the computations to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((1, 2), device=device)\n",
    "y = torch.ones((1, 2), device=device)\n",
    "z = x * y + 2\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c6981",
   "metadata": {},
   "source": [
    "==========================================Automatic Parallelism======================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b4449",
   "metadata": {},
   "source": [
    "#### Computational Graphs and Automatic Parallelism\n",
    "\n",
    "Modern deep learning frameworks such as **PyTorch** and **MXNet** automatically build **computational graphs** that represent operations and their dependencies. These graphs make it clear which computations depend on others and which can run **in parallel**.\n",
    "\n",
    "By explicitly encoding dependencies, the framework can execute **independent operations simultaneously**, improving performance without manual coordination.\n",
    "\n",
    "##### Execution and Parallelism\n",
    "\n",
    "On a **single device**, most operators (e.g., matrix multiplication or convolution) already use **all available hardware resources**:\n",
    "- CPUs utilize all cores and threads\n",
    "- GPUs utilize all compute units\n",
    "\n",
    "Because of this, additional parallelism provides limited benefit on a single device. Significant speedups appear mainly in **multi-device setups**.\n",
    "\n",
    "##### Automatic Multi-Device Scaling\n",
    "\n",
    "When multiple devices are available, frameworks can automatically:\n",
    "- Distribute computations across GPUs and CPUs\n",
    "- Overlap computation with communication\n",
    "- Increase overall training throughput\n",
    "\n",
    "This requires little to no manual device management.\n",
    "\n",
    "##### Key Takeaways\n",
    "\n",
    "| Concept | Description |\n",
    "|-------|-------------|\n",
    "| **Computational Graph** | Encodes operations and dependencies |\n",
    "| **Automatic Parallelism** | Independent operations run concurrently |\n",
    "| **Single Device** | One operator already saturates hardware |\n",
    "| **Multi-Device** | Workload distribution enables major speedups |\n",
    "| **Developer Experience** | Simple code with optimized execution |\n",
    "\n",
    "##### Summary\n",
    "\n",
    "Deep learning frameworks rely on **computational graphs** to automatically schedule and parallelize computation across CPUs and GPUs. This allows developers to write **high-level, concise code** while still achieving **efficient hardware utilization**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10136096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need at least two GPUs to run the experiments in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = d2l.try_all_gpus()\n",
    "def run(x):\n",
    "    return [x.mm(x) for _ in range(50)]\n",
    "\n",
    "# two variables\n",
    "x_gpu1 = torch.rand(size=(4000, 4000), device=devices[0])\n",
    "x_gpu2 = torch.rand(size=(4000, 4000), device=devices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm up all devices first\n",
    "run(x_gpu1)\n",
    "run(x_gpu2)\n",
    "torch.cuda.synchronize(devices[0]) #  waits for all kernels in all streams on a CUDA device to complete. \n",
    "torch.cuda.synchronize(devices[1])\n",
    "\n",
    "with d2l.Benchmark('GPU1 time'):\n",
    "    run(x_gpu1)\n",
    "    torch.cuda.synchronize(devices[0])\n",
    "\n",
    "with d2l.Benchmark('GPU2 time'):\n",
    "    run(x_gpu2)\n",
    "    torch.cuda.synchronize(devices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we remove the synchronize statement between both tasks the system is free to parallelize computation on both devices automatically.\n",
    "\n",
    "with d2l.Benchmark('GPU1 & GPU2'):\n",
    "    run(x_gpu1)\n",
    "    run(x_gpu2)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d251329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In many cases we need to move data between different devices, say between the CPU and GPU, or between different GPUs. For instance, this occurs when we want to perform \n",
    "# distributed optimization where we need to aggregate the gradients over multiple accelerator cards. Let‚Äôs simulate this by computing on the GPU and then copying the results \n",
    "# back to the CPU.\n",
    "\n",
    "def copy_to_cpu(x, non_blocking=False):\n",
    "    return [y.to('cpu', non_blocking=non_blocking) for y in x]\n",
    "\n",
    "with d2l.Benchmark('Run on GPU1'):\n",
    "    y = run(x_gpu1)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "with d2l.Benchmark('Copy to CPU'):\n",
    "    y_cpu = copy_to_cpu(y)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is somewhat inefficient. Note that we could already start copying parts of y to the CPU while the remainder of the list is still being computed. This situation occurs, \n",
    "# e.g., when we compute the (backprop) gradient on a minibatch. The gradients of some of the parameters will be available earlier than that of others. Hence it works to our\n",
    "# advantage to start using PCI-Express bus bandwidth while the GPU is still running. In PyTorch, several functions such as to() and copy_() admit an explicit non_blocking argument, \n",
    "# which lets the caller bypass synchronization when it is unnecessary. Setting non_blocking=True allows us to simulate this scenario.\n",
    "\n",
    "# The total time required for both operations is (as expected) less than the sum of their parts. Note that this task is different from parallel computation as it uses a different \n",
    "# resource: the bus between the CPU and GPUs. In fact, we could compute on both devices and communicate, all at the same time. As noted above, there is a dependency between \n",
    "# computation and communication: y[i] must be computed before it can be copied to the CPU. Fortunately, the system can copy y[i-1] while computing y[i] to reduce the total \n",
    "# running time.\n",
    "\n",
    "with d2l.Benchmark('Run on GPU1 and copy to CPU'):\n",
    "    y = run(x_gpu1)\n",
    "    y_cpu = copy_to_cpu(y, True)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2ac5f",
   "metadata": {},
   "source": [
    "Below is an illustration of the computational graph and its dependencies for a simple two-layer MLP when training on a CPU and two GPUs. \n",
    "It would be quite painful to schedule the parallel program resulting from this manually. This is where it is advantageous to have a graph-based computing backend for optimization.\n",
    "\n",
    "![\"c g\"](./Images/Computationalgraph1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876296ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be45cf65",
   "metadata": {},
   "source": [
    "=======================================Hardware========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d73d9",
   "metadata": {},
   "source": [
    "![Latency numbers](./Images/LatencyNumbers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6966cae6",
   "metadata": {},
   "source": [
    "#### Computers\n",
    "\n",
    "##### 1. Overview\n",
    "\n",
    "Modern computers used in **deep learning** or **high-performance computing (HPC)** environments are composed of multiple key components that work together to execute, store, and transfer data efficiently.\n",
    "\n",
    "A well-balanced system design ensures that **no single component becomes a bottleneck** during computation, data movement, or communication.\n",
    "\n",
    "##### 2. Key Components of a Computer\n",
    "\n",
    "A typical deep learning computer includes:\n",
    "\n",
    "1. **Processor (CPU)**  \n",
    "   - Executes programs and system operations.  \n",
    "   - Modern CPUs have 8 or more cores.  \n",
    "   - Manages program logic, control flow, and data orchestration.\n",
    "\n",
    "2. **Memory (RAM)**  \n",
    "   - Temporarily stores data during computation, such as model parameters and activations.  \n",
    "   - Provides fast access for CPUs and GPUs.  \n",
    "\n",
    "3. **Network Connection**  \n",
    "   - Provides external communication (e.g., Ethernet).  \n",
    "   - Bandwidth ranges from **1 GB/s to 100 GB/s**, depending on hardware.  \n",
    "   - In servers, advanced interconnects improve data flow between machines.\n",
    "\n",
    "4. **Expansion Bus (PCIe)**  \n",
    "   - High-speed interface connecting CPUs to GPUs, SSDs, and network cards.  \n",
    "   - Provides **direct data transfer** between components.  \n",
    "   - Servers may include up to **8 GPUs** connected via PCIe; desktops usually have 1‚Äì2.\n",
    "\n",
    "5. **Durable Storage**  \n",
    "   - Includes hard drives (HDDs) or solid-state drives (SSDs).  \n",
    "   - Often connected via PCIe for fast data movement.  \n",
    "   - Stores large datasets and checkpoints efficiently.\n",
    "\n",
    "##### 3. Data Flow and Connectivity\n",
    "\n",
    "In a typical system, the **CPU acts as the central hub**:\n",
    "- It connects to **RAM, storage, GPUs, and the network** via the **PCIe bus**.\n",
    "- Data flows between these components as shown:\n",
    "\n",
    "![PCI connection](./Images/PCIConnection.png)\n",
    "\n",
    "##### Example:\n",
    "- AMD‚Äôs **Threadripper 3** has **64 PCIe 4.0 lanes**, each supporting **16 Gbit/s bidirectional transfer**.\n",
    "- Total memory bandwidth can reach **up to 100 GB/s**.\n",
    "\n",
    "##### 4. System Performance Considerations\n",
    "\n",
    "To achieve high performance:\n",
    "- Data movement between CPU, GPU, and storage must be **well-balanced**.  \n",
    "- **CPU/GPU starvation** occurs if data isn‚Äôt fed quickly enough to processing units.  \n",
    "- The **network should not slow down synchronization** when using distributed training.\n",
    "\n",
    "##### Optimization Techniques:\n",
    "- **Interleave computation and communication** ‚Äî ensures processors are always busy.  \n",
    "- **Avoid bottlenecks** ‚Äî each component should operate near its optimal throughput.\n",
    "\n",
    "##### ‚úÖ Key Takeaways\n",
    "\n",
    "| Component | Function | Typical Bandwidth | Key Role in Deep Learning |\n",
    "|------------|-----------|-------------------|---------------------------|\n",
    "| **CPU** | Executes instructions, orchestrates operations | 100+ GB/s (RAM access) | Control & coordination |\n",
    "| **RAM** | Stores intermediate results and activations | 50‚Äì100 GB/s | Fast temporary storage |\n",
    "| **PCIe** | Connects CPU, GPU, and storage | 16‚Äì32 GB/s per lane | High-speed interconnect |\n",
    "| **GPU** | Performs large-scale parallel computation | 500+ GB/s (onboard) | Training/inference core |\n",
    "| **Storage (SSD)** | Persistent data storage | 1‚Äì3 GB/s | Dataset & checkpoint storage |\n",
    "| **Network** | Cross-system data exchange | 1‚Äì100 GB/s | Distributed computing |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b55bf",
   "metadata": {},
   "source": [
    "\n",
    "#### Memory\n",
    "\n",
    "##### 1. CPU Memory (Main RAM)\n",
    "\n",
    "Modern CPUs typically use **DDR4 RAM**, providing **20‚Äì25 GB/s bandwidth per module**.  \n",
    "Each module has a **64-bit-wide bus**, and CPUs often support **2‚Äì4 memory channels**, giving a total bandwidth between **40 GB/s and 100 GB/s**.  \n",
    "There are usually multiple **banks per channel** (e.g., AMD‚Äôs Threadripper CPUs have 8 slots).\n",
    "\n",
    "##### 2. How Memory Access Works\n",
    "\n",
    "When data is accessed from RAM, the CPU must:\n",
    "1. **Send an address** to the memory module.\n",
    "2. Perform a **burst read**, where multiple consecutive data elements are read after the initial setup.\n",
    "\n",
    "- **First access latency:** ~100 ns  \n",
    "- **Subsequent reads:** ~0.2 ns  \n",
    "  ‚Üí The first read is about **500√ó slower** than subsequent ones.\n",
    "\n",
    "This means:\n",
    "- **Random memory access** is costly.\n",
    "- **Sequential (burst) access** is much faster and should be preferred whenever possible.\n",
    "\n",
    "##### 3. Memory Banks and Alignment\n",
    "\n",
    "Memory is divided into **banks**, each capable of independent access.  \n",
    "This allows:\n",
    "- **Up to 4√ó higher random read throughput** when accesses are evenly distributed.  \n",
    "- However, **burst reads** still outperform random reads overall.\n",
    "\n",
    "To optimize memory performance:\n",
    "- **Align data structures** to 64-bit boundaries.\n",
    "- Modern compilers handle alignment **automatically** when appropriate flags are used.\n",
    "\n",
    "##### 4. GPU Memory\n",
    "\n",
    "GPUs have far higher bandwidth needs due to massive parallelism.  \n",
    "They address this through two main design strategies:\n",
    "1. **Wider memory buses** (e.g., NVIDIA RTX 2080 Ti ‚Üí 352-bit bus).  \n",
    "2. **High-performance memory types**, such as:\n",
    "   - **GDDR6** (500+ GB/s bandwidth)\n",
    "   - **HBM (High Bandwidth Memory)** for top-end models like **NVIDIA Volta V100**.\n",
    "\n",
    "GPU memory:\n",
    "- Uses a **dedicated silicon interface**, making it **very fast but expensive**.\n",
    "- Has **smaller capacity** than CPU RAM.\n",
    "- Prioritizes **throughput over capacity** for deep learning workloads.\n",
    "\n",
    "##### ‚úÖ Key Takeaways\n",
    "\n",
    "| Aspect | CPU Memory | GPU Memory |\n",
    "|--------|-------------|------------|\n",
    "| Type | DDR4 | GDDR6 / HBM |\n",
    "| Bandwidth | 20‚Äì100 GB/s | 500+ GB/s |\n",
    "| Capacity | Larger | Smaller |\n",
    "| Latency | Higher initial read | Optimized for throughput |\n",
    "| Access pattern | Prefer sequential (burst) | Highly parallel |\n",
    "| Cost | Relatively cheap | Expensive (dedicated silicon) |\n",
    "\n",
    "##### 5. Summary\n",
    "\n",
    "- Memory serves as the CPU/GPU‚Äôs workspace for actively used data.\n",
    "- The key to high performance lies in **sequential access** and **alignment**.\n",
    "- GPU memory is designed for **speed and parallelism**, while CPU memory is optimized for **capacity and flexibility**.\n",
    "- Understanding burst reads, banks, and alignment helps avoid inefficient random memory access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3828709",
   "metadata": {},
   "source": [
    "#### Storage\n",
    "\n",
    "##### 1. Overview\n",
    "\n",
    "Like RAM, **storage devices** are defined by two main performance metrics:\n",
    "- **Bandwidth** ‚Äì how much data can be transferred per second.  \n",
    "- **Latency** ‚Äì how long it takes to start a transfer.  \n",
    "\n",
    "Storage systems tend to have much higher latency and lower bandwidth than memory.  \n",
    "Different storage technologies balance cost, speed, and capacity differently.\n",
    "\n",
    "##### 2. Hard Disk Drives (HDDs)\n",
    "\n",
    "**Hard disk drives (HDDs)** are mechanical storage devices that have been used for decades.  \n",
    "They store data on **spinning platters** accessed by **read/write heads** that move physically to the correct track.\n",
    "\n",
    "##### Key characteristics\n",
    "- Speed: ~**7,200 RPM**  \n",
    "- Latency: ~**8 ms per access**  \n",
    "- Performance: ~**100 IOPs (I/O operations per second)**  \n",
    "- Bandwidth: **100‚Äì200 MB/s**\n",
    "\n",
    "##### Limitations\n",
    "- Slow random access due to mechanical movement.  \n",
    "- Bandwidth improvements limited by physics (disk speed and density).  \n",
    "- Susceptible to **mechanical failure** (catastrophic).  \n",
    "\n",
    "üü† **Best for:** archival or low-cost large-capacity storage.\n",
    "\n",
    "##### 3. Solid State Drives (SSDs)\n",
    "\n",
    "**Solid state drives (SSDs)** use flash memory to store data without moving parts.  \n",
    "They are **orders of magnitude faster** than HDDs, especially for random access.\n",
    "\n",
    "##### Performance\n",
    "- **100,000‚Äì500,000 IOPs** (vs. ~100 for HDDs)  \n",
    "- **1‚Äì3 GB/s bandwidth**, up to **8 GB/s** with **NVMe PCIe 4.0**\n",
    "\n",
    "##### Advantages\n",
    "- No moving parts ‚Üí low latency and high reliability.  \n",
    "- Much faster for both sequential and random reads.\n",
    "\n",
    "##### Drawbacks\n",
    "- **Writes are slow**: SSDs store data in large blocks (‚â•256 KB).  \n",
    "  Writing requires reading, erasing, and rewriting an entire block.  \n",
    "- **Limited write endurance:** cells wear out after thousands of writes.  \n",
    "  ‚Üí Mitigated by **wear leveling** in firmware.  \n",
    "- Not recommended for swap space or large log aggregation due to high write volume.  \n",
    "\n",
    "üü¢ **Best for:** OS drives, high-speed data processing, ML datasets, caching.\n",
    "\n",
    "##### 4. Cloud Storage\n",
    "\n",
    "**Cloud storage** provides virtualized, scalable storage over a network.\n",
    "\n",
    "##### Characteristics\n",
    "- Dynamically adjustable capacity and bandwidth.  \n",
    "- Users can configure **IOPs (Input/Output Operations per Second)** to control performance.  \n",
    "- Higher latency due to network communication.\n",
    "\n",
    "üü£ **Best for:** scalable workloads and distributed systems, not low-latency operations like local ML training.\n",
    "\n",
    "##### ‚úÖ Key Comparison\n",
    "\n",
    "| Feature | HDD | SSD | Cloud Storage |\n",
    "|----------|-----|-----|---------------|\n",
    "| Type | Mechanical | Flash (no moving parts) | Virtual / Network |\n",
    "| Typical IOPS | ~100 | 100,000‚Äì500,000 | Configurable |\n",
    "| Bandwidth | 100‚Äì200 MB/s | 1‚Äì8 GB/s | Varies (network-limited) |\n",
    "| Latency | ~8 ms | ~100 ¬µs | Depends on network |\n",
    "| Cost | Low | Medium | Pay-as-you-go |\n",
    "| Durability | Fragile | Moderate (wear-out) | High (redundant) |\n",
    "| Best for | Archival / cold storage | Fast workloads | Elastic storage |\n",
    "\n",
    "##### 5. Summary\n",
    "\n",
    "- **HDDs**: Cheap, high capacity, but slow and mechanical.  \n",
    "- **SSDs**: Fast, silent, and reliable, but have limited write endurance.  \n",
    "- **Cloud storage**: Scalable and flexible, but slower due to network latency.  \n",
    "Understanding these trade-offs is essential for balancing **speed, cost, and capacity** in computing and deep learning workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b62c17",
   "metadata": {},
   "source": [
    "#### CPUs ‚Äî Summary & Explanation\n",
    "\n",
    "##### 1. Overview\n",
    "\n",
    "**Central Processing Units (CPUs)** are the core computational components of a computer system.  \n",
    "They contain:\n",
    "- **Processor cores** ‚Äî execute instructions.\n",
    "- **Caches** ‚Äî store recently used data for fast access.\n",
    "- **Interconnects** ‚Äî connect cores, caches, and memory subsystems.\n",
    "\n",
    "Modern CPUs often include:\n",
    "- **Integrated GPUs** (for graphics and computation)\n",
    "- **Vector processing units** (for high-performance math operations, e.g., convolutions)\n",
    "\n",
    "**Example:**  \n",
    "An Intel Skylake quad-core CPU integrates cores, caches, GPU, and system interfaces (Ethernet, Wi-Fi, USB) via a ring bus or PCIe connection.\n",
    "\n",
    "##### 2. Microarchitecture\n",
    "\n",
    "Each CPU core contains a pipeline with multiple stages that process instructions efficiently.\n",
    "\n",
    "##### Typical stages:\n",
    "1. **Fetch** ‚Äî Load instructions from memory.  \n",
    "2. **Decode** ‚Äî Translate assembly code to micro-operations.  \n",
    "3. **Dispatch** ‚Äî Send instructions to execution units.  \n",
    "4. **Execute** ‚Äî Perform arithmetic or logical operations.  \n",
    "5. **Write-back** ‚Äî Store results in registers or memory.\n",
    "\n",
    "Modern CPUs execute **multiple instructions per clock cycle** (superscalar and out-of-order execution).  \n",
    "For example, **ARM Cortex A77** can perform **up to 8 operations per cycle**.\n",
    "\n",
    "**Branch prediction:**  \n",
    "When encountering a conditional branch, CPUs predict which path will be taken to avoid idle cycles.  \n",
    "If the prediction is wrong, the pipeline is flushed and restarted ‚Äî losing cycles but improving average throughput.\n",
    "\n",
    "##### 3. Vectorization\n",
    "\n",
    "To boost performance in compute-heavy tasks like deep learning, CPUs use **vectorization**, i.e., performing the same operation on multiple data points simultaneously.\n",
    "\n",
    "##### SIMD (Single Instruction, Multiple Data)\n",
    "Instruction sets enabling vectorization:\n",
    "- ARM: **NEON**\n",
    "- Intel/AMD: **AVX**, **AVX2**, **AVX-512**\n",
    "\n",
    "Example: **128-bit NEON** can process 8 integers in one clock cycle.\n",
    "\n",
    "**Benefit:**  \n",
    "Vectorization enables CPUs to process arrays, matrices, or tensors much faster by grouping operations, reducing the number of instructions executed.\n",
    "\n",
    "While this boosts CPU throughput, **GPUs** still outperform CPUs because they have **thousands of parallel vector units**.\n",
    "\n",
    "##### 4. Cache\n",
    "\n",
    "**Caches** are small, fast memories inside the CPU that minimize data retrieval delays from slower RAM.\n",
    "\n",
    "##### Cache hierarch\n",
    "\n",
    "| Level | Typical Size | Speed | Description |\n",
    "|--------|---------------|--------|--------------|\n",
    "| **Registers** | Few KB | Fastest | Store currently executing data. |\n",
    "| **L1 Cache** | 32‚Äì64 KB per core | Extremely fast | First-level data and instruction cache. |\n",
    "| **L2 Cache** | 256‚Äì512 KB per core | Fast | Intermediate cache layer. |\n",
    "| **L3 Cache** | 4‚Äì8 MB shared | Slower | Shared across cores, used for inter-core data. |\n",
    "\n",
    "**Example:** AMD EPYC CPUs can feature **256 MB of L3 cache** to boost multi-core data sharing.\n",
    "\n",
    "##### Optimization Concepts\n",
    "- **Spatial locality:** Consecutive data stored together to exploit sequential access.  \n",
    "- **Temporal locality:** Recently used data kept in cache for reuse.  \n",
    "- **Prefetching:** Predicts and loads data likely to be used next.\n",
    "\n",
    "##### Cache Misses and Performance\n",
    "\n",
    "A **cache miss** occurs when data is not found in cache, forcing the CPU to fetch it from RAM (much slower).\n",
    "\n",
    "- **Miss penalty:** CPU stalls while waiting for data.  \n",
    "- **Mitigation:** Access data sequentially, reuse data, and design algorithms for locality.\n",
    "\n",
    "##### Trade-offs\n",
    "- Larger caches reduce misses but increase **latency** and **power usage**.  \n",
    "- CPU design balances cache size and access speed to optimize performance.\n",
    "\n",
    "##### ‚úÖ Key Takeaways\n",
    "\n",
    "| Concept | Purpose | Example / Benefit |\n",
    "|----------|----------|------------------|\n",
    "| **Cores** | Execute parallel tasks | Multicore performance |\n",
    "| **Pipelines** | Overlap instruction executionÔºõ parallel arithmetic at the register level | Improved throughput |\n",
    "| **Vector Units** | SIMD parallel operations | Boost ML and math performance |\n",
    "| **Caches** | Reduce latency | Faster memory access |\n",
    "| **Branch Prediction** | Minimize stalls | Higher efficiency |\n",
    "\n",
    "While CPUs handle **control-heavy and sequential** logic efficiently, GPUs dominate in **massively parallel** computation tasks like deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48581eaa",
   "metadata": {},
   "source": [
    "#### GPUs and Other Accelerators\n",
    "\n",
    "##### 1. Role of GPUs in Deep Learning\n",
    "\n",
    "It‚Äôs no exaggeration to say that **deep learning owes its success to GPUs**. GPU evolution paralleled the rise of modern neural networks ‚Äî their massive parallelism and floating-point efficiency made training feasible on large datasets.\n",
    "\n",
    "Unlike CPUs, GPUs are optimized for **massive parallel computation**, making them ideal for:\n",
    "- **Training neural networks** (large matrix multiplications, gradient accumulation)\n",
    "- **Inference** (forward propagation with minimal intermediate storage)\n",
    "\n",
    "##### 2. Precision and Efficiency\n",
    "\n",
    "Training requires maintaining numerical stability while storing gradients ‚Äî hence **mixed precision** is used:\n",
    "- **FP16 (half-precision)** for efficiency and memory savings  \n",
    "- **FP32 (single-precision)** for minimal numerical error  \n",
    "\n",
    "Modern GPUs (e.g., NVIDIA T4) balance both to handle training and inference efficiently using specialized units (Tensor Cores).\n",
    "\n",
    "##### 3. Vectorization and Core Scaling\n",
    "\n",
    "##### 3.1 Vectorization Beyond CPUs\n",
    "Traditional CPUs perform scalar or short-vector operations (e.g., SIMD).  \n",
    "GPUs extend this by:\n",
    "- Performing **dozens of operations in parallel** per core.\n",
    "- Handling **matrix operations** rather than just vector arithmetic.\n",
    "\n",
    "For instance, NVIDIA Turing GPUs allow **16 floating-point operations per vector simultaneously**.\n",
    "\n",
    "##### 3.2 Adding More Cores\n",
    "Instead of a few powerful cores like CPUs, GPUs scale horizontally with **thousands of smaller cores**, each performing simple operations in parallel.  \n",
    "This design massively boosts throughput for matrix-heavy computations like convolutions or attention mechanisms.\n",
    "\n",
    "##### 4. GPU Architecture Overview\n",
    "\n",
    "Each modern GPU contains:\n",
    "- Multiple **Streaming Multiprocessors (SMs)** ‚Äî independent blocks of parallel execution.  \n",
    "- Each SM includes **integer units**, **floating-point units**, and **Tensor Cores**.  \n",
    "- SMs are grouped into **Graphics Processing Clusters (GPCs)**, which together form the GPU die.\n",
    "\n",
    "**Example:**\n",
    "NVIDIA‚Äôs **Turing TU102** GPU (e.g., RTX 2080 Ti) consists of:\n",
    "- **12 streaming multiprocessors per GPC**\n",
    "- **Shared L2 cache and memory channels**\n",
    "- **Flexible modular design** ‚Äî blocks can be enabled/disabled for yield or thermal reasons.\n",
    "\n",
    "##### 5. Tensor Cores\n",
    "\n",
    "##### What They Are\n",
    "Tensor Cores are **specialized hardware units** that accelerate matrix multiplications ‚Äî the heart of deep learning.  \n",
    "They execute small dense matrix operations (e.g., **4√ó4**, **8√ó8**, **16√ó16**) extremely efficiently.\n",
    "\n",
    "##### Purpose\n",
    "Tensor Cores are optimized for:\n",
    "- **Mixed precision arithmetic** (FP16 input, FP32 accumulation)\n",
    "- **High-throughput training and inference**\n",
    "- **Deep learning frameworks** (CUDA, cuDNN, PyTorch) that automatically leverage them.\n",
    "\n",
    "**Comparison Example:**\n",
    "| Operation Type | Hardware | Efficiency |\n",
    "|----------------|-----------|-------------|\n",
    "| Scalar ops | CPU | Low |\n",
    "| Vector ops | GPU SM | High |\n",
    "| Matrix ops | Tensor Core | Extremely high |\n",
    "\n",
    "##### 6. Practical Implications for Deep Learning\n",
    "\n",
    "##### Advantages:\n",
    "- **Massive parallelism** ‚Üí ideal for batch matrix operations.  \n",
    "- **Efficient mixed precision** ‚Üí reduced memory use and higher speed.  \n",
    "- **Dedicated hardware (Tensor Cores)** ‚Üí exponential speed-up for neural networks.\n",
    "\n",
    "##### Limitations:\n",
    "- GPUs are **less efficient for serial or branching logic**.  \n",
    "- Limited **on-chip memory** can restrict model size.  \n",
    "- Performance depends heavily on **memory bandwidth** and **framework optimization**.\n",
    "\n",
    "##### ‚úÖ Key Takeaways\n",
    "\n",
    "| Feature | CPU | GPU | Tensor Core |\n",
    "|----------|-----|-----|--------------|\n",
    "| Parallelism | Few cores | Thousands of cores | Specialized matrix units |\n",
    "| Optimized for | Sequential logic | Vector/matrix operations | Matrix multiplications |\n",
    "| Precision | FP32 / FP64 | FP16 / FP32 | FP16 / FP32 mixed |\n",
    "| Typical use | Control-heavy workloads | Deep learning, rendering | Neural network training |\n",
    "| Example | Intel Core i9 | NVIDIA Turing TU102 | NVIDIA Tensor Core in T4/V100 |\n",
    "\n",
    "##### In short:\n",
    "GPUs and their accelerators (like TPUs) are built for **parallel math**, not general computation.  \n",
    "They revolutionized deep learning by enabling large-scale training with **billions of parameters** through:\n",
    "- **Parallel execution of small operations**\n",
    "- **Tensor Core acceleration**\n",
    "- **Mixed-precision computation**\n",
    "\n",
    "These architectural innovations make GPUs indispensable for both **AI research** and **real-world model deployment**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aada0b",
   "metadata": {},
   "source": [
    "#### Networks and Buses\n",
    "\n",
    "##### 1. Overview\n",
    "\n",
    "When a single device (like a CPU or GPU) is not enough for computation, **data transfer** between devices becomes essential.  \n",
    "This is where **networks and buses** play a critical role in synchronizing and transferring information efficiently.\n",
    "\n",
    "Key trade-offs include:\n",
    "- **Bandwidth** ‚Äî how much data can be transferred per second.\n",
    "- **Latency** ‚Äî how fast communication happens.\n",
    "- **Cost, distance, and flexibility** ‚Äî affecting scalability and architecture.\n",
    "\n",
    "While **WiFi** is flexible and cheap, it‚Äôs unsuitable for deep learning due to low bandwidth and high latency.  \n",
    "Instead, we focus on **high-performance interconnects** like PCIe, Ethernet, NVLink, and switches.\n",
    "\n",
    "##### 2. PCIe (Peripheral Component Interconnect Express)\n",
    "\n",
    "- **Purpose:** High-bandwidth, low-latency communication between CPU, GPU, and other peripherals.  \n",
    "- **Speed:** Up to **32 GB/s per lane** on PCIe 4.0 with a 16-lane slot.  \n",
    "- **Latency:** Extremely low (~5 ¬µs).  \n",
    "- **Limitations:**  \n",
    "  - Limited number of lanes per processor (e.g., 128 for AMD EPYC, 48 for Intel Xeon, 16 for Core i9).  \n",
    "  - GPUs often occupy 16 lanes each, limiting how many can be connected to the CPU.  \n",
    "  - Large bulk transfers are preferred to minimize packet overhead.\n",
    "\n",
    "##### 3. Ethernet\n",
    "\n",
    "- **Most common** networking method for connecting computers and servers.  \n",
    "- **Pros:**  \n",
    "  - Cheap, resilient, and supports long-distance connections.  \n",
    "  - Widely used in **data centers and cloud environments** (e.g., AWS, Azure).  \n",
    "- **Bandwidth:**  \n",
    "  - Consumer-grade: **1 Gbit/s**  \n",
    "  - High-end / cloud: **10‚Äì100 Gbit/s**\n",
    "- **Overhead:**  \n",
    "  - Ethernet requires protocols like **TCP/IP or UDP**, which add extra latency.  \n",
    "  - Typically connects two devices (computer ‚Üî switch).\n",
    "\n",
    "##### 4. Switches\n",
    "\n",
    "- **Function:** Allow multiple devices to communicate concurrently, enabling **many-to-many connections**.  \n",
    "- Example:  \n",
    "  - An Ethernet switch might connect **40+ servers** at high bandwidth.  \n",
    "- **In deep learning clusters:**  \n",
    "  - Switches can also link multiple GPUs or nodes together (e.g., distributed training setups).  \n",
    "- **PCIe Switching:**  \n",
    "  - PCIe itself can also be switched to connect multiple GPUs to a single CPU host.\n",
    "\n",
    "##### 5. NVLink\n",
    "\n",
    "- **Purpose:** NVIDIA‚Äôs high-speed alternative to PCIe for **GPU-to-GPU** and **GPU-to-CPU** communication.  \n",
    "- **Performance:**  \n",
    "  - Up to **300 GB/s per link** (significantly higher than PCIe).  \n",
    "  - **Server GPUs** (e.g., Volta V100) have 6 NVLink connections.  \n",
    "  - **Consumer GPUs** (e.g., RTX 2080 Ti) have only one NVLink (100 GB/s).  \n",
    "- **Use Case:**  \n",
    "  - Used with **NCCL (NVIDIA Collective Communication Library)** for fast inter-GPU communication in distributed deep learning.\n",
    "\n",
    "##### ‚úÖ Key Takeaways\n",
    "\n",
    "| Technology | Typical Bandwidth | Latency | Purpose | Use Case |\n",
    "|-------------|------------------|----------|----------|-----------|\n",
    "| **PCIe 4.0** | Up to 32 GB/s/lane | ~5 ¬µs | CPU‚ÄìGPU / storage interconnect | Internal device communication |\n",
    "| **Ethernet** | 1‚Äì100 Gbit/s | Higher | Long-distance connectivity | Data centers, cloud |\n",
    "| **Switches** | Varies | Depends on setup | Multi-device connectivity | Distributed GPU/CPU clusters |\n",
    "| **NVLink** | Up to 300 GB/s | Very low | GPU‚ÄìGPU / GPU‚ÄìCPU | High-speed deep learning clusters |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03fcfde",
   "metadata": {},
   "source": [
    "=====================================Training on Multiple GPUs==========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a82c6",
   "metadata": {},
   "source": [
    "#### Training on Multiple GPUs\n",
    "\n",
    "##### 1. Overview\n",
    "\n",
    "When deep learning models grow large ‚Äî with millions or even billions of parameters ‚Äî a **single GPU** often becomes insufficient due to **memory limitations** or **computation bottlenecks**.  \n",
    "To overcome this, we distribute the model or data across **multiple GPUs**.  \n",
    "\n",
    "This section explains how training is split across GPUs using **model parallelism** and **data parallelism**.\n",
    "\n",
    "##### 2. Splitting the Problem\n",
    "\n",
    "##### 2.1 Motivation\n",
    "Large models (e.g., ResNet, BERT, GPT) cannot fit entirely into one GPU‚Äôs memory.  \n",
    "By dividing the model across multiple GPUs, we can:\n",
    "- Reduce memory load per GPU.\n",
    "- Train larger networks efficiently.\n",
    "- Parallelize computation to shorten training time.\n",
    "\n",
    "##### 3. Model Parallelism\n",
    "\n",
    "##### Concept\n",
    "In **model parallelism**, different parts (layers or operations) of the model are placed on different GPUs.  \n",
    "Each GPU computes only a portion of the forward and backward passes.\n",
    "\n",
    "**Example:**\n",
    "- GPU 1 handles the first half of the network layers.\n",
    "- GPU 2 handles the later layers.\n",
    "- Activations are passed between GPUs during the forward pass, and gradients are passed back during the backward pass.\n",
    "\n",
    "##### Advantages\n",
    "- Enables training of **very large models** that exceed single-GPU memory.\n",
    "- Efficient for architectures with large layers (e.g., Transformers).\n",
    "\n",
    "##### Disadvantages\n",
    "- Requires frequent **synchronization** between GPUs.\n",
    "- Introduces **communication overhead** during gradient transfers.\n",
    "- Imbalanced workloads can cause **idle GPU time** (known as ‚Äúpipeline bubbles‚Äù).\n",
    "\n",
    "##### Types of Model Parallelism\n",
    "| Type | Description |\n",
    "|------|--------------|\n",
    "| **Layer Partitioning** | Different layers assigned to different GPUs. |\n",
    "| **Tensor Partitioning** | Splits large matrix/tensor operations across GPUs. |\n",
    "| **Pipeline Parallelism** | Stages of the model processed in a streaming fashion ‚Äî while GPU 1 handles batch *n*, GPU 2 processes batch *n-1*. |\n",
    "\n",
    "##### 4. Data Parallelism\n",
    "\n",
    "##### Concept\n",
    "In **data parallelism**, each GPU holds a **full copy of the model** but trains on **different mini-batches** of data simultaneously.\n",
    "\n",
    "Steps:\n",
    "1. Each GPU performs a **forward and backward pass** on its batch.  \n",
    "2. The **gradients** are averaged (synchronized) across GPUs.  \n",
    "3. Model parameters are updated consistently across all GPUs.\n",
    "\n",
    "**Example:**  \n",
    "If we use 4 GPUs, each processes ¬º of the training data per step, then synchronizes gradients with others.\n",
    "\n",
    "##### Advantages\n",
    "- Scales efficiently to **many GPUs**.\n",
    "- Requires **minimal model changes**.\n",
    "- Works well for large datasets with many examples.\n",
    "\n",
    "##### Disadvantages\n",
    "- Synchronization cost grows with GPU count.\n",
    "- May lead to communication bottlenecks if network bandwidth is low.\n",
    "\n",
    "##### 5. Combining Techniques\n",
    "\n",
    "In modern deep learning:\n",
    "- **Hybrid strategies** (e.g., model + data parallelism) are used for massive models.  \n",
    "- Frameworks like **PyTorch Distributed**, **DeepSpeed**, and **Megatron-LM** support multi-level parallelism for large-scale training (billions of parameters).\n",
    "\n",
    "##### ‚úÖ Key Takeaways\n",
    "\n",
    "| Approach | Model Copy | GPU Role | Best For | Main Limitation |\n",
    "|-----------|-------------|----------|-----------|------------------|\n",
    "| **Model Parallelism** | Split across GPUs | Each GPU handles part of the model | Very large models | High communication overhead |\n",
    "| **Data Parallelism** | Full model per GPU | Each GPU handles different data | Large datasets | Gradient synchronization cost |\n",
    "| **Pipeline Parallelism** | Split sequentially | GPUs work in stages | Deep sequential models | Idle time (pipeline bubbles) |\n",
    "\n",
    "##### Summary:\n",
    "- **Model parallelism** breaks up the **model**; **data parallelism** splits the **data**.  \n",
    "- Synchronization and communication efficiency determine scalability.  \n",
    "- Combining both enables training of models far beyond single-GPU limits ‚Äî the foundation of **modern distributed deep learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c21254af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a5059da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "scale = 0.01\n",
    "W1 = torch.randn(size=(20, 1, 3, 3)) * scale\n",
    "b1 = torch.zeros(20)\n",
    "W2 = torch.randn(size=(50, 20, 5, 5)) * scale\n",
    "b2 = torch.zeros(50)\n",
    "W3 = torch.randn(size=(800, 128)) * scale\n",
    "b3 = torch.zeros(128)\n",
    "W4 = torch.randn(size=(128, 10)) * scale\n",
    "b4 = torch.zeros(10)\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "# Define the model\n",
    "def lenet(X, params):\n",
    "    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\n",
    "    h1_activation = F.relu(h1_conv)\n",
    "    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])\n",
    "    h2_activation = F.relu(h2_conv)\n",
    "    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2 = h2.reshape(h2.shape[0], -1)\n",
    "    h3_linear = torch.mm(h2, params[4]) + params[5]\n",
    "    h3 = F.relu(h3_linear)\n",
    "    y_hat = torch.mm(h3, params[6]) + params[7]\n",
    "    return y_hat\n",
    "\n",
    "# Cross-entropy loss function\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e2d4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Synchronization:\n",
    "# 1. we need to have the ability to distribute a list of parameters to multiple devices and to attach gradients (get_params).\n",
    "# 2. we need the ability to sum parameters across multiple devices, i.e., we need an allreduce function.\n",
    "\n",
    "def get_params(params, device):\n",
    "    new_params = [p.to(device) for p in params]\n",
    "    for p in new_params:\n",
    "        p.requires_grad_()\n",
    "    return new_params\n",
    "\n",
    "def allreduce(data):\n",
    "    # Sum all parameters to the first device\n",
    "    for i in range(1, len(data)):\n",
    "        data[0][:] += data[i].to(data[0].device)\n",
    "        # broadcast the summed parameters back to all devices\n",
    "    for i in range(1, len(data)):\n",
    "        data[i][:] = data[0].to(data[i].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2bf0402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 weight: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "b1 grad: None\n"
     ]
    }
   ],
   "source": [
    "new_params = get_params(params, d2l.try_gpu(0))\n",
    "print('b1 weight:', new_params[1])\n",
    "print('b1 grad:', new_params[1].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d753a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before allreduce:\n",
      " tensor([[1., 1.]]) \n",
      " tensor([[2., 2.]])\n",
      "after allreduce:\n",
      " tensor([[3., 3.]]) \n",
      " tensor([[3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "data = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)]\n",
    "print('before allreduce:\\n', data[0], '\\n', data[1])\n",
    "allreduce(data)\n",
    "print('after allreduce:\\n', data[0], '\\n', data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963618e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  distribute a minibatch evenly across multiple ( for example there are 100 training examples in the minibatch, send 50 training examples to each GPU)\n",
    "\n",
    "data = torch.arange(20).reshape(4, 5)\n",
    "devices = [torch.device('cuda:0'), torch.device('cuda:1')]\n",
    "split = nn.parallel.scatter(data, devices)\n",
    "print('input :', data)\n",
    "print('load into', devices)\n",
    "print('output:', split)\n",
    "\n",
    "# For later reuse we define a split_batch function that splits both data and labels.\n",
    "def split_batch(X, y, devices):\n",
    "    \"\"\"Split `X` and `y` into multiple devices.\"\"\"\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    return (nn.parallel.scatter(X, devices), # data\n",
    "            nn.parallel.scatter(y, devices)) # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(X, y, device_params, devices, lr):\n",
    "    X_shards, y_shards = split_batch(X, y, devices)\n",
    "    # Loss is calculated separately on each GPU\n",
    "    ls = [loss(lenet(X_shard, device_W), y_shard).sum()\n",
    "          for X_shard, y_shard, device_W in zip(X_shards, y_shards, device_params)]\n",
    "    for l in ls:  # Backpropagation is performed separately on each GPU\n",
    "        l.backward()\n",
    "    # Sum all gradients from each GPU and broadcast them to all GPUs\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(device_params[0])):\n",
    "            allreduce([device_params[c][i].grad for c in range(len(devices))])\n",
    "    # The model parameters are updated separately on each GPU\n",
    "    for param in device_params:\n",
    "        d2l.sgd(param, lr, X.shape[0]) # Here, we use a full-size batch\n",
    "\n",
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    # Copy model parameters to `num_gpus` GPUs\n",
    "    device_params = [get_params(params, d) for d in devices]\n",
    "    num_epochs = 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    timer = d2l.Timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            # Perform multi-GPU training for a single minibatch\n",
    "            train_batch(X, y, device_params, devices, lr)\n",
    "            torch.cuda.synchronize()\n",
    "        timer.stop()\n",
    "        # Evaluate the model on GPU 0\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n",
    "          f'on {str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18666f",
   "metadata": {},
   "source": [
    "train(num_gpus=1, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96e37f",
   "metadata": {},
   "source": [
    "train(num_gpus=2, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c11650",
   "metadata": {},
   "source": [
    "====================================Concise Implementation for Multiple GPUs================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8432a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04a22028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def resnet18(num_classes, in_channels=1):\n",
    "    \"\"\"A slightly modified ResNet-18 model.\"\"\"\n",
    "    def resnet_block(in_channels, out_channels, num_residuals,\n",
    "                     first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(d2l.Residual(out_channels, use_1x1conv=True,\n",
    "                                        strides=2))\n",
    "            else:\n",
    "                blk.append(d2l.Residual(out_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "    # This model uses a smaller convolution kernel, stride, and padding and\n",
    "    # removes the max-pooling layer\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU())\n",
    "    net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\n",
    "    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
    "    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
    "    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n",
    "    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1,1)))\n",
    "    net.add_module(\"fc\", nn.Sequential(nn.Flatten(),\n",
    "                                       nn.Linear(512, num_classes)))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c68c71e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huiya\\pytorch-basics\\.venv311\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "net = resnet18(10)\n",
    "# Get a list of GPUs\n",
    "devices = d2l.try_all_gpus()\n",
    "# We will initialize the network inside the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training code needs to perform several basic functions for efficient parallelism:\n",
    "# 1. Network parameters need to be initialized across all devices.\n",
    "# 2. While iterating over the dataset minibatches are to be divided across all devices.\n",
    "# 3. We compute the loss and its gradient in parallel across devices.\n",
    "# 4. Gradients are aggregated and parameters are updated accordingly.\n",
    "\n",
    "def train(net, num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    def init_weights(module):\n",
    "        if type(module) in [nn.Linear, nn.Conv2d]:\n",
    "            nn.init.normal_(module.weight, std=0.01)\n",
    "    net.apply(init_weights)\n",
    "    # Set the model on multiple GPUs\n",
    "    net = nn.DataParallel(net, device_ids=devices)\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    timer, num_epochs = d2l.Timer(), 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(devices[0]), y.to(devices[0]) # ????\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "        timer.stop()\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))\n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n",
    "          f'on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, num_gpus=1, batch_size=256, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f576256",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, num_gpus=2, batch_size=512, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aa367b",
   "metadata": {},
   "source": [
    "===================================Parameter Servers=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01994890",
   "metadata": {},
   "source": [
    "![Data Parallel training](./Images//DataParallelTraining.png)\n",
    "![Parameter sync strategies](./Images/ParameterSyncStragegies.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
