{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e07934e",
   "metadata": {},
   "source": [
    "========= From Fully Connected Layers to Convolutions ========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c31e9d",
   "metadata": {},
   "source": [
    "### Invariance\n",
    "\n",
    "The chapter motivates CNNs through the \"Where's Waldo\" analogy. Three key principles for image processing:\n",
    "\n",
    "1. **Translation Invariance**: The network should respond similarly regardless of WHERE an object appears in the image\n",
    "2. **Locality**: Early layers should focus on local regions, not distant pixels\n",
    "3. **Hierarchical Features**: Deeper layers capture longer-range features by aggregating local information\n",
    "\n",
    "### Constraining the MLP\n",
    "\n",
    "#### Starting Point: Fully Connected Layer\n",
    "\n",
    "For a 2D image input $\\mathbf{X}$ and hidden representation $\\mathbf{H}$, a fully connected layer would be:\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l [\\mathsf{W}]_{i, j, k, l} [\\mathbf{X}]_{k, l}$$\n",
    "\n",
    "where:\n",
    "- $[\\mathbf{X}]_{i,j}$ is the pixel at position $(i, j)$\n",
    "- $[\\mathsf{W}]_{i,j,k,l}$ are the weights connecting input $(k, l)$ to output $(i, j)$\n",
    "- $[\\mathbf{U}]_{i,j}$ is the bias\n",
    "\n",
    "**Problem**: For a $1000 \\times 1000$ image, this requires $10^{12}$ parameters!\n",
    "\n",
    "#### Translation Invariance\n",
    "\n",
    "We invoke the first principle: the detector should work the same regardless of position.\n",
    "\n",
    "Reindex by setting $a = k - i$ and $b = l - j$:\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = [\\mathbf{U}]_{i, j} + \\sum_a \\sum_b [\\mathsf{V}]_{a, b} [\\mathbf{X}]_{i+a, j+b}$$\n",
    "\n",
    "Key insight: $\\mathsf{V}$ no longer depends on $(i, j)$ — **weights are shared across all positions**.\n",
    "\n",
    "This is a **convolution**! We weight pixels at $(i+a, j+b)$ near $(i, j)$ using coefficients $[\\mathsf{V}]_{a, b}$.\n",
    "\n",
    "#### Locality\n",
    "\n",
    "We invoke the second principle: only nearby pixels matter for local features.\n",
    "\n",
    "Restrict the sum to a local neighborhood of size $\\Delta$:\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b} [\\mathbf{X}]_{i+a, j+b}$$\n",
    "\n",
    "**Parameter reduction**: From $4 \\times 10^6$ parameters to $(2\\Delta + 1)^2$ parameters (e.g., $4 \\times 10^4$ for typical kernel sizes).\n",
    "\n",
    "This is a **convolutional layer** — the foundation of CNNs.\n",
    "\n",
    "### Convolutions\n",
    "\n",
    "#### Mathematical Definition\n",
    "\n",
    "**Continuous convolution**:\n",
    "$$(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x} - \\mathbf{z}) \\, d\\mathbf{z}$$\n",
    "\n",
    "**Discrete convolution (1D)**:\n",
    "$$(f * g)(i) = \\sum_a f(a) g(i - a)$$\n",
    "\n",
    "**Discrete convolution (2D)**:\n",
    "$$(f * g)(i, j) = \\sum_a \\sum_b f(a, b) g(i - a, j - b)$$\n",
    "\n",
    "#### Convolution vs Cross-Correlation\n",
    "\n",
    "Note the sign difference between:\n",
    "- **Convolution**: $g(i - a, j - b)$\n",
    "- **Cross-correlation**: $g(i + a, j + b)$\n",
    "\n",
    "Our formula in (7.1.3) is technically a **cross-correlation**, but in deep learning we call it \"convolution\" by convention. The difference doesn't matter because the kernel weights are learned.\n",
    "\n",
    "### Channels\n",
    "\n",
    "Real images have multiple input channels (e.g., RGB = 3 channels). We extend to handle this:\n",
    "\n",
    "$$[\\mathsf{H}]_{i,j,d} = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c [\\mathsf{V}]_{a, b, c, d} [\\mathsf{X}]_{i+a, j+b, c}$$\n",
    "\n",
    "where:\n",
    "- $c$ indexes **input channels** (e.g., R, G, B)\n",
    "- $d$ indexes **output channels** (feature maps in the hidden layer)\n",
    "- $[\\mathsf{V}]_{a, b, c, d}$ is a 4D tensor of learnable parameters\n",
    "\n",
    "This is the **general form of a convolutional layer**.\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Aspect | Fully Connected | Convolutional |\n",
    "|--------|-----------------|---------------|\n",
    "| **Parameters** | $O(n^4)$ for $n \\times n$ images | $O(\\Delta^2 \\cdot c_{in} \\cdot c_{out})$ |\n",
    "| **Translation Invariance** | No | Yes (weight sharing) |\n",
    "| **Locality** | No (all pixels connected) | Yes (kernel size $\\Delta$) |\n",
    "| **Inductive Bias** | None | Matches image structure |\n",
    "\n",
    "### Key Principles Derived:\n",
    "\n",
    "1. **Translation Invariance** → **Weight Sharing**: Same kernel applied everywhere\n",
    "2. **Locality** → **Small Kernels**: Only neighboring pixels influence each output\n",
    "3. **Channels** → **Feature Maps**: Multiple learned filters capture different patterns\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "- **Massive parameter reduction**: Makes training feasible on images\n",
    "- **Built-in inductive bias**: Reflects true structure of visual data\n",
    "- **Hierarchical learning**: \n",
    "  - Early layers: edges, textures\n",
    "  - Middle layers: parts, patterns\n",
    "  - Deep layers: objects, scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00586b6",
   "metadata": {},
   "source": [
    "================ Convolutions for Images ================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90779565",
   "metadata": {},
   "source": [
    "### The Cross-Correlation Operation\n",
    "\n",
    "Strictly speaking, convolutional layers perform **cross-correlation**, not convolution (the kernel is not flipped).\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "Given:\n",
    "- Input tensor $\\mathbf{X}$ of shape $(n_h \\times n_w)$\n",
    "- Kernel tensor $\\mathbf{K}$ of shape $(k_h \\times k_w)$\n",
    "\n",
    "The kernel slides over the input, computing element-wise products and summing:\n",
    "\n",
    "**Example** (from Fig 7.2.1 with $2 \\times 2$ kernel on $3 \\times 3$ input):\n",
    "\n",
    "$$\\begin{aligned}\n",
    "0\\times0+1\\times1+3\\times2+4\\times3&=19\\\\\n",
    "1\\times0+2\\times1+4\\times2+5\\times3&=25\\\\\n",
    "3\\times0+4\\times1+6\\times2+7\\times3&=37\\\\\n",
    "4\\times0+5\\times1+7\\times2+8\\times3&=43\n",
    "\\end{aligned}$$\n",
    "\n",
    "#### Output Size Formula\n",
    "\n",
    "$$\\text{Output size} = (n_h - k_h + 1) \\times (n_w - k_w + 1)$$\n",
    "\n",
    "For the example: $(3-2+1) \\times (3-2+1) = 2 \\times 2$\n",
    "\n",
    "### Convolutional Layers\n",
    "\n",
    "A convolutional layer consists of:\n",
    "1. **Kernel (weights)**: Learnable parameters\n",
    "2. **Bias**: Scalar added to output\n",
    "\n",
    "$$\\mathbf{Y} = \\text{corr2d}(\\mathbf{X}, \\mathbf{K}) + b$$\n",
    "\n",
    "An $h \\times w$ convolution (or $h \\times w$ convolution kernel) refers to a kernel of height $h$ and width $w$.\n",
    "\n",
    "### Object Edge Detection in Images\n",
    "\n",
    "#### Example: Vertical Edge Detection\n",
    "\n",
    "**Input**: $6 \\times 8$ image with vertical edges\n",
    "```\n",
    "[[1, 1, 0, 0, 0, 0, 1, 1],\n",
    " [1, 1, 0, 0, 0, 0, 1, 1],\n",
    " ...]]\n",
    "```\n",
    "\n",
    "**Kernel**: $1 \\times 2$ edge detector\n",
    "$$\\mathbf{K} = [1, -1]$$\n",
    "\n",
    "**Output**: Detects transitions\n",
    "- White-to-black edge: outputs $1$\n",
    "- Black-to-white edge: outputs $-1$\n",
    "- No change: outputs $0$\n",
    "\n",
    "**Key insight**: This kernel only detects **vertical** edges. Applying it to the transposed image (horizontal edges) produces all zeros.\n",
    "\n",
    "### Learning a Kernel\n",
    "\n",
    "Instead of hand-designing kernels, we can **learn** them from data.\n",
    "\n",
    "**Training loop** (simplified):\n",
    "```python\n",
    "conv2d = Conv2D(kernel_size=(1, 2))\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    loss = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    loss.sum().backward()\n",
    "    conv2d.weight.data -= lr * conv2d.weight.grad\n",
    "```\n",
    "\n",
    "After training, the learned kernel approximates $[1, -1]$.\n",
    "\n",
    "### Cross-Correlation and Convolution\n",
    "\n",
    "#### Mathematical Difference\n",
    "\n",
    "**Cross-correlation**:\n",
    "$$(\\mathbf{X} \\star \\mathbf{K})_{i,j} = \\sum_a \\sum_b \\mathbf{K}_{a,b} \\cdot \\mathbf{X}_{i+a, j+b}$$\n",
    "\n",
    "**True convolution** (kernel flipped):\n",
    "$$(\\mathbf{X} * \\mathbf{K})_{i,j} = \\sum_a \\sum_b \\mathbf{K}_{a,b} \\cdot \\mathbf{X}_{i-a, j-b}$$\n",
    "\n",
    "#### Why It Doesn't Matter\n",
    "\n",
    "- Flipping the kernel $\\mathbf{K}$ gives $\\mathbf{K}'$\n",
    "- Cross-correlation with $\\mathbf{K}' =$ Convolution with $\\mathbf{K}$\n",
    "- Since kernels are **learned**, the network learns the appropriate (possibly flipped) version\n",
    "\n",
    "**Convention**: Deep learning uses \"convolution\" to mean cross-correlation.\n",
    "\n",
    "### Feature Map and Receptive Field\n",
    "\n",
    "#### Feature Map\n",
    "The output of a convolutional layer is called a **feature map** — it represents learned spatial features of the input.\n",
    "\n",
    "#### Receptive Field\n",
    "\n",
    "The **receptive field** of an output element is the region in the input that affects its computation.\n",
    "\n",
    "- For a single $k \\times k$ conv layer: receptive field = $k \\times k$\n",
    "- For stacked layers: receptive field grows with depth\n",
    "\n",
    "**Biological inspiration**: Named after neuroscience concept — neurons in visual cortex respond to specific regions of the visual field (Hubel & Wiesel, Nobel Prize 1981).\n",
    "\n",
    "#### Deep Networks and Receptive Fields\n",
    "\n",
    "With each convolutional layer, the receptive field grows:\n",
    "- Layer 1: $k \\times k$\n",
    "- Layer 2: $(2k-1) \\times (2k-1)$\n",
    "- Layer $n$: receptive field increases, capturing larger-scale patterns\n",
    "\n",
    "This is why **deep** CNNs can recognize complex objects — early layers detect edges, deeper layers detect parts, even deeper layers detect whole objects.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Concept | Formula/Description |\n",
    "|---------|---------------------|\n",
    "| Cross-correlation | $\\sum_a \\sum_b K_{a,b} \\cdot X_{i+a, j+b}$ |\n",
    "| Output size | $(n_h - k_h + 1) \\times (n_w - k_w + 1)$ |\n",
    "| Convolutional layer | $Y = \\text{corr2d}(X, K) + b$ |\n",
    "| Edge detection kernel | $[1, -1]$ for vertical edges |\n",
    "| Receptive field | Input region affecting one output element |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Cross-correlation** is the actual operation (kernel not flipped), but called \"convolution\" by convention\n",
    "2. **Kernels can be learned** — no need for manual design\n",
    "3. **Feature maps** are spatial representations of detected patterns\n",
    "4. **Receptive field** grows with network depth, enabling hierarchical feature learning\n",
    "5. **Edge detection** is a simple but powerful example of what convolutions can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8856f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f80e872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d94a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7434a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b87a4c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ca5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f90f60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25448024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(), K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9fc8b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 5.894\n",
      "epoch 4, loss 1.783\n",
      "epoch 6, loss 0.624\n",
      "epoch 8, loss 0.238\n",
      "epoch 10, loss 0.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/pytorch-d2l/.venv310/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# Construct a two-dimensional convolutional layer with 1 output channel and a\n",
    "# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# The two-dimensional convolutional layer uses four-dimensional input and\n",
    "# output in the format of (example, channel, height, width), where the batch\n",
    "# size (number of examples in the batch) and the number of channels are both 1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 3e-2  # Learning rate\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    # Update the kernel\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i + 1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb10e088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0230, -0.9602]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
