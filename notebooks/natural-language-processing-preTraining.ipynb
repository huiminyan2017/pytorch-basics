{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad96b6b",
   "metadata": {},
   "source": [
    "================================================Word Embedding (word2vec)======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b5728",
   "metadata": {},
   "source": [
    "Word vectors are vectors used to represent words, and can also be considered as feature vectors or representations of words. The technique of mapping words to real vectors is called word embedding.\n",
    "\n",
    "The word2vec tool contains both the skip-gram and continuous bag of words models.\n",
    "\n",
    "The skip-gram model assumes that a word can be used to generate its surrounding words in a text sequence; while the continuous bag of words model assumes that a center word is generated based on its surrounding context words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3d9a4",
   "metadata": {},
   "source": [
    "\n",
    "| Symbol             | Meaning                                         | Role in Skip-Gram                                | Role in CBOW                                                                               |\n",
    "| ------------------ | ----------------------------------------------- | ------------------------------------------------ | ------------------------------------------------------------------------------------------ |\n",
    "| $V$                | Vocabulary set                                  | All possible words                               | All possible words                                                                         |\n",
    "| $T$                | Length of training corpus                       | Sequence positions $t=1\\dots T$                  | Same                                                                                       |\n",
    "| $m$                | Context window size                             | Number of words on either side                   | Same                                                                                       |\n",
    "| $w_t$              | Word at position $t$ in corpus                  | *Center word* for predicting contexts            | *Target/center word* to be predicted                                                       |\n",
    "| $w_{t+j}$          | Word at position $t+j$                          | One *context word* to be predicted               | One *context word* used to predict center                                                  |\n",
    "| $W_c$              | Set of context words around a center            | Not used                                         | $W_c={w_{t+j}: -m\\le j\\le m,j\\ne0}$                                                        |\n",
    "| $\\mathbf{v}_w$     | Input embedding                                 | Vector of the center word $w$                    | Vector of each context word $w$                                                            |\n",
    "| $\\mathbf{u}_w$     | Output embedding                                | Vector of a context word $w$                     | Vector of the target center word $w_t$                                                     |\n",
    "| $\\mathbf{v}_{w_c}$ | Center embedding of word $w_c$                  | Used to score $P(w_o\\mid w_c)$                   | Not used                                                                                   |\n",
    "| $\\mathbf{v}_{W_c}$ | *Context summary vector*                        | Not used                                         | Averaged context: $$\\mathbf{v}*{W_c}=\\frac{1}{2m}\\sum*{j=-m,j\\ne0}^m\\mathbf{v}*{w*{t+j}}$$ |\n",
    "| $\\mathbf{u}_{w_o}$ | Output embedding of word $w_o$                  | Used in softmax score $P(w_o\\mid w_c)$           | Not used                                                                                   |\n",
    "| $\\mathbf{u}_{w_t}$ | Output embedding of word $w_t$                  | Not used                                         | Used in softmax score $P(w_t\\mid W_c)$                                                     |\n",
    "| $P(w_o\\mid w_c)$   | Predict context word given center               | Modeled by softmax in Skip-Gram                  | ‚Äî                                                                                          |\n",
    "| $P(w_t\\mid W_c)$   | Predict center given context set                | ‚Äî                                                | Modeled by softmax in CBOW                                                                 |\n",
    "| $\\hat{y}$          | Model probability output (softmax)              | Softmax over contexts                            | Softmax over center vocab                                                                  |\n",
    "| Loss               | Negative log-likelihood                         | $$-\\sum_{t}\\sum_{j\\ne0}\\log P(w_{t+j}\\mid w_t)$$ | $$-\\sum_{t}\\log P(w_t\\mid W_c)$$                                                           |\n",
    "| SGD                | Stochastic gradient descent                     | Updates $\\mathbf{v},\\mathbf{u}$                  | Same                                                                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775b128",
   "metadata": {},
   "source": [
    "#### The Skip-Gram Model\n",
    "\n",
    "##### Overview\n",
    "\n",
    "The **Skip-Gram model** learns word embeddings by predicting **context words** given a **center word**.\n",
    "It is the core model behind **Word2Vec**, producing dense vectors that capture semantic similarity through shared contexts.\n",
    "\n",
    "**Example:**  \n",
    "For the sentence *‚Äúthe man loves his son‚Äù*, with center word *‚Äúloves‚Äù* and window size $m=2$, the model predicts:\n",
    "$\\{\\text{the}, \\text{man}, \\text{his}, \\text{son}\\}$.\n",
    "\n",
    "##### 1Ô∏è. Conditional Independence Assumption\n",
    "\n",
    "Skip-Gram assumes context words are **conditionally independent** given the center word $w_t$:\n",
    "\n",
    "$$\n",
    "P(\\{w_{t+j}\\}_{j \\neq 0} \\mid w_t)\n",
    "=\n",
    "\\prod_{\\substack{-m \\le j \\le m \\\\ j \\neq 0}}\n",
    "P(w_{t+j} \\mid w_t).\n",
    "$$\n",
    "\n",
    "This simplifies learning by treating each center‚Äìcontext pair independently.\n",
    "\n",
    "##### 2Ô∏è. Probability Model (Softmax)\n",
    "\n",
    "Each word $w \\in V$ has two vector representations:\n",
    "\n",
    "- $\\mathbf{v}_w$: **center (input) embedding**\n",
    "- $\\mathbf{u}_w$: **context (output) embedding**\n",
    "\n",
    "The probability of observing a context word $w_o$ given a center word $w_c$ is:\n",
    "\n",
    "$$\n",
    "P(w_o \\mid w_c)\n",
    "=\n",
    "\\frac{\\exp(\\mathbf{u}_{w_o}^T \\mathbf{v}_{w_c})}\n",
    "{\\sum_{w \\in V} \\exp(\\mathbf{u}_w^T \\mathbf{v}_{w_c})}.\n",
    "$$\n",
    "\n",
    "The softmax normalizes scores over the entire vocabulary.\n",
    "\n",
    "##### 3Ô∏è. Objective Function\n",
    "\n",
    "Given a corpus $\\{w_1,\\dots,w_T\\}$ and window size $m$, Skip-Gram maximizes the likelihood:\n",
    "\n",
    "$$\n",
    "\\prod_{t=1}^{T}\n",
    "\\prod_{\\substack{-m \\le j \\le m \\\\ j \\neq 0}}\n",
    "P(w_{t+j} \\mid w_t).\n",
    "$$\n",
    "\n",
    "Equivalently, it minimizes the **average negative log-likelihood**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\n",
    "=\n",
    "-\\frac{1}{T}\n",
    "\\sum_{t=1}^{T}\n",
    "\\sum_{\\substack{-m \\le j \\le m \\\\ j \\neq 0}}\n",
    "\\log P(w_{t+j} \\mid w_t).\n",
    "$$\n",
    "\n",
    "##### 4. Training Procedure\n",
    "\n",
    "The parameters $\\{\\mathbf{v}_w, \\mathbf{u}_w\\}_{w \\in V}$ are learned by minimizing $\\mathcal{L}$ using\n",
    "**stochastic gradient descent (SGD)**.\n",
    "\n",
    "At each step:\n",
    "- sample a center word $w_c$,\n",
    "- sample its context words $w_o$ within window $m$,\n",
    "- update the corresponding embeddings.\n",
    "\n",
    "##### 5Ô∏è. Gradient Computation\n",
    "\n",
    "For a single center‚Äìcontext pair $(w_c, w_o)$:\n",
    "\n",
    "$$\n",
    "\\log P(w_o \\mid w_c)\n",
    "=\n",
    "\\mathbf{u}_{w_o}^T \\mathbf{v}_{w_c}\n",
    "-\n",
    "\\log \\sum_{w \\in V} \\exp(\\mathbf{u}_w^T \\mathbf{v}_{w_c}).\n",
    "$$\n",
    "\n",
    "**Gradient w.r.t. center embedding $\\mathbf{v}_{w_c}$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\log P(w_o \\mid w_c)}{\\partial \\mathbf{v}_{w_c}}\n",
    "=\n",
    "\\mathbf{u}_{w_o}\n",
    "-\n",
    "\\sum_{w \\in V} P(w \\mid w_c)\\mathbf{u}_w.\n",
    "$$\n",
    "\n",
    "**Gradient w.r.t. context embedding $\\mathbf{u}_{w_o}$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\log P(w_o \\mid w_c)}{\\partial \\mathbf{u}_{w_o}}\n",
    "=\n",
    "\\mathbf{v}_{w_c}\\bigl(1 - P(w_o \\mid w_c)\\bigr).\n",
    "$$\n",
    "\n",
    "##### 6Ô∏è. Intuition\n",
    "\n",
    "- Skip-Gram learns by **predicting context from center words**.\n",
    "- Words that appear in similar contexts are pushed toward **similar embeddings**.\n",
    "- After training:\n",
    "  - $\\mathbf{v}_w$: commonly used as the final **word embedding**\n",
    "  - $\\mathbf{u}_w$: represents **context-specific embeddings**\n",
    "\n",
    "The model encodes semantic similarity through shared prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956acd5e",
   "metadata": {},
   "source": [
    "#### The Continuous Bag of Words (CBOW) Model\n",
    "\n",
    "##### Overview\n",
    "\n",
    "The **Continuous Bag of Words (CBOW)** model learns word representations by **predicting a center word from its surrounding context words**. It is the inverse of the **Skip-Gram** model, which predicts context words from a center word.\n",
    "\n",
    "**Example:**  \n",
    "For the sentence *‚Äúthe man loves his son‚Äù* with window size $m=2$, the context words $\\{\\text{the}, \\text{man}, \\text{his}, \\text{son}\\}$ are used to predict the center word *‚Äúloves‚Äù*.\n",
    "\n",
    "##### 1Ô∏è. Context Representation\n",
    "\n",
    "Let the context of center word $w_t$ be:\n",
    "$$\n",
    "W_c = \\{\\,w_{t+j}:\\ -m \\le j \\le m,\\ j \\ne 0\\,\\}.\n",
    "$$\n",
    "\n",
    "Each word $w \\in V$ has two vector representations:\n",
    "- $\\mathbf{v}_w$: **input (context) vector**\n",
    "- $\\mathbf{u}_w$: **output (center/target) vector**\n",
    "\n",
    "CBOW represents the context by averaging the input vectors:\n",
    "$$\n",
    "\\mathbf{v}_{W_c}\n",
    "=\n",
    "\\frac{1}{2m}\n",
    "\\sum_{\\substack{-m \\le j \\le m \\\\ j \\ne 0}}\n",
    "\\mathbf{v}_{w_{t+j}}.\n",
    "$$\n",
    "\n",
    "This mirrors Skip-Gram‚Äôs use of a single center vector $\\mathbf{v}_{w_c}$, but aggregates multiple context vectors instead.\n",
    "\n",
    "##### 2Ô∏è. Softmax Probability\n",
    "\n",
    "Given the aggregated context vector $\\mathbf{v}_{W_c}$, the probability of predicting the center word $w_t$ is:\n",
    "\n",
    "$$\n",
    "P(w_t \\mid W_c)\n",
    "=\n",
    "\\frac{\\exp(\\mathbf{u}_{w_t}^\\top \\mathbf{v}_{W_c})}\n",
    "{\\sum_{w \\in V} \\exp(\\mathbf{u}_w^\\top \\mathbf{v}_{W_c})}.\n",
    "$$\n",
    "\n",
    "This is a softmax over the vocabulary $V$, analogous to Skip-Gram‚Äôs\n",
    "$P(w_o \\mid w_c)$ formulation.\n",
    "\n",
    "##### 3Ô∏è. Objective Function\n",
    "\n",
    "Given a corpus $\\{w_1,\\dots,w_T\\}$, CBOW maximizes the likelihood:\n",
    "$$\n",
    "\\prod_{t=1}^{T}\n",
    "P(w_t \\mid w_{t-m}, \\ldots, w_{t+m}).\n",
    "$$\n",
    "\n",
    "Equivalently, it minimizes the **negative log-likelihood**:\n",
    "$$\n",
    "\\mathcal{L}\n",
    "=\n",
    "- \\sum_{t=1}^{T}\n",
    "\\log P(w_t \\mid W_c).\n",
    "$$\n",
    "\n",
    "This has the same structure as Skip-Gram, with center and context roles reversed.\n",
    "\n",
    "##### 4Ô∏è. Training Procedure\n",
    "\n",
    "CBOW is trained using **stochastic gradient descent (SGD)**.\n",
    "\n",
    "At each step:\n",
    "- sample a center word $w_t$ and its context $W_c$,\n",
    "- compute the averaged context vector,\n",
    "- update the corresponding $\\mathbf{v}_w$ and $\\mathbf{u}_w$.\n",
    "\n",
    "Averaging context vectors makes CBOW computationally efficient.\n",
    "\n",
    "##### 5Ô∏è. Gradient Computation\n",
    "\n",
    "For a single training pair $(W_c, w_t)$:\n",
    "$$\n",
    "\\log P(w_t \\mid W_c)\n",
    "=\n",
    "\\mathbf{u}_{w_t}^\\top \\mathbf{v}_{W_c}\n",
    "-\n",
    "\\log \\sum_{w \\in V} \\exp(\\mathbf{u}_w^\\top \\mathbf{v}_{W_c}).\n",
    "$$\n",
    "\n",
    "**Gradient with respect to a context embedding $\\mathbf{v}_{w_i}$:**\n",
    "$$\n",
    "\\frac{\\partial \\log P(w_t \\mid W_c)}{\\partial \\mathbf{v}_{w_i}}\n",
    "=\n",
    "\\frac{1}{2m}\n",
    "\\left(\n",
    "\\mathbf{u}_{w_t}\n",
    "-\n",
    "\\sum_{w \\in V} P(w \\mid W_c)\\mathbf{u}_w\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Each context word receives an equal share of the gradient due to averaging.\n",
    "\n",
    "##### 6Ô∏è. Intuition\n",
    "\n",
    "- CBOW predicts **one center word** from **many context words**.\n",
    "- Context embeddings are averaged to form a single representation.\n",
    "- Words appearing in similar contexts learn similar embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36995a",
   "metadata": {},
   "source": [
    "Due to the nature of the softmax operation, since a context word may be anyone in the dictionary, the opposite of avove algorithm contains the summation of items as many as the entire size of the vocabulary. Consequently, the gradient calculation for the skip-gram model and that for the continuous bag-of-words model both contain the summation. Unfortunately, the computational cost for such gradients that sum over a large dictionary (often with hundreds of thousands or millions of words) is huge!\n",
    "In order to reduce the computational complexity, this section will introduce two approximate training methods: **negative sampling** and **hierarchical softmax**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fecd00",
   "metadata": {},
   "source": [
    "#### Negative Sampling\n",
    "\n",
    "##### Overview\n",
    "\n",
    "The original Skip-Gram objective uses a full softmax over the entire vocabulary $‚à£ùëâ‚à£$, which is computationally expensive because it requires updating gradients for all words.\n",
    "\n",
    "Instead of updating **all word vectors**, it updates only: - the **positive example** (true context word)\\- **K negative samples** (noise words). This makes training fast and scalable.\n",
    "\n",
    "##### 1Ô∏è. Positive Example Probability\n",
    "\n",
    "Given a center word $w_c$ and true context word $w_o$:\n",
    "\n",
    "$$\n",
    "P(D = 1 \\mid w_c, w_o) = \\sigma(\\mathbf{u}_{w_o}^ T \\mathbf{v}_{w_c})\n",
    "$$\n",
    "\n",
    "Sigmoid:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "##### 2. Why Negative Sampling?\n",
    "\n",
    "The original Skip-Gram objective considers only **positive examples**, which leads to:\n",
    "- $\\prod_{t=1}^{T} \\prod_{-m<=j<=m, j \\ne 0}P(D = 1 \\mid w^{(t)}, w^{(t+j)})$ Only maximizes when all word vectors grow to infinity ‚Üí meaningless maximization (vectors blow up to infinity) \n",
    "- extremely expensive softmax (needs $|V|$ updates)\n",
    "\n",
    "Negative sampling fixes both.\n",
    "\n",
    "##### 3. How Negative Sampling Works\n",
    "\n",
    "For each positive pair ($w_c$, $w_o$):\n",
    "- Draw **K negative samples** $w_k$ from noise distribution $P(w)$\n",
    "- Treat them as \"fake context words\" with label $D=0$\n",
    "\n",
    "The new objective approximation:\n",
    "\n",
    "$$\n",
    "P(w_o \\mid w_c) \\approx \n",
    "P(D = 1 \\mid w_c, w_o)\n",
    "\\prod_{k=1}^{K} P(D = 0 \\mid w_c, w_k)\n",
    "$$\n",
    "\n",
    "##### 4. Loss Function\n",
    "\n",
    "Let $i$ = index of center word, $h$ = index of true context word:\n",
    "\n",
    "$$\n",
    "-\\log \\sigma(\\mathbf{u}_{h}^ T \\mathbf{v}_{i})\n",
    "-\n",
    "\\sum_{k=1}^{K} \\log(1 - \\sigma(\\mathbf{u}_{k}^ T \\mathbf{v}_{i}))\n",
    "$$\n",
    "\n",
    "Equivalent form:\n",
    "\n",
    "$$\n",
    "-\\log \\sigma(\\mathbf{u}_{h}^ T \\mathbf{v}_{i})\n",
    "+\n",
    "\\sum_{k=1}^{K} \\log \\sigma(-\\mathbf{u}_{k}^ T \\mathbf{v}_{i})\n",
    "$$\n",
    "\n",
    "**Meaning:** \n",
    "- Positive pair ‚Üí push dot product higher\n",
    "- Negative pairs ‚Üí push dot products lower\n",
    "\n",
    "##### 5Ô∏è. Computational Efficiency\n",
    "\n",
    "With negative sampling: - Computation depends only on **K**, not vocabulary size\n",
    "- Typical K = 5 to 20\n",
    "- Enables training on large corpora quickly\n",
    "\n",
    "Negative sampling reduces training to a scalable binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b9369",
   "metadata": {},
   "source": [
    "#### Hierarchical Softmax\n",
    "\n",
    "##### Overview\n",
    "\n",
    "Hierarchical softmax is an efficient alternative to the traditional softmax used in Word2Vec. Instead of computing probabilities over the entire vocabulary $|V|$,\n",
    "it organizes all words into a **binary tree**, where each **leaf node** corresponds to a word.\n",
    "\n",
    "Predicting a word becomes making a sequence of **binary decisions** along the path from the root to that word's leaf node.\n",
    "\n",
    "This reduces computational cost from:\n",
    "\n",
    "$$\n",
    "O(|V|) \\quad \\rightarrow \\quad O(\\log |V|)\n",
    "$$\n",
    "\n",
    "##### 1Ô∏è. Tree Structure\n",
    "\n",
    "- Each word $w$ is represented as a **leaf node**.\n",
    "- Let $L(w)$ be the number of nodes on the path (including the leaf).\n",
    "- Let $n(w, j)$ denote the $j$-th node on the path.\n",
    "- Each **internal node** $n$ has an associated vector $\\mathbf{u}_n$.\n",
    "- Each internal node performs a **binary decision**:\n",
    "  - go **left**\n",
    "  - or go **right**\n",
    "\n",
    "Thus, predicting a word is equivalent to predicting the correct sequence of left/right decisions.\n",
    "\n",
    "\n",
    "![Hierarchical softmax](./Images/hierarchicalSoftmax.png)\n",
    "\n",
    "##### 2Ô∏è. Probability of a Word\n",
    "\n",
    "Hierarchical softmax models\n",
    "$$\n",
    "P(w_o \\mid w_c)\n",
    "$$\n",
    "as the probability of making **all correct binary decisions** along the path from the root to word $w_o$.\n",
    "\n",
    "##### Binary Decision at One Node\n",
    "\n",
    "At internal node $n$, given center word $w_c$, define:\n",
    "$$\n",
    "P(\\text{go left at } n \\mid w_c)\n",
    "= \\sigma(\\mathbf{u}_n^\\top \\mathbf{v}_{w_c}),\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{go right at } n \\mid w_c)\n",
    "= 1 - \\sigma(\\mathbf{u}_n^\\top \\mathbf{v}_{w_c})\n",
    "= \\sigma(-\\mathbf{u}_n^\\top \\mathbf{v}_{w_c}),\n",
    "$$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$.\n",
    "\n",
    "##### Encoding the Correct Direction\n",
    "\n",
    "Define a sign function:\n",
    "$$\n",
    "\\delta(w_o, j)\n",
    "=\n",
    "\\begin{cases}\n",
    "+1, & \\text{if } n(w_o, j+1) \\text{ is the left child of } n(w_o, j), \\\\\n",
    "-1, & \\text{if } n(w_o, j+1) \\text{ is the right child}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This encodes whether the correct decision at node $n(w_o,j)$ is **left** or **right**.\n",
    "\n",
    "##### Full Probability Formula\n",
    "\n",
    "The probability of generating word $w_o$ is the product of correct binary decisions:\n",
    "\n",
    "$$\n",
    "P(w_o \\mid w_c)\n",
    "=\n",
    "\\prod_{j = 1}^{L(w_o)-1}\n",
    "\\sigma\\!\\left(\n",
    "\\delta(w_o, j)\\,\n",
    "\\mathbf{u}_{n(w_o, j)}^\\top\n",
    "\\mathbf{v}_{w_c}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Each term is the probability of choosing the correct branch at one node.\n",
    "\n",
    "\n",
    "Or the formula can be written as:\n",
    "\n",
    "$$\n",
    "P(w_o \\mid w_c)\n",
    "=\n",
    "\\prod_{j = 1}^{L(w_o)-1}\n",
    "\\sigma\\left(\n",
    "\\big[ n(w_o, j+1) = \\text{leftChild}(n(w_o, j)) \\big]\n",
    "\\cdot\n",
    "\\mathbf{u}_{n(w_o, j)}^\\top \\mathbf{v}_{w_c}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "-   The bracket equals **+1** if the next node is the left child\n",
    "-   Equals **-1** if the next node is the right child\n",
    "\n",
    "So the model learns to output a correct sequence of binary choices leading to the target word‚Äôs leaf.\n",
    "\n",
    "##### 3Ô∏è. Example\n",
    "\n",
    "For word $w_3$, suppose the path is:\n",
    "\n",
    "-   root ‚Üí left ‚Üí right ‚Üí leaf\n",
    "\n",
    "Probability becomes:\n",
    "\n",
    "$$\n",
    "P(w_3 \\mid w_c)\n",
    "=\n",
    "\\sigma(\\mathbf{u}_{n_1}^T \\mathbf{v}_{w_c})\n",
    "\\cdot\n",
    "\\sigma(-\\mathbf{u}_{n_2}^T \\mathbf{v}_{w_c})\n",
    "\\cdot\n",
    "\\sigma(\\mathbf{u}_{n_3}^T \\mathbf{v}_{w_c})\n",
    "$$\n",
    "\n",
    "##### 4Ô∏è. Valid Probability Distribution\n",
    "\n",
    "Hierarchical softmax ensures that:\n",
    "\n",
    "$$\n",
    "\\sum_{w \\in V} P(w \\mid w_c) = 1\n",
    "$$\n",
    "\n",
    "even though it bypasses the full softmax.\n",
    "\n",
    "##### Key Takeaways\n",
    "\n",
    "-   Converts predicting a word into making $\\log |V|$ binary decisions.\n",
    "-   Efficient for large vocabularies.\n",
    "-   Widely used in Word2Vec models.\n",
    "-   Provides a scalable alternative to full softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbae97",
   "metadata": {},
   "source": [
    "================================================The Dataset for Pretraining Word Embeddings======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab301ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd11399e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# sentences: 42069'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip','319d85e578af0cdc590547f26231e4e31cdf1e42')\n",
    "\n",
    "#@save\n",
    "def read_ptb():\n",
    "    \"\"\"Load the PTB dataset into a list of text lines.\"\"\"\n",
    "    data_dir = d2l.download_extract('ptb')\n",
    "    # Read the training set\n",
    "    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n",
    "        raw_text = f.read()\n",
    "    return [line.split() for line in raw_text.split('\\n')]\n",
    "\n",
    "sentences = read_ptb()\n",
    "f'# sentences: {len(sentences)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391edb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vocab size: 6719'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "f'vocab size: {len(vocab)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc45a16",
   "metadata": {},
   "source": [
    "#### Subsampling\n",
    "\n",
    "##### Why Subsampling?\n",
    "\n",
    "In natural language corpora, **high-frequency words** (e.g., ‚Äúthe‚Äù, ‚Äúa‚Äù, ‚Äúin‚Äù) appear extremely often.\n",
    "These words contribute little semantic information for learning word embeddings, yet they dominate\n",
    "training examples and slow down optimization.\n",
    "\n",
    "Subsampling addresses this by **randomly discarding frequent words** before training.\n",
    "\n",
    "##### What Subsampling Achieves\n",
    "\n",
    "Subsampling:\n",
    "- Reduces the number of trivial and redundant training pairs\n",
    "- Speeds up training\n",
    "- Emphasizes informative word co-occurrences\n",
    "- Improves the quality of learned embeddings\n",
    "\n",
    "Rare words are kept almost always, while very frequent words are often removed.\n",
    "\n",
    "##### Subsampling Rule\n",
    "\n",
    "For each word token $w_i$ in the corpus, the probability of **keeping** it is:\n",
    "\n",
    "$$\n",
    "P(\\text{keep } w_i)\n",
    "= \\sqrt{\\frac{t}{f(w_i)}},\n",
    "$$\n",
    "\n",
    "and the probability of **discarding** it is:\n",
    "\n",
    "$$\n",
    "P(\\text{discard } w_i)\n",
    "= \\max\\left(1 - \\sqrt{\\frac{t}{f(w_i)}},\\, 0\\right),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $f(w_i)$ is the relative frequency of word $w_i$ in the corpus\n",
    "- $t$ is a small threshold (typically $10^{-4}$)\n",
    "\n",
    "##### Intuition\n",
    "\n",
    "- If $f(w_i) \\ll t$:  \n",
    "  the word is rare ‚Üí almost always kept\n",
    "- If $f(w_i) \\gg t$:  \n",
    "  the word is very frequent ‚Üí likely to be discarded\n",
    "\n",
    "This selectively removes words that carry little semantic signal.\n",
    "\n",
    "##### Effect on the Dataset\n",
    "\n",
    "After subsampling:\n",
    "- Sentences become shorter\n",
    "- Context windows contain more informative words\n",
    "- The effective training corpus is smaller but higher quality\n",
    "\n",
    "This preprocessing step is crucial for efficient and effective training of\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f9ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def subsample(sentences, vocab):\n",
    "    \"\"\"Subsample high-frequency words.\"\"\"\n",
    "    # Exclude unknown tokens ('<unk>')\n",
    "    sentences = [[token for token in line if vocab[token] != vocab.unk] for line in sentences]\n",
    "    counter = collections.Counter([token for line in sentences for token in line])\n",
    "    num_tokens = sum(counter.values())\n",
    "\n",
    "    # Return True if `token` is kept during subsampling\n",
    "    def keep(token):\n",
    "        return(random.uniform(0, 1) <\n",
    "               math.sqrt(1e-4 / counter[token] * num_tokens))\n",
    "\n",
    "    return ([[token for token in line if keep(token)] for line in sentences],\n",
    "            counter)\n",
    "\n",
    "subsampled, counter = subsample(sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5cf97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.190625pt\" height=\"184.649167pt\" viewBox=\"0 0 262.190625 184.649167\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-12-31T17:18:06.566556</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 184.649167 \n",
       "L 262.190625 184.649167 \n",
       "L 262.190625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 59.690625 147.092917 \n",
       "L 254.990625 147.092917 \n",
       "L 254.990625 8.492917 \n",
       "L 59.690625 8.492917 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 68.567898 147.092917 \n",
       "L 75.814651 147.092917 \n",
       "L 75.814651 125.508457 \n",
       "L 68.567898 125.508457 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 86.684781 147.092917 \n",
       "L 93.931534 147.092917 \n",
       "L 93.931534 87.249796 \n",
       "L 86.684781 87.249796 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 104.801664 147.092917 \n",
       "L 112.048417 147.092917 \n",
       "L 112.048417 77.957318 \n",
       "L 104.801664 77.957318 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 122.918547 147.092917 \n",
       "L 130.1653 147.092917 \n",
       "L 130.1653 98.153623 \n",
       "L 122.918547 98.153623 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 141.03543 147.092917 \n",
       "L 148.282183 147.092917 \n",
       "L 148.282183 126.68975 \n",
       "L 141.03543 126.68975 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 159.152313 147.092917 \n",
       "L 166.399067 147.092917 \n",
       "L 166.399067 140.163026 \n",
       "L 159.152313 140.163026 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 177.269196 147.092917 \n",
       "L 184.51595 147.092917 \n",
       "L 184.51595 145.525118 \n",
       "L 177.269196 145.525118 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 195.38608 147.092917 \n",
       "L 202.632833 147.092917 \n",
       "L 202.632833 146.695524 \n",
       "L 195.38608 146.695524 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 213.502963 147.092917 \n",
       "L 220.749716 147.092917 \n",
       "L 220.749716 146.956824 \n",
       "L 213.502963 146.956824 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 231.619846 147.092917 \n",
       "L 238.866599 147.092917 \n",
       "L 238.866599 147.016705 \n",
       "L 231.619846 147.016705 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 75.814651 147.092917 \n",
       "L 83.061404 147.092917 \n",
       "L 83.061404 15.092917 \n",
       "L 75.814651 15.092917 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: url(#hb34200e03c)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 93.931534 147.092917 \n",
       "L 101.178287 147.092917 \n",
       "L 101.178287 60.749631 \n",
       "L 93.931534 60.749631 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: url(#hb34200e03c)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 112.048417 147.092917 \n",
       "L 119.29517 147.092917 \n",
       "L 119.29517 137.054646 \n",
       "L 112.048417 137.054646 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: url(#hb34200e03c)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 130.1653 147.092917 \n",
       "L 137.412054 147.092917 \n",
       "L 137.412054 146.504992 \n",
       "L 130.1653 146.504992 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: url(#hb34200e03c)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 148.282183 147.092917 \n",
       "L 155.528937 147.092917 \n",
       "L 155.528937 147.049367 \n",
       "L 148.282183 147.049367 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: url(#hb34200e03c)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 166.399067 147.092917 \n",
       "L 173.64582 147.092917 \n",
       "L 173.64582 147.092917 \n",
       "L 166.399067 147.092917 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: url(#hb34200e03c)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_19\">\n",
       "    <path d=\"M 184.51595 147.092917 \n",
       "L 191.762703 147.092917 \n",
       "L 191.762703 147.092917 \n",
       "L 184.51595 147.092917 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: url(#hb34200e03c)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 202.632833 147.092917 \n",
       "L 209.879586 147.092917 \n",
       "L 209.879586 147.092917 \n",
       "L 202.632833 147.092917 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: url(#hb34200e03c)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 220.749716 147.092917 \n",
       "L 227.996469 147.092917 \n",
       "L 227.996469 147.092917 \n",
       "L 220.749716 147.092917 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: url(#hb34200e03c)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_22\">\n",
       "    <path d=\"M 238.866599 147.092917 \n",
       "L 246.113352 147.092917 \n",
       "L 246.113352 147.092917 \n",
       "L 238.866599 147.092917 \n",
       "z\n",
       "\" clip-path=\"url(#pfe38d5db0b)\" style=\"fill: url(#hb34200e03c)\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m1b39f89101\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1b39f89101\" x=\"66.756209\" y=\"147.092917\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.574959 161.691355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1b39f89101\" x=\"110.943729\" y=\"147.092917\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(104.581229 161.691355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1b39f89101\" x=\"155.131249\" y=\"147.092917\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(148.768749 161.691355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1b39f89101\" x=\"199.318769\" y=\"147.092917\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(192.956269 161.691355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1b39f89101\" x=\"243.506289\" y=\"147.092917\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 80 -->\n",
       "      <g transform=\"translate(237.143789 161.691355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- # tokens per sentence -->\n",
       "     <g transform=\"translate(100.6125 175.36948) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-23\" d=\"M 3272 2816 \n",
       "L 2363 2816 \n",
       "L 2100 1772 \n",
       "L 3016 1772 \n",
       "L 3272 2816 \n",
       "z\n",
       "M 2803 4594 \n",
       "L 2478 3297 \n",
       "L 3391 3297 \n",
       "L 3719 4594 \n",
       "L 4219 4594 \n",
       "L 3897 3297 \n",
       "L 4872 3297 \n",
       "L 4872 2816 \n",
       "L 3775 2816 \n",
       "L 3519 1772 \n",
       "L 4513 1772 \n",
       "L 4513 1294 \n",
       "L 3397 1294 \n",
       "L 3072 0 \n",
       "L 2572 0 \n",
       "L 2894 1294 \n",
       "L 1978 1294 \n",
       "L 1656 0 \n",
       "L 1153 0 \n",
       "L 1478 1294 \n",
       "L 494 1294 \n",
       "L 494 1772 \n",
       "L 1594 1772 \n",
       "L 1856 2816 \n",
       "L 850 2816 \n",
       "L 850 3297 \n",
       "L 1978 3297 \n",
       "L 2297 4594 \n",
       "L 2803 4594 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-23\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"83.789062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"115.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"154.785156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\" x=\"215.966797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"270.251953\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"331.775391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"395.154297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"447.253906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"479.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"542.517578\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"604.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"645.154297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"676.941406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"729.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"790.564453\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"853.943359\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"893.152344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"954.675781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"1018.054688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"1073.035156\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m74dcadb3c0\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m74dcadb3c0\" x=\"59.690625\" y=\"147.092917\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(46.328125 150.892136) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m74dcadb3c0\" x=\"59.690625\" y=\"119.874178\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 5000 -->\n",
       "      <g transform=\"translate(27.240625 123.673396) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m74dcadb3c0\" x=\"59.690625\" y=\"92.655438\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 10000 -->\n",
       "      <g transform=\"translate(20.878125 96.454657) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m74dcadb3c0\" x=\"59.690625\" y=\"65.436698\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 15000 -->\n",
       "      <g transform=\"translate(20.878125 69.235917) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m74dcadb3c0\" x=\"59.690625\" y=\"38.217958\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 20000 -->\n",
       "      <g transform=\"translate(20.878125 42.017177) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m74dcadb3c0\" x=\"59.690625\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 25000 -->\n",
       "      <g transform=\"translate(20.878125 14.798437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- count -->\n",
       "     <g transform=\"translate(14.798438 91.899167) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-63\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"116.162109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"179.541016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"242.919922\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_23\">\n",
       "    <path d=\"M 59.690625 147.092917 \n",
       "L 59.690625 8.492917 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_24\">\n",
       "    <path d=\"M 254.990625 147.092917 \n",
       "L 254.990625 8.492917 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_25\">\n",
       "    <path d=\"M 59.690625 147.092917 \n",
       "L 254.990625 147.092917 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_26\">\n",
       "    <path d=\"M 59.690625 8.492917 \n",
       "L 254.990625 8.492917 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_27\">\n",
       "     <path d=\"M 155.389063 45.849167 \n",
       "L 247.990625 45.849167 \n",
       "Q 249.990625 45.849167 249.990625 43.849167 \n",
       "L 249.990625 15.492917 \n",
       "Q 249.990625 13.492917 247.990625 13.492917 \n",
       "L 155.389063 13.492917 \n",
       "Q 153.389063 13.492917 153.389063 15.492917 \n",
       "L 153.389063 43.849167 \n",
       "Q 153.389063 45.849167 155.389063 45.849167 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"patch_28\">\n",
       "     <path d=\"M 157.389063 25.091355 \n",
       "L 177.389063 25.091355 \n",
       "L 177.389063 18.091355 \n",
       "L 157.389063 18.091355 \n",
       "z\n",
       "\" style=\"fill: #1f77b4\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- origin -->\n",
       "     <g transform=\"translate(185.389063 25.091355) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"61.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"102.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"130.078125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"193.554688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"221.337891\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"patch_29\">\n",
       "     <path d=\"M 157.389063 39.76948 \n",
       "L 177.389063 39.76948 \n",
       "L 177.389063 32.76948 \n",
       "L 157.389063 32.76948 \n",
       "z\n",
       "\" style=\"fill: url(#hb34200e03c)\"/>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- subsampled -->\n",
       "     <g transform=\"translate(185.389063 39.76948) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-62\" x=\"115.478516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"178.955078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"231.054688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"292.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"389.746094\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"453.222656\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"481.005859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"542.529297\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pfe38d5db0b\">\n",
       "   <rect x=\"59.690625\" y=\"8.492917\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       " <defs>\n",
       "  <pattern id=\"hb34200e03c\" patternUnits=\"userSpaceOnUse\" x=\"0\" y=\"0\" width=\"72\" height=\"72\">\n",
       "   <rect x=\"0\" y=\"0\" width=\"73\" height=\"73\" fill=\"#ff7f0e\"/>\n",
       "   <path d=\"M -36 36 \n",
       "L 36 -36 \n",
       "M -24 48 \n",
       "L 48 -24 \n",
       "M -12 60 \n",
       "L 60 -12 \n",
       "M 0 72 \n",
       "L 72 0 \n",
       "M 12 84 \n",
       "L 84 12 \n",
       "M 24 96 \n",
       "L 96 24 \n",
       "M 36 108 \n",
       "L 108 36 \n",
       "\" style=\"fill: #000000; stroke: #000000; stroke-width: 1.0; stroke-linecap: butt; stroke-linejoin: miter\"/>\n",
       "  </pattern>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.show_list_len_pair_hist(['origin', 'subsampled'], '# tokens per sentence',\n",
    "                            'count', sentences, subsampled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af37761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# of \"the\": before=50770, after=2016'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_counts(token):\n",
    "    return (f'# of \"{token}\": '\n",
    "            f'before={sum([l.count(token) for l in sentences])}, '\n",
    "            f'after={sum([l.count(token) for l in subsampled])}')\n",
    "\n",
    "compare_counts('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4613068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# of \"join\": before=45, after=45'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('join')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a50f08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [6697, 4127, 3228, 4060], [3922, 1922, 4743]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [vocab[line] for line in subsampled]\n",
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941569d",
   "metadata": {},
   "source": [
    "Extracting Center Words and Context Words\n",
    "\n",
    "The `get_centers_and_contexts` function below processes the corpus to generate training examples for word embedding models.\n",
    "\n",
    "**Core Idea**\n",
    "\n",
    "- Each word in the corpus is treated as a **center word**.\n",
    "- For every center word, a **context window size** is sampled **uniformly at random**\n",
    "  from the range: $\\{1, 2, \\ldots, \\text{max\\_window\\_size}\\}.$\n",
    "\n",
    "**Context Selection**\n",
    "\n",
    "Given a center word at position $t$ and a sampled window size $m$:\n",
    "- All words whose positions satisfy $|j - t| \\le m, \\quad j \\ne t$ are selected as its **context words**.\n",
    "This means the context window varies across training examples. **Each training example has its own context window size.**\n",
    "\n",
    "**Why Random Window Sizes?**\n",
    "\n",
    "- Introduces variability in context ranges\n",
    "- Prevents overemphasis on fixed-distance neighbors\n",
    "- Improves robustness and generalization of learned embeddings\n",
    "\n",
    "**Output**\n",
    "\n",
    "The function returns:\n",
    "- a list of **center words**\n",
    "- a corresponding list of **context word lists**\n",
    "\n",
    "Each center word is paired with its dynamically sampled context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa2ef3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "    \"\"\"Return center words and context words in skip-gram.\"\"\"\n",
    "    centers, contexts = [], []\n",
    "    for line in corpus:\n",
    "        # To form a \"center word -- context word\" pair, each sentence needs to\n",
    "        # have at least 2 words\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        centers += line\n",
    "        for i in range(len(line)):  # Context window centered at `i`\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, i - window_size),\n",
    "                                 min(len(line), i + 1 + window_size)))\n",
    "            # Exclude the center word from the context words\n",
    "            indices.remove(i)\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe940111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1]\n",
      "center 1 has contexts [0, 2, 3]\n",
      "center 2 has contexts [0, 1, 3, 4]\n",
      "center 3 has contexts [1, 2, 4, 5]\n",
      "center 4 has contexts [3, 5]\n",
      "center 5 has contexts [4, 6]\n",
      "center 6 has contexts [5]\n",
      "center 7 has contexts [8, 9]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [7, 8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc872271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# center-context pairs: 1497733'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When training on the PTB dataset, we set the maximum context window size to 5. The following extracts all the center words and their context words in the dataset.\n",
    "\n",
    "all_centers, all_contexts = get_centers_and_contexts(corpus, 5)\n",
    "f'# center-context pairs: {sum([len(contexts) for contexts in all_contexts])}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ee254",
   "metadata": {},
   "source": [
    "**Negative Sampling: Noise Word Generation**\n",
    "\n",
    "To enable **approximate training** with negative sampling, noise words must be drawn from a predefined probability distribution.\n",
    "\n",
    "**Core Idea**\n",
    "\n",
    "- Negative sampling requires repeatedly drawing **noise words** according to a fixed distribution.\n",
    "- The distribution is specified via **sampling weights**, which may be unnormalized.\n",
    "- Efficient sampling is crucial because noise words are generated frequently during training.\n",
    "\n",
    "**RandomGenerator class**\n",
    "\n",
    "this class provides an efficient mechanism to perform this sampling during training:\n",
    "- accept a list of `sampling_weights` (e.g., proportional to $f(w)^{0.75}$), `sampling_weights` define how likely each word is to be sampled.\n",
    "- efficiently sample word indices according to these weights,\n",
    "- avoid repeatedly recomputing the distribution.\n",
    "\n",
    "This abstraction allows negative sampling to be fast, reusable, and decoupled\n",
    "from the rest of the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ec791e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative sampling for picking noise words\n",
    "\n",
    "#@save\n",
    "class RandomGenerator:\n",
    "    \"\"\"Randomly draw among {1, ..., n} according to n sampling weights.\"\"\"\n",
    "    def __init__(self, sampling_weights):\n",
    "        # Exclude\n",
    "        self.population = list(range(1, len(sampling_weights) + 1))\n",
    "        self.sampling_weights = sampling_weights\n",
    "        self.candidates = []\n",
    "        self.i = 0\n",
    "\n",
    "    def draw(self):\n",
    "        if self.i == len(self.candidates):\n",
    "            # Cache `k` random sampling results\n",
    "            self.candidates = random.choices(self.population, self.sampling_weights, k=10000) # sample 10000 at first, later it will be used one by one\n",
    "            # print(self.candidates)\n",
    "            self.i = 0\n",
    "        self.i += 1\n",
    "        return self.candidates[self.i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "453e911e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 2, 1, 2, 1, 2, 3, 1, 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example, we can draw 10 random variables X among indices 1, 2, and 3 with sampling probabilities P(X=1)=2/9, P(X=2)=3/9, P(X=3)=4/9 as follows.\n",
    "\n",
    "generator = RandomGenerator([2, 3, 4])\n",
    "[generator.draw() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "091ed2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a pair of center word and context word, we randomly sample K (5 in the experiment) noise words. \n",
    "# In the word2vec paper, the sampling probability P(w) of a noise word is set to its relative frequency in the dictionary raised to the power of 0.75\n",
    "\n",
    "#@save\n",
    "def get_negatives(all_contexts, vocab, counter, K):\n",
    "    \"\"\"Return noise words in negative sampling.\"\"\"\n",
    "    # Sampling weights for words with indices 1, 2, ... (index 0 is the excluded unknown token) in the vocabulary\n",
    "    sampling_weights = [counter[vocab.to_tokens(i)]**0.75 for i in range(1, len(vocab))]\n",
    "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            neg = generator.draw()\n",
    "            # Noise words cannot be context words\n",
    "            if neg not in contexts:\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "all_negatives = get_negatives(all_contexts, vocab, counter, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531db90",
   "metadata": {},
   "source": [
    "Loading Training Examples in Minibatches\n",
    "\n",
    "After extracting **center words**, **context words**, and **negative samples**, the data must be organized into **minibatches** so it can be efficiently fed into the training loop.\n",
    "\n",
    "**Challenge**: Variable-Length Contexts\n",
    "\n",
    "- Each training example consists of:\n",
    "  - one center word\n",
    "  - a variable number of context words\n",
    "  - a variable number of negative samples\n",
    "- Because context window sizes vary, the total number of context + negative words differs across examples.\n",
    "\n",
    "**Key Idea**: Padding + Masking\n",
    "\n",
    "To batch examples together:\n",
    "1. Concatenate context words and negative words into one list:\n",
    "contexts_negatives\n",
    "2. Pad with zeros so all examples reach the same length:\n",
    "max_len = max length in the batch\n",
    "3. Create a mask to ignore padded positions during loss computation.\n",
    "\n",
    "For each minibatch:\n",
    "- **centers**  \n",
    "Center word indices (shape: `batch_size √ó 1`)\n",
    "\n",
    "- **contexts_negatives**  \n",
    "Concatenated context + negative word indices, padded to `max_len`\n",
    "\n",
    "- **masks**  \n",
    "Binary mask with:\n",
    "- `1` ‚Üí valid word\n",
    "- `0` ‚Üí padding (ignored in loss)\n",
    "\n",
    "- **labels**  \n",
    "Binary labels aligned with `contexts_negatives`:\n",
    "- `1` ‚Üí context word (positive example)\n",
    "- `0` ‚Üí negative sample\n",
    "\n",
    "Each element in `masks` and `labels` corresponds one-to-one with `contexts_negatives`.\n",
    "\n",
    "**Why Masks and Labels Are Needed**\n",
    "\n",
    "- **Masks** ensure padding tokens do not affect loss or gradients.\n",
    "- **Labels** distinguish:\n",
    "  - real context words (positive)\n",
    "  - noise words (negative)\n",
    "\n",
    "This setup supports **negative sampling loss** in minibatch form.\n",
    "\n",
    "**Implementation Note**\n",
    "\n",
    "A `batchify` function:\n",
    "- takes a list of examples `(center, context, negative)`\n",
    "- pads and packs them into tensors\n",
    "- returns all variables needed for training (`centers`, `contexts_negatives`, `masks`, `labels`)\n",
    "\n",
    "**Summary**\n",
    "\n",
    "- Minibatching requires handling variable-length context windows\n",
    "- Padding + masking makes batching possible\n",
    "- Labels separate positive and negative samples\n",
    "- This structure enables efficient training with negative sampling\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a734c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def batchify(data):\n",
    "    \"\"\"Return a minibatch of examples for skip-gram with negative sampling.\"\"\"\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).reshape((-1, 1)), \n",
    "            torch.tensor(contexts_negatives), \n",
    "            torch.tensor(masks),\n",
    "            torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "079aa4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers = tensor([[1],\n",
      "        [3]])\n",
      "contexts_negatives = tensor([[ 2,  4,  5,  6,  7,  8,  9, 10,  0,  0,  0,  0],\n",
      "        [ 1,  2,  4,  6,  7,  8,  9, 10, 11, 12, 13, 14]])\n",
      "masks = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "labels = tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x_1 = (1, [2, 4], [5, 6, 7, 8, 9, 10])               # (center, context, negative), len(negative) is usually = len(context) * K\n",
    "x_2 = (3, [1, 2, 4], [6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
    "batch = batchify((x_1, x_2))\n",
    "\n",
    "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
    "for name, data in zip(names, batch):\n",
    "    print(name, '=', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ead3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_ptb(batch_size, max_window_size, num_noise_words):\n",
    "    \"\"\"Download the PTB dataset and then load it into memory.\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    sentences = read_ptb()\n",
    "    vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "    subsampled, counter = subsample(sentences, vocab)\n",
    "    corpus = [vocab[line] for line in subsampled]\n",
    "    all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)\n",
    "    all_negatives = get_negatives(all_contexts, vocab, counter, num_noise_words)\n",
    "\n",
    "    class PTBDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, centers, contexts, negatives):\n",
    "            assert len(centers) == len(contexts) == len(negatives)\n",
    "            self.centers = centers\n",
    "            self.contexts = contexts\n",
    "            self.negatives = negatives\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.centers)\n",
    "\n",
    "    dataset = PTBDataset(all_centers, all_contexts, all_negatives)\n",
    "\n",
    "    data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                                      collate_fn=batchify,\n",
    "                                      num_workers=num_workers)\n",
    "    return data_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64b27e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/pytorch-basics/.venv310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "data_iter, vocab = load_data_ptb(batch_size = 512, max_window_size = 5, num_noise_words = 5)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(names, batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c4d6f7",
   "metadata": {},
   "source": [
    "- context_size max is: 2 * max_window_size = 10\n",
    "- contexts_negatives = context words + negative words = context_size + max_window_size * context_size = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf2382",
   "metadata": {},
   "source": [
    "======================================Pretraining word2vec============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32163f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size, max_window_size, num_noise_words = 512, 5, 5\n",
    "data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size, num_noise_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0afdc65",
   "metadata": {},
   "source": [
    "An **embedding layer** maps token indices to dense vector representations. It is a lookup table: indices in, vectors out ‚Äî with one vector per token.\n",
    "\n",
    "- The input to an embedding layer is a tensor of **token indices**.\n",
    "- Each token index $i$ corresponds to the $i$-th row of the embedding\n",
    "  weight matrix.\n",
    "- The embedding dimension is set by `output_dim`.\n",
    "\n",
    "If:\n",
    "- input shape = $(2, 3)$ (two examples, three tokens each)\n",
    "- embedding dimension = $4$\n",
    "\n",
    "Then:\n",
    "- output shape = $(2, 3, 4)$\n",
    "\n",
    "Each token index is replaced by its corresponding 4-dimensional embedding vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "915dc322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter embedding_weight (torch.Size([20, 4]), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "embed = nn.Embedding(num_embeddings=20, embedding_dim=4) # (vocab_size, embed_size)\n",
    "print(f'Parameter embedding_weight ({embed.weight.shape}, 'f'dtype={embed.weight.dtype})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bd181ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3308, -0.3372, -0.8600, -0.7603],\n",
       "         [-0.3090,  1.6024,  0.8086,  0.6145],\n",
       "         [-0.5710, -2.2856,  0.0814, -0.6121]],\n",
       "\n",
       "        [[ 1.0145, -0.7849, -0.8184,  0.1040],\n",
       "         [ 0.0043, -1.2932,  0.6911, -1.7616],\n",
       "         [-0.2732, -0.4100,  0.4556,  0.1090]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]]) # (2, 3) -> (2, 3, 4)\n",
    "embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75158d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    # center: (batch size, 1)\n",
    "    # contexts_and_negatives: (batch size, max_len)\n",
    "    v = embed_v(center) # (batch size, 1, embed_size)\n",
    "    u = embed_u(contexts_and_negatives) # (batch size, max_len, embed_size)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1)) # (batch size, 1, max_len)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79247bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram(torch.ones((2, 1), dtype=torch.long),\n",
    "          torch.ones((2, 4), dtype=torch.long), embed, embed).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a1352e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBCELoss(nn.Module):\n",
    "    # Binary cross-entropy loss with masking\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, target, mask=None):\n",
    "        out = nn.functional.binary_cross_entropy_with_logits(inputs, target, weight=mask, reduction=\"none\")\n",
    "        return out.mean(dim=1)\n",
    "\n",
    "loss = SigmoidBCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7e850f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9352, 1.8462])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([[1.1, -2.2, 3.3, -4.4]] * 2)\n",
    "label = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0]])\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d086ef09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9352\n",
      "1.8462\n"
     ]
    }
   ],
   "source": [
    "def sigmd(x):\n",
    "    return -math.log(1 / (1 + math.exp(-x)))\n",
    "\n",
    "print(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')\n",
    "print(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92cff6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two embedding layers for all the words in the vocabulary when they are used as center words and context words, respectively. \n",
    "\n",
    "embed_size = 100\n",
    "net = nn.Sequential(nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_size), # center word embedding\n",
    "                    nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_size)) # context word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a6ad19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):\n",
    "    def init_weights(module):\n",
    "        if type(module) == nn.Embedding:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "    net.apply(init_weights)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, num_epochs])\n",
    "    # Sum of normalized losses, no. of normalized losses\n",
    "    metric = d2l.Accumulator(2)\n",
    "    for epoch in range(num_epochs):\n",
    "        timer, num_batches = d2l.Timer(), len(data_iter)\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            center, context_negative, mask, label = [data.to(device) for data in batch]\n",
    "\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            l = (loss(pred.reshape(label.shape).float(), label.float(), mask) / mask.sum(axis=1) * mask.shape[1])\n",
    "            l.sum().backward()\n",
    "            optimizer.step()\n",
    "            metric.add(l.sum(), l.numel())\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, '\n",
    "          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a9591c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.410, 71561.4 tokens/sec on cpu\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"255.825pt\" height=\"183.35625pt\" viewBox=\"0 0 255.825 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-12-31T17:20:48.032350</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 255.825 183.35625 \n",
       "L 255.825 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "L 50.14375 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 50.14375 145.8 \n",
       "L 50.14375 7.2 \n",
       "\" clip-path=\"url(#p2cbc4c725b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m6d445b9f0e\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d445b9f0e\" x=\"50.14375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(46.9625 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 98.96875 145.8 \n",
       "L 98.96875 7.2 \n",
       "\" clip-path=\"url(#p2cbc4c725b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d445b9f0e\" x=\"98.96875\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(95.7875 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 147.79375 145.8 \n",
       "L 147.79375 7.2 \n",
       "\" clip-path=\"url(#p2cbc4c725b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d445b9f0e\" x=\"147.79375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(144.6125 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 196.61875 145.8 \n",
       "L 196.61875 7.2 \n",
       "\" clip-path=\"url(#p2cbc4c725b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d445b9f0e\" x=\"196.61875\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(193.4375 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" clip-path=\"url(#p2cbc4c725b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d445b9f0e\" x=\"245.44375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(242.2625 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(132.565625 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 50.14375 145.714607 \n",
       "L 245.44375 145.714607 \n",
       "\" clip-path=\"url(#p2cbc4c725b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"m1a2a0d7b75\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1a2a0d7b75\" x=\"50.14375\" y=\"145.714607\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.40 -->\n",
       "      <g transform=\"translate(20.878125 149.513825) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 50.14375 115.068084 \n",
       "L 245.44375 115.068084 \n",
       "\" clip-path=\"url(#p2cbc4c725b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1a2a0d7b75\" x=\"50.14375\" y=\"115.068084\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.45 -->\n",
       "      <g transform=\"translate(20.878125 118.867303) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 50.14375 84.421562 \n",
       "L 245.44375 84.421562 \n",
       "\" clip-path=\"url(#p2cbc4c725b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1a2a0d7b75\" x=\"50.14375\" y=\"84.421562\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(20.878125 88.22078) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 50.14375 53.775039 \n",
       "L 245.44375 53.775039 \n",
       "\" clip-path=\"url(#p2cbc4c725b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1a2a0d7b75\" x=\"50.14375\" y=\"53.775039\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.55 -->\n",
       "      <g transform=\"translate(20.878125 57.574258) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 50.14375 23.128517 \n",
       "L 245.44375 23.128517 \n",
       "\" clip-path=\"url(#p2cbc4c725b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1a2a0d7b75\" x=\"50.14375\" y=\"23.128517\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.60 -->\n",
       "      <g transform=\"translate(20.878125 26.927736) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798437 86.157813) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_21\">\n",
       "    <path d=\"M 11.025364 13.5 \n",
       "L 20.731979 60.594416 \n",
       "L 30.438593 79.547321 \n",
       "L 40.145207 89.641284 \n",
       "L 49.851822 96.097999 \n",
       "L 50.14375 96.25451 \n",
       "L 59.850364 101.079629 \n",
       "L 69.556979 104.780898 \n",
       "L 79.263593 107.806061 \n",
       "L 88.970207 110.430808 \n",
       "L 98.676822 112.730871 \n",
       "L 98.96875 112.792858 \n",
       "L 108.675364 115.242135 \n",
       "L 118.381979 117.451764 \n",
       "L 128.088593 119.448412 \n",
       "L 137.795207 121.270359 \n",
       "L 147.501822 122.967486 \n",
       "L 147.79375 123.013736 \n",
       "L 157.500364 125.089357 \n",
       "L 167.206979 126.957006 \n",
       "L 176.913593 128.669909 \n",
       "L 186.620207 130.248371 \n",
       "L 196.326822 131.709957 \n",
       "L 196.61875 131.751147 \n",
       "L 206.325364 133.633839 \n",
       "L 216.031979 135.314558 \n",
       "L 225.738593 136.834322 \n",
       "L 235.445207 138.206713 \n",
       "L 245.151822 139.466392 \n",
       "L 245.44375 139.5 \n",
       "\" clip-path=\"url(#p2cbc4c725b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 50.14375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 7.2 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p2cbc4c725b\">\n",
       "   <rect x=\"50.14375\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr, num_epochs = 0.002, 5\n",
    "train(net, data_iter, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be72574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.699: microprocessor\n",
      "cosine sim=0.699: drives\n",
      "cosine sim=0.681: chips\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[vocab[query_token]]\n",
    "    # Compute the cosine similarity. Add 1e-9 for numerical stability\n",
    "    # torch.mv: matrix-vector product\n",
    "    cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9)\n",
    "    topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')\n",
    "    for i in topk[1:]:  # Remove the input words\n",
    "        print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')\n",
    "\n",
    "get_similar_tokens('chip', 3, net[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa94ad",
   "metadata": {},
   "source": [
    "=====================Word Embedding with Global Vectors (GloVe)========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f355bc94",
   "metadata": {},
   "source": [
    "#### Skip-Gram with Global Corpus Statistics (SG-GCS)\n",
    "\n",
    "##### Overview\n",
    "- Defines $ q_{ij} = P(w_j \\mid w_i) $, the conditional probability of context word $ w_j $ given center word $ w_i $.  \n",
    "- Computed via softmax: $ q_{ij} = \\frac{\\exp(u_j \\cdot v_i)}{\\sum_k \\exp(u_k \\cdot v_i)} $\n",
    "\n",
    "##### Key Idea\n",
    "- Uses **global co-occurrence statistics** rather than sliding local windows.  \n",
    "- Let $ x_{ij} $ be the number of times $ w_j $ appears in the context of $ w_i $.  \n",
    "- Objective:\n",
    "  $ J = - \\sum_i \\sum_j x_{ij} \\log q_{ij} $\n",
    "- Equivalent form:\n",
    "  $ J = - \\sum_i x_i \\sum_j p_{ij} \\log q_{ij}, \\quad p_{ij} = \\frac{x_{ij}}{x_i} $\n",
    "\n",
    "##### Interpretation\n",
    "- $ \\sum_j p_{ij} \\log q_{ij} $ is the **cross-entropy** between:\n",
    "  - empirical distribution $ p_{ij} $ (from corpus counts), and  \n",
    "  - model distribution $ q_{ij} $, weighted by $ x_i $.\n",
    "- Minimizing this loss aligns model predictions with corpus statistics.\n",
    "\n",
    "- Disadvantages:\n",
    "  - Full softmax over the vocabulary is computationally expensive.\n",
    "  - Rare words can dominate the loss:\n",
    "    - The loss sums over **all observed (center, context) pairs**.\n",
    "    - Many rare words appear only once or twice.\n",
    "    - For rare pairs, $ \\hat{p}(c \\mid w) $ is very small, so $ -\\log \\hat{p}(c \\mid w) $ is large.\n",
    "    - Despite being rare, such events produce large gradients.\n",
    "    - In large corpora, many rare pairs accumulate ‚Üí unstable or skewed embeddings.\n",
    "\n",
    "#### The GloVe Model\n",
    "\n",
    "##### Motivation\n",
    "The **GloVe (Global Vectors)** model addresses these issues by:\n",
    "- replacing cross-entropy with a **squared loss**, and  \n",
    "- directly modeling **log co-occurrence counts / ratios**, rather than probabilities.\n",
    "\n",
    "##### Main Changes\n",
    "1. Replace probabilities with co-occurrence counts:\n",
    "   $ f_{ij} = x_{ij}, \\quad \\hat{f}_{ij} = \\exp(u_j \\cdot v_i) $\n",
    "\n",
    "2. Use a **squared error objective**:\n",
    "   $ J = (u_j \\cdot v_i + b_i + c_j - \\log x_{ij})^2 $\n",
    "\n",
    "3. Add a **weighting function** to control frequent and rare pairs:\n",
    "   $$\n",
    "   h(x_{ij}) =\n",
    "   \\begin{cases}\n",
    "   (x_{ij}/a)^\\alpha, & x_{ij} < a \\\\\n",
    "   1, & \\text{otherwise}\n",
    "   \\end{cases}\n",
    "   $$\n",
    "   Typical values: $ a = 100 $, $ \\alpha = 0.75 $.\n",
    "\n",
    "##### Final GloVe Loss Function\n",
    "$ J = \\sum_{i,j} h(x_{ij}) \\, (u_j \\cdot v_i + b_i + c_j - \\log x_{ij})^2 $\n",
    "\n",
    "##### Intuition\n",
    "- GloVe learns embeddings where **vector differences encode semantic relationships**.  \n",
    "- The weighting function stabilizes training by:\n",
    "  - down-weighting very rare pairs, and  \n",
    "  - preventing extremely frequent pairs from dominating.\n",
    "- The formulation is symmetric in $ (w_i, w_j) $, yielding comparable embeddings when roles are swapped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57638bcb",
   "metadata": {},
   "source": [
    "#### Interpreting GloVe from the Ratio of Co-occurrence Probabilities\n",
    "\n",
    "##### Core Concept\n",
    "Let $p_{ij} = P(w_j \\mid w_i)$ be the conditional probability of word $w_j$ appearing in the context of $w_i$.  \n",
    "The **ratio** $p_{ij} / p_{ik}$ captures how strongly different context words distinguish the meaning of $w_i$.\n",
    "\n",
    "##### Example\n",
    "| Context Word | solid | gas | water | fashion |\n",
    "|---------------|-------|-----|--------|----------|\n",
    "| $p_1 = P(w_j \\mid ice)$   | 0.00019 | 0.000066 | 0.003  | 0.000017 |\n",
    "| $p_2 = P(w_j \\mid steam)$ | 0.000022| 0.00078  | 0.0022 | 0.000018 |\n",
    "| Ratio ($p_1/p_2$)         | 8.9     | 0.085    | 1.36   | 0.96     |\n",
    "\n",
    "Observations:\n",
    "- **solid** is strongly associated with *ice* ‚Üí large ratio (8.9).\n",
    "- **gas** is strongly associated with *steam* ‚Üí small ratio (0.085).\n",
    "- **water** relates to both ‚Üí ratio ‚âà 1.\n",
    "- **fashion** is unrelated ‚Üí ratio ‚âà 1 but semantically uninformative.\n",
    "\n",
    "##### Mathematical View\n",
    "We want a function $f$ such that:\n",
    "$$\n",
    "f(w_i, w_j, w_k) \\approx \\frac{P_{ij}}{P_{ik}}\n",
    "$$\n",
    "\n",
    "A natural exponential form using embeddings is:\n",
    "$$\n",
    "f(w_i, w_j, w_k)\n",
    "= \\frac{\\exp(u_i^\\top v_j)}{\\exp(u_i^\\top v_k)}\n",
    "= \\exp\\!\\big((v_j - v_k)^\\top u_i\\big)\n",
    "$$\n",
    "\n",
    "Equating both sides:\n",
    "$$\n",
    "\\exp\\!\\big((v_j - v_k)^\\top u_i\\big) \\approx \\frac{P_{ij}}{P_{ik}}\n",
    "$$\n",
    "\n",
    "Taking logs:\n",
    "$$\n",
    "(v_j - v_k)^\\top u_i = \\log P_{ij} - \\log P_{ik}\n",
    "$$\n",
    "\n",
    "This implies each dot product should match log co-occurrence probabilities (up to bias):\n",
    "$$\n",
    "u_i^\\top v_j + b_i + c_j \\approx \\log P_{ij}\n",
    "$$\n",
    "\n",
    "Since $P_{ij} = \\frac{x_{ij}}{x_i}$, the normalization term $\\log x_i$ can be absorbed into $b_i$:\n",
    "$$\n",
    "u_i^\\top v_j + b_i + c_j \\approx \\log x_{ij}\n",
    "$$\n",
    "\n",
    "##### Learning Objective\n",
    "To satisfy this relation across all word pairs, GloVe minimizes a **weighted least-squares loss**:\n",
    "$$\n",
    "J = \\sum_{i,j} f(x_{ij}) \\, (u_i^\\top v_j + b_i + c_j - \\log x_{ij})^2\n",
    "$$\n",
    "\n",
    "The weighting function limits extreme frequencies:\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "(x / x_{\\text{max}})^\\alpha, & x < x_{\\text{max}} \\\\\n",
    "1, & x \\ge x_{\\text{max}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Common values: $x_{\\text{max}} = 100$, $\\alpha = 0.75$.\n",
    "\n",
    "##### Intuition\n",
    "- GloVe models **semantic meaning via ratios of co-occurrence probabilities**, not raw probabilities.\n",
    "- Embeddings are learned so **vector differences encode log co-occurrence ratios**, e.g.  \n",
    "  $$\n",
    "  \\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}\n",
    "  $$\n",
    "  $$\n",
    "  \\text{ice} - \\text{steam} + \\text{solid} \\approx \\text{gas}\n",
    "  $$\n",
    "- **Why log ratios?**\n",
    "  - Convert multiplicative relationships into linear ones\n",
    "  - Reduce sensitivity to extreme probabilities\n",
    "- **Why GloVe > SG-GCS**\n",
    "  - Smooth, weighted loss (robust to rare counts)\n",
    "  - No expensive global softmax\n",
    "  - Directly captures semantic regularities\n",
    "  - Trains efficiently from the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dabc470",
   "metadata": {},
   "source": [
    "=========================================Subword Embedding======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664fa22b",
   "metadata": {},
   "source": [
    "#### The fastText Model\n",
    "\n",
    "##### Key Idea\n",
    "- In **word2vec**, each *word* is assigned a single embedding vector.\n",
    "- In **fastText**, each *word* is represented as the **sum of its character n-gram (subword) embeddings**.\n",
    "\n",
    "This design captures **morphological structure** (prefixes, suffixes, roots) and enables meaningful representations for **rare or unseen words**.\n",
    "\n",
    "##### How Subwords Are Built\n",
    "1. Add boundary symbols `<` and `>` to mark word start and end.  \n",
    "   Example: `\"where\"` ‚Üí `\"<where>\"`.\n",
    "2. Extract all character n-grams of lengths typically between 3 and 6:\n",
    "   - Examples: `\"<wh\"`, `\"whe\"`, `\"her\"`, `\"ere\"`, `\"re>\"`, `\"wher\"`, `\"here\"`, `\"<whe\"`, ‚Ä¶\n",
    "3. Each distinct n-gram is treated as a **subword unit** and assigned its own embedding vector.\n",
    "\n",
    "> Subwords are **not linguistically predefined**.  \n",
    "> They are generated deterministically from raw character strings and learned purely from data.\n",
    "\n",
    "##### How Subwords Are Discovered / Learned\n",
    "- fastText does **not** discover subwords via clustering or segmentation.\n",
    "- Instead:\n",
    "  - All possible n-grams in the chosen length range are generated.\n",
    "  - Each n-gram is mapped (often via hashing) to an embedding index.\n",
    "  - During training, gradients flow to subword embeddings whenever their parent word appears.\n",
    "- Over time, useful subwords (e.g., ‚Äúing‚Äù, ‚Äútion‚Äù, ‚Äúun‚Äù, ‚Äúpre‚Äù) acquire meaningful vectors because they recur across many words.\n",
    "\n",
    "##### Word Representation\n",
    "If a word $w$ has a set of subwords $G_w$, its embedding is: $ v_w = \\sum_{g \\in G_w} z_g $, where $z_g$ is the embedding of subword $g$.\n",
    "Thus, a word‚Äôs meaning is the **sum of its subword meanings**.\n",
    "\n",
    "##### Training Objective\n",
    "- fastText uses the **same objectives as word2vec**:\n",
    "  - Skip-gram with negative sampling (most common)\n",
    "  - CBOW\n",
    "- The only change is that the *input word vector* is replaced by the sum of its subword vectors.\n",
    "\n",
    "##### Advantages\n",
    "- **Robust to rare and OOV words**:  \n",
    "  Even unseen words can be embedded via known subwords.\n",
    "- **Parameter sharing across morphology**:  \n",
    "  Words like *run*, *running*, *runner*, *runs* share many subwords ‚Üí better generalization.\n",
    "- **Captures spelling‚Äìmeaning correlations**, useful for morphologically rich languages.\n",
    "\n",
    "##### Trade-offs\n",
    "- **Higher memory usage**: many subword embeddings.\n",
    "- **More computation**: word vectors require summing multiple n-gram vectors at each update.\n",
    "- Uses hashing to control vocabulary size, which can introduce collisions (usually minor in practice)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e09d29",
   "metadata": {},
   "source": [
    "#### Byte Pair Encoding (BPE)\n",
    "\n",
    "##### Motivation\n",
    "FastText fixes subword length (e.g., 3‚Äì6 characters).  \n",
    "To allow *variable-length subwords* and a *controlled vocabulary size*, **Byte Pair Encoding (BPE)** is used.\n",
    "\n",
    "##### How BPE Works\n",
    "1. **Start** with all characters as basic symbols (e.g., `'a', 'b', ..., 'z', '_', [UNK]`).\n",
    "2. **Count frequencies** of all symbol pairs (like `'t'` + `'h'` ‚Üí `'th'`).\n",
    "3. **Merge the most frequent pair** into a new symbol.\n",
    "4. **Repeat** until you reach a target vocabulary size.\n",
    "\n",
    "The result is a vocabulary of subwords that frequently appear together, efficiently capturing both characters and common morphemes.\n",
    "\n",
    "##### Example (simplified)\n",
    "Initial symbols: `[\"l\", \"o\", \"w\", \"e\", \"r\"]`\n",
    "\n",
    "Frequent pairs:\n",
    "- merge `(\"l\", \"o\")` ‚Üí `\"lo\"`\n",
    "- merge `(\"lo\", \"w\")` ‚Üí `\"low\"`\n",
    "\n",
    "Final tokens may include `\"low\"`, `\"er\"`, `\"low_er\"` etc.\n",
    "\n",
    "##### Why BPE Matters\n",
    "- Produces a **compact, data-driven subword vocabulary**.  \n",
    "- Handles both **rare words** and **morphologically rich languages**.\n",
    "- Used in **modern NLP models** (e.g., GPT-2, RoBERTa) to segment text into efficient subword units.\n",
    "\n",
    "#### Insight\n",
    "Both **fastText** and **BPE** are subword-based solutions to the same problem:  \n",
    "> How to represent words that are rare, unseen, or morphologically complex.\n",
    "\n",
    "- **fastText**: Summation of character n-gram vectors ‚Üí captures morphology in embeddings.  \n",
    "- **BPE**: Learns optimal subword units via data compression ‚Üí efficient tokenization for large-scale NLP models.\n",
    "\n",
    "#### Intuitive Comparison\n",
    "\n",
    "| Feature | fastText | BPE |\n",
    "|----------|-----------|-----|\n",
    "| Subword type | Fixed-length character n-grams | Variable-length learned units |\n",
    "| Handles OOV words | Yes | Yes |\n",
    "| Vocabulary size | Larger, unbounded | Controlled, predefined |\n",
    "| Used in | Classical word embeddings | Modern Transformer tokenization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c0e9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "           '_', '[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb2527ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\n",
    "token_freqs = {}\n",
    "for token, freq in raw_token_freqs.items():\n",
    "    token_freqs[' '.join(list(token))] = raw_token_freqs[token]\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f94d57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_freq_pair(token_freqs):\n",
    "    print(\"token_freqs:\", token_freqs)\n",
    "\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for token, freq in token_freqs.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # key is a tuple of two consecutive symbols\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "\n",
    "    max_pair = max(pairs, key=pairs.get)\n",
    "    max_value = pairs[max_pair]\n",
    "\n",
    "    print(f\"Max pair: {max_pair}, Frequency: {max_value}\")\n",
    "\n",
    "    return max_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0534ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_symbols(max_freq_pair, token_freqs, symbols):\n",
    "    symbols.append(''.join(max_freq_pair))\n",
    "    new_token_freqs = dict()\n",
    "    for token, freq in token_freqs.items():\n",
    "        new_token = token.replace(' '.join(max_freq_pair), ''.join(max_freq_pair))\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "    return new_token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61fda373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_freqs: {'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}\n",
      "Max pair: ('t', 'a'), Frequency: 9\n",
      "merge #1 as frequency is: ('t', 'a') {'f a s t _': 4, 'f a s t e r _': 3, 'ta l l _': 5, 'ta l l e r _': 4}\n",
      "\n",
      "token_freqs: {'f a s t _': 4, 'f a s t e r _': 3, 'ta l l _': 5, 'ta l l e r _': 4}\n",
      "Max pair: ('ta', 'l'), Frequency: 9\n",
      "merge #2 as frequency is: ('ta', 'l') {'f a s t _': 4, 'f a s t e r _': 3, 'tal l _': 5, 'tal l e r _': 4}\n",
      "\n",
      "token_freqs: {'f a s t _': 4, 'f a s t e r _': 3, 'tal l _': 5, 'tal l e r _': 4}\n",
      "Max pair: ('tal', 'l'), Frequency: 9\n",
      "merge #3 as frequency is: ('tal', 'l') {'f a s t _': 4, 'f a s t e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "\n",
      "token_freqs: {'f a s t _': 4, 'f a s t e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "Max pair: ('f', 'a'), Frequency: 7\n",
      "merge #4 as frequency is: ('f', 'a') {'fa s t _': 4, 'fa s t e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "\n",
      "token_freqs: {'fa s t _': 4, 'fa s t e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "Max pair: ('fa', 's'), Frequency: 7\n",
      "merge #5 as frequency is: ('fa', 's') {'fas t _': 4, 'fas t e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "\n",
      "token_freqs: {'fas t _': 4, 'fas t e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "Max pair: ('fas', 't'), Frequency: 7\n",
      "merge #6 as frequency is: ('fas', 't') {'fast _': 4, 'fast e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "\n",
      "token_freqs: {'fast _': 4, 'fast e r _': 3, 'tall _': 5, 'tall e r _': 4}\n",
      "Max pair: ('e', 'r'), Frequency: 7\n",
      "merge #7 as frequency is: ('e', 'r') {'fast _': 4, 'fast er _': 3, 'tall _': 5, 'tall er _': 4}\n",
      "\n",
      "token_freqs: {'fast _': 4, 'fast er _': 3, 'tall _': 5, 'tall er _': 4}\n",
      "Max pair: ('er', '_'), Frequency: 7\n",
      "merge #8 as frequency is: ('er', '_') {'fast _': 4, 'fast er_': 3, 'tall _': 5, 'tall er_': 4}\n",
      "\n",
      "token_freqs: {'fast _': 4, 'fast er_': 3, 'tall _': 5, 'tall er_': 4}\n",
      "Max pair: ('tall', '_'), Frequency: 5\n",
      "merge #9 as frequency is: ('tall', '_') {'fast _': 4, 'fast er_': 3, 'tall_': 5, 'tall er_': 4}\n",
      "\n",
      "token_freqs: {'fast _': 4, 'fast er_': 3, 'tall_': 5, 'tall er_': 4}\n",
      "Max pair: ('fast', '_'), Frequency: 4\n",
      "merge #10 as frequency is: ('fast', '_') {'fast_': 4, 'fast er_': 3, 'tall_': 5, 'tall er_': 4}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\n",
    "    print(f'merge #{i + 1} as frequency is:', max_freq_pair, token_freqs)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb042289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\n"
     ]
    }
   ],
   "source": [
    "print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "076a1991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fast_', 'fast er_', 'tall_', 'tall er_']\n"
     ]
    }
   ],
   "source": [
    "print(list(token_freqs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcb4ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_BPE(tokens, symbols):\n",
    "    outputs = []\n",
    "    for token in tokens:\n",
    "        start, end = 0, len(token)\n",
    "        cur_output = []\n",
    "        # Segment token with the longest possible subwords from symbols\n",
    "        while start < len(token) and start < end:\n",
    "            if token[start: end] in symbols:\n",
    "                cur_output.append(token[start: end])\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            else:\n",
    "                end -= 1\n",
    "        if start < len(token):\n",
    "            cur_output.append('[UNK]')\n",
    "        outputs.append(' '.join(cur_output))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21fe0dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tall e s t _', 'fa t t er_']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['tallest_', 'fatter_']\n",
    "print(segment_BPE(tokens, symbols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31e028",
   "metadata": {},
   "source": [
    "=================================================Word Similarity and Analogy================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a098d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b8bd471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',\n",
    "                                '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',\n",
    "                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',\n",
    "                                  'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',\n",
    "                           'c1816da3821ae9f43899be655002f6c723e91b88')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8921aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class TokenEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        data_dir = d2l.download_extract(embedding_name)\n",
    "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText website: https://fasttext.cc/\n",
    "        with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
    "                   for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38ea6f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6b50d = TokenEmbedding('glove.6b.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e36e405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400001"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef814dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3367, 'beautiful')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29d9b000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3367, 'beautiful')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0c19b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(W, x, k):\n",
    "    # Add 1e-9 for numerical stability\n",
    "    cos = torch.mv(W, x.reshape(-1,)) / (\n",
    "        torch.sqrt(torch.sum(W * W, axis=1) + 1e-9) *\n",
    "        torch.sqrt((x * x).sum()))\n",
    "    _, topk = torch.topk(cos, k=k)\n",
    "    return topk, [cos[int(i)] for i in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f187d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)\n",
    "    for i, c in zip(topk[1:], cos[1:]):  # Exclude the input word\n",
    "        print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47aa34cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.782: chips\n",
      "cosine sim=0.668: semiconductor\n",
      "cosine sim=0.656: intel\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('chip', 3, glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23b9fe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.787: babies\n",
      "cosine sim=0.752: newborn\n",
      "cosine sim=0.751: boy\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('baby', 3, glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fcb03e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.891: lovely\n",
      "cosine sim=0.872: gorgeous\n",
      "cosine sim=0.808: wonderful\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('beautiful', 3, glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b86197d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(token_a, token_b, token_c, embed):\n",
    "    vecs = embed[[token_a, token_b, token_c]]\n",
    "    x = vecs[1] - vecs[0] + vecs[2]\n",
    "    topk, cos = knn(embed.idx_to_vec, x, 1)\n",
    "    return embed.idx_to_token[int(topk[0])]  # Remove unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "abcb4c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daughter'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('man', 'woman', 'son', glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ba07e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'japan'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('beijing', 'china', 'tokyo', glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5fbab5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biggest'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('bad', 'worst', 'big', glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6dbb319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('do', 'did', 'go', glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93387e",
   "metadata": {},
   "source": [
    "==============================================Bidirectional Encoder Representations from Transformers (BERT)==========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad0d36",
   "metadata": {},
   "source": [
    "##### From Context-Independent to Context-Sensitive\n",
    "Traditional models like **word2vec** and **GloVe** assign a single vector to each word, regardless of context (e.g., ‚Äúbank‚Äù is identical in ‚Äúriver bank‚Äù and ‚Äúbank loan‚Äù).\n",
    "\n",
    "**Context-sensitive representations** address this by making each token‚Äôs embedding depend on surrounding words.\n",
    "\n",
    "Examples:\n",
    "- **TagLM** ‚Äì augments existing models with language-model embeddings.\n",
    "- **CoVe** ‚Äì uses encoder states from machine translation models.\n",
    "- **ELMo** ‚Äì applies deep bidirectional LSTMs to combine left and right context, producing embeddings that vary by context; improves tasks like sentiment analysis, NER, and QA.\n",
    "\n",
    "##### From Task-Specific to Task-Agnostic\n",
    "Although ELMo boosts many NLP tasks, it must still be integrated into each task-specific model.\n",
    "\n",
    "**GPT** introduced a task-agnostic pretraining approach:\n",
    "- Pretrains a Transformer on large text corpora via next-token prediction.\n",
    "- Fine-tunes the same model for downstream tasks with minimal changes.\n",
    "- Uses a left-to-right (unidirectional) language model, so each token depends only on left context.\n",
    "\n",
    "##### BERT: Combining the Best of Both Worlds\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)** merges ELMo‚Äôs bidirectional context with GPT‚Äôs task-agnostic fine-tuning.\n",
    "\n",
    "Key ideas:\n",
    "- Uses a bidirectional Transformer encoder to capture both left and right context.\n",
    "- Adapts to downstream tasks by adding a small output layer, keeping architecture changes minimal.\n",
    "\n",
    "Compared to GPT, BERT:\n",
    "- Models bidirectional context (vs. left-to-right only).\n",
    "- Fine-tunes all Transformer parameters for each task.\n",
    "\n",
    "This enables strong performance across many NLP tasks, including classification, question answering, and inference.\n",
    "\n",
    "**In short:**\n",
    "- **ELMo** ‚Üí contextual embeddings added to existing models.\n",
    "- **GPT** ‚Üí task-agnostic, left-to-right Transformer with fine-tuning.\n",
    "- **BERT** ‚Üí bidirectional Transformer, pretrain once, fine-tune easily ‚Äî the best of both worlds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab643686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a20cee",
   "metadata": {},
   "source": [
    "Each input token embedding is the sum of:\n",
    "- **Token embedding** (word/subword)\n",
    "- **Segment embedding** (sentence A or B)\n",
    "- **Positional embedding** (token position in sequence)\n",
    "\n",
    "![BERT Input](./Images/BERT_input.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09c6043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"Get tokens of the BERT input sequence and their segment IDs.\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0 and 1 are marking segment A and B, respectively\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1) # 0 0 0 0 1 1 1 1 1 1\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0817a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                 num_blks, dropout, max_len=1000, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", d2l.TransformerEncoderBlock(\n",
    "                num_hiddens, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # In BERT, positional embeddings are learnable, thus we create a\n",
    "        # parameter of positional embeddings that are long enough\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
    "                                                      num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # Shape of `X` remains unchanged in the following code snippet:\n",
    "        # (batch size, max sequence length, `num_hiddens`)\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4076c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "ffn_num_input, num_blks, dropout = 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eb02ba69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.randint(0, vocab_size, (2, 8)) # (2: batch size, 8: sequence length)\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]]) # Each batch element contains 2 sentences, packed into a single sequence using segment IDs.\n",
    "# Batch 0:\n",
    "# - Tokens with segment `0` ‚Üí sentence A\n",
    "# - Tokens with segment `1` ‚Üí sentence B\n",
    "# Batch 1:\n",
    "# - Tokens with segment `0` ‚Üí sentence A\n",
    "# - Tokens with segment `1` ‚Üí sentence B\n",
    "\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4803fed9",
   "metadata": {},
   "source": [
    "#### Masked Language Modeling (MLM) in BERT Pretraining\n",
    "\n",
    "##### 1. The Training Input: Masking Strategy\n",
    "\n",
    "BERT takes plain text (no labels) and creates its own supervised learning problem by masking.\n",
    "\n",
    "Example sentence:: ‚ÄúThe quick brown fox jumps over the lazy dog.‚Äù\n",
    "\n",
    "BERT randomly picks **15% of tokens** for possible masking:\n",
    "\n",
    "| Tokens | [The] | [quick] | [brown] | [fox] | [jumps] | [over] | [the] | [lazy] | [dog] |\n",
    "|---------|-------|---------|---------|-------|---------|--------|-------|--------|-------|\n",
    "| Mask?   | -     | ‚úîÔ∏è      | -       | -     | ‚úîÔ∏è      | -      | -     | ‚úîÔ∏è     | -     |\n",
    "\n",
    "Then for those selected tokens (15%):\n",
    "\n",
    "- **80% of the time ‚Üí replace with `[MASK]`**  \n",
    "  e.g. ‚ÄúThe quick brown fox `[MASK]` over the `[MASK]` dog‚Äù\n",
    "\n",
    "- **10% of the time ‚Üí replace with a random word**  \n",
    "  e.g. ‚ÄúThe quick brown fox *eats* over the `[MASK]` dog‚Äù\n",
    "\n",
    "- **10% of the time ‚Üí keep the same word**  \n",
    "  (This keeps the model from relying on the presence of `[MASK]` too literally.)\n",
    "\n",
    "##### 2. Model Forward Pass\n",
    "\n",
    "Each token (wordpiece) is converted to an **embedding vector**, plus **position** and **segment embeddings**.  \n",
    "These go through multiple **Transformer encoder layers** (attention + feedforward blocks).\n",
    "\n",
    "The output is a **contextual embedding** for every token ‚Äî meaning each position‚Äôs vector encodes context from the *entire sentence* (both left and right).\n",
    "\n",
    "##### 3. The ‚ÄúSupervision‚Äù Signal\n",
    "\n",
    "Even though no human labels are provided, BERT **creates supervision automatically**:\n",
    "\n",
    "For every masked word, BERT knows the original word (since it masked it itself).  \n",
    "That becomes the **label**.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Input tokens:  [The] [quick] [brown] [fox] [MASK] [over] [the] [MASK] [dog]\n",
    "True tokens:   [The] [quick] [brown] [fox] [jumps] [over] [the] [lazy] [dog]\n",
    "```\n",
    "\n",
    "**Training labels:**\n",
    "\n",
    "- `[MASK]_1` ‚Üí ‚Äújumps‚Äù  \n",
    "- `[MASK]_2` ‚Üí ‚Äúlazy‚Äù\n",
    "\n",
    "The model predicts a probability distribution over the entire vocabulary for each masked position:\n",
    "\n",
    "```\n",
    "P(jumps | context) = 0.92\n",
    "P(run   | context) = 0.03\n",
    "P(eats  | context) = 0.01\n",
    "...\n",
    "```\n",
    "\n",
    "Then it computes **cross-entropy loss** between predicted and true tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0399754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretraing Phase: Masked Language Modeling (MLM)\n",
    "\n",
    "# It extracts a subset of hidden states (masked_X) from a tensor X at specific prediction positions within each sequence in the batch, then passes them to a small MLP to predict masked tokens\n",
    "\n",
    "#@save\n",
    "class MaskLM(nn.Module):\n",
    "    \"\"\"The masked language model task of BERT.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "\n",
    "        # MLM prediction head ‚Äî a small neural network that takes the hidden representation of masked tokens and predicts their original vocabulary word.\n",
    "        self.mlp = nn.Sequential(nn.LazyLinear(num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.LazyLinear(vocab_size))\n",
    "\n",
    "    # X: output from BERT's encoder\n",
    "    def forward(self, X, pred_positions):\n",
    "        # shape of X: (batch size, max sequence length, num_hiddens)\n",
    "        # shape of pred_positions: (batch size, num pred positions)\n",
    "        num_pred_positions = pred_positions.shape[1] # how many masked tokens per sequence\n",
    "        # flatten the array into a 1-D vector: (batch size * num pred positions,)\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        # Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n",
    "        # `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "\n",
    "        # gather hidden states from X at those batch/position pairs\n",
    "        # batch_idx ‚Üí which batch each row comes from\n",
    "        # pred_positions ‚Üí which token index (position) in that batch to pick\n",
    "        # X: (batch size, max sequence length, num_hiddens)\n",
    "        # masked_X: (batch_size * num_pred_positions, num_hiddens)\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        # masked_X: (batch_size, num_pred_positions, num_hiddens)\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d1ae09",
   "metadata": {},
   "source": [
    "#### Tensor Advanced Indexing (PyTorch)\n",
    "\n",
    "Advanced indexing lets you **select arbitrary elements** from a tensor using other tensors (integer or boolean), instead of simple slices.\n",
    "\n",
    "##### Using Integer Tensors\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.arange(12).view(3, 4)\n",
    "# x =\n",
    "# tensor([[ 0,  1,  2,  3],\n",
    "#         [ 4,  5,  6,  7],\n",
    "#         [ 8,  9, 10, 11]])\n",
    "\n",
    "indices_rows = torch.tensor([0, 1, 2])\n",
    "indices_cols = torch.tensor([1, 3, 0])\n",
    "\n",
    "result = x[indices_rows, indices_cols]\n",
    "print(result)\n",
    "# tensor([1, 7, 8])\n",
    "```\n",
    "How It Works:\n",
    "- indices_rows and indices_cols are paired element-wise.\n",
    "- PyTorch selects: (0, 1) ‚Üí 1, (1, 3) ‚Üí 7, (2, 0) ‚Üí 8\n",
    "- The output shape matches the shape of the index tensors.\n",
    "\n",
    "Key rule: **When multiple integer index tensors are used, PyTorch performs element-wise indexing, not slicing.**\n",
    "\n",
    "##### Using Boolean Tensors (Masking)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "x = torch.arange(9).view(3, 3)\n",
    "# x =\n",
    "# tensor([[0, 1, 2],\n",
    "#         [3, 4, 5],\n",
    "#         [6, 7, 8]])\n",
    "\n",
    "mask = x > 4\n",
    "mask = x > 4\n",
    "# mask =\n",
    "# tensor([[False, False, False],\n",
    "#         [False, False,  True],\n",
    "#         [ True,  True,  True]])\n",
    "result = x[mask]\n",
    "print(result)\n",
    "# tensor([5, 6, 7, 8])\n",
    "```\n",
    "\n",
    "How It Works:\n",
    "- The boolean mask has the same shape as x.\n",
    "- True values indicate which elements to keep.\n",
    "- **All selected elements are flattened into a 1D tensor. Boolean indexing does not preserve the original shape.**\n",
    "\n",
    "Equivalent logic: [x[i, j] for all (i, j) where x[i, j] > 4]\n",
    "\n",
    "| Feature    | Basic Slicing          | Advanced Indexing                |\n",
    "| ---------- | ---------------------- | -------------------------------- |\n",
    "| Index type | Integers, slices (`:`) | Integer tensors, boolean tensors |\n",
    "| Selection  | Rectangular blocks     | Arbitrary elements               |\n",
    "| Output     | View (when possible)   | **Copy** (not a view)            |\n",
    "| Shape      | Predictable            | Follows index tensors            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "01f39b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "\n",
    "# `mlm_positions` specifies the token indices to be predicted (masked) in each input sequence.\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "\n",
    "# encoded_X represents the output of the **BERTEncoder** for 2 input sequences (batch size = 2).\n",
    "# encoded_X: (batch_size, max sequence len, num_hiddens)\n",
    "\n",
    "# - The forward pass of `mlm`:\n",
    "#   - Takes `encoded_X` and `mlm_positions` as input.\n",
    "#   - Produces predictions `mlm_Y_hat` only at the masked positions.\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad76fe5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]]) # ground truth \n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab9fa7",
   "metadata": {},
   "source": [
    "##### Next Sentence Prediction (NSP)\n",
    "Masked Language Modeling (MLM) helps BERT learn **word-level** understanding. But MLM alone doesn‚Äôt teach BERT **how two sentences relate to each other**. To fill that gap, BERT adds Next Sentence Prediction (NSP) during pretraining.\n",
    "\n",
    "NSP teaches BERT to understand sentence relationships such as:\n",
    "- Does sentence B logically follow sentence A?\n",
    "- Is sentence B a random sentence?\n",
    "\n",
    "This is useful for downstream tasks like QA, NLI, and summarization.\n",
    "\n",
    "##### What NSP Does During Pretraining\n",
    "For every pair of sentences fed into BERT:\n",
    "\n",
    "- 50% of the time the second sentence is the actual continuation ‚Üí label = **True (1)**\n",
    "- 50% of the time the second sentence is a random sentence from the corpus ‚Üí label = **False (0)**\n",
    "\n",
    "BERT must classify which case it is.\n",
    "\n",
    "All the information needed is stored in the **[CLS] token‚Äôs embedding**, because the Transformer encodes the meaning of the entire input sequence into that embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "389fa1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretraining Phase: Next Sentence Prediction\n",
    "\n",
    "#@save\n",
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"The next sentence prediction task of BERT.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        # 0 -> not next sentence, 1 -> next sentence\n",
    "        self.output = nn.LazyLinear(2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # `X` is the embedding of the `<cls>` token\n",
    "        # `X` shape: (batch size, `num_hiddens`)\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "510cadaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_X before:  torch.Size([2, 8, 768])\n",
      "input encoded_X: torch.Size([2, 6144]), output nsp_Y_hat shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# encoded_X: (batch_size, max_sequence_len, num_hiddens)\n",
    "# encoded_X: (batch_size, max_sequence_len * num_hiddens)\n",
    "print(\"encoded_X before: \", encoded_X.shape)\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "\n",
    "nsp = NextSentencePred()\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "print(f\"input encoded_X: {encoded_X.shape}, output nsp_Y_hat shape: {nsp_Y_hat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "22916c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsp_l: torch.Size([2]), nsp_l: tensor([1.6472, 0.2599], grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "print(f\"nsp_l: {nsp_l.shape}, nsp_l: {nsp_l}\")\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e92af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"The BERT model.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                 num_heads, num_blks, dropout, max_len=1000):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size,\n",
    "                                   num_hiddens,\n",
    "                                   ffn_num_hiddens,\n",
    "                                   num_heads,\n",
    "                                   num_blks, # how many Transformer encoder blocks are stacked.\n",
    "                                   dropout,\n",
    "                                   max_len=max_len)\n",
    "        self.hidden = nn.Sequential(nn.LazyLinear(num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens)\n",
    "        self.nsp = NextSentencePred()\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "\n",
    "        # The hidden layer of the MLP classifier for next sentence prediction.\n",
    "        # 0 is the index of the '<cls>' token\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "\n",
    "        # When pretraining BERT, the final loss function is a linear combination of both the loss functions for masked language modeling and next sentence prediction.\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a632e",
   "metadata": {},
   "source": [
    "num_blks = how many Transformer encoder blocks are stacked.\n",
    "Each block = one full Transformer layer:\n",
    "- Multi-head self-attention\n",
    "- Feed-forward network (FFN)\n",
    "- Residual connections + LayerNorm\n",
    "\n",
    "So internally, BERTEncoder does something like: Input ‚Üí Block‚ÇÅ ‚Üí Block‚ÇÇ ‚Üí ... ‚Üí Block_num_blks ‚Üí Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bc84f",
   "metadata": {},
   "source": [
    "======================================The Dataset for Pretraining BERT======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6dd80954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3660b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "\n",
    "#@save\n",
    "def _read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # Uppercase letters are converted to lowercase ones\n",
    "    paragraphs = [line.strip().lower().split(' . ')\n",
    "                  for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "35edff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # `paragraphs` is a list of lists of lists\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c7d57276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
    "            paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # Consider 1 '<cls>' token and 2 '<sep>' tokens\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8f25185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the Masked Language Modeling Task\n",
    "\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    # For the input of a masked language model, make a new copy of tokens and\n",
    "    # replace some of them by '<mask>' or random tokens\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "    # Shuffle for getting 15% random tokens for prediction in the masked\n",
    "    # language modeling task\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds: # num_mlm_preds=15% * len(tokens)\n",
    "            break\n",
    "        masked_token = None\n",
    "        # 80% of the time: replace the word with the '<mask>' token\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10% of the time: keep the word unchanged\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10% of the time: replace the word with a random word\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, pred_positions_and_labels\n",
    "\n",
    "#@save\n",
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_pred_positions = []\n",
    "    # `tokens` is a list of strings\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Special tokens are not predicted in the masked language modeling task\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    # 15% of random tokens are predicted in the masked language modeling task\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05cfb099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/wikitext-2-v1.zip from https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip...\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_iter, train_set\u001b[38;5;241m.\u001b[39mvocab\n\u001b[1;32m     73\u001b[0m batch_size, max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m---> 74\u001b[0m train_iter, vocab \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_wiki\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n\u001b[1;32m     77\u001b[0m      mlm_Y, nsp_y) \u001b[38;5;129;01min\u001b[39;00m train_iter:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokens_X\u001b[38;5;241m.\u001b[39mshape, segments_X\u001b[38;5;241m.\u001b[39mshape, valid_lens_x\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m     79\u001b[0m           pred_positions_X\u001b[38;5;241m.\u001b[39mshape, mlm_weights_X\u001b[38;5;241m.\u001b[39mshape, mlm_Y\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m     80\u001b[0m           nsp_y\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[92], line 66\u001b[0m, in \u001b[0;36mload_data_wiki\u001b[0;34m(batch_size, max_len)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the WikiText-2 dataset.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mget_dataloader_workers()\n\u001b[0;32m---> 66\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwikitext-2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwikitext-2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m paragraphs \u001b[38;5;241m=\u001b[39m _read_wiki(data_dir)\n\u001b[1;32m     68\u001b[0m train_set \u001b[38;5;241m=\u001b[39m _WikiTextDataset(paragraphs, max_len)\n",
      "File \u001b[0;32m/workspaces/pytorch-basics/.venv310/lib/python3.10/site-packages/d2l/torch.py:3247\u001b[0m, in \u001b[0;36mdownload_extract\u001b[0;34m(name, folder)\u001b[0m\n\u001b[1;32m   3245\u001b[0m data_dir, ext \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(fname)\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3248\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ext \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gz\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   3249\u001b[0m     fp \u001b[38;5;241m=\u001b[39m tarfile\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/zipfile.py:1288\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1288\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1290\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/zipfile.py:1355\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[0;32m-> 1355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "# Transforming Text into the Pretraining Dataset\n",
    "\n",
    "#@save\n",
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n",
    "         is_next) in examples:\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n",
    "            max_len - len(token_ids)), dtype=torch.long))\n",
    "        all_segments.append(torch.tensor(segments + [0] * (\n",
    "            max_len - len(segments)), dtype=torch.long))\n",
    "        # `valid_lens` excludes count of '<pad>' tokens\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n",
    "            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "        # Predictions of padded tokens will be filtered out in the loss via\n",
    "        # multiplication of 0 weights\n",
    "        all_mlm_weights.append(\n",
    "            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n",
    "                max_num_mlm_preds - len(pred_positions)),\n",
    "                dtype=torch.float32))\n",
    "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n",
    "            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
    "            all_mlm_weights, all_mlm_labels, nsp_labels)\n",
    "\n",
    "#@save\n",
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # Input `paragraphs[i]` is a list of sentence strings representing a paragraph;\n",
    "        # while output `paragraphs[i]` is a list of sentences representing a paragraph, where each sentence is a list of tokens\n",
    "        paragraphs = [d2l.tokenize(paragraph, token='word') for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        # Get data for the next sentence prediction task\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(\n",
    "                paragraph, paragraphs, self.vocab, max_len))\n",
    "        # Get data for the masked language model task\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
    "                      + (segments, is_next))\n",
    "                     for tokens, segments, is_next in examples]\n",
    "        # Pad inputs\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "         self.all_pred_positions, self.all_mlm_weights,\n",
    "         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)\n",
    "    \n",
    "#@save\n",
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"Load the WikiText-2 dataset.\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                        shuffle=True, num_workers=num_workers)\n",
    "    return train_iter, train_set.vocab\n",
    "\n",
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
    "     mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
    "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
    "          nsp_y.shape)\n",
    "    break\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b86cc4",
   "metadata": {},
   "source": [
    "================================Pretraining BERT===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ea6dfd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d1bfbd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/wikitext-2-v1.zip from https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip...\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size, max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train_iter, vocab \u001b[38;5;241m=\u001b[39m \u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data_wiki\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/pytorch-basics/.venv310/lib/python3.10/site-packages/d2l/torch.py:2443\u001b[0m, in \u001b[0;36mload_data_wiki\u001b[0;34m(batch_size, max_len)\u001b[0m\n\u001b[1;32m   2439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the WikiText-2 dataset.\u001b[39;00m\n\u001b[1;32m   2440\u001b[0m \n\u001b[1;32m   2441\u001b[0m \u001b[38;5;124;03mDefined in :numref:`subsec_prepare_mlm_data`\"\"\"\u001b[39;00m\n\u001b[1;32m   2442\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mget_dataloader_workers()\n\u001b[0;32m-> 2443\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwikitext-2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwikitext-2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2444\u001b[0m paragraphs \u001b[38;5;241m=\u001b[39m _read_wiki(data_dir)\n\u001b[1;32m   2445\u001b[0m train_set \u001b[38;5;241m=\u001b[39m _WikiTextDataset(paragraphs, max_len)\n",
      "File \u001b[0;32m/workspaces/pytorch-basics/.venv310/lib/python3.10/site-packages/d2l/torch.py:3247\u001b[0m, in \u001b[0;36mdownload_extract\u001b[0;34m(name, folder)\u001b[0m\n\u001b[1;32m   3245\u001b[0m data_dir, ext \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(fname)\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3248\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ext \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gz\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   3249\u001b[0m     fp \u001b[38;5;241m=\u001b[39m tarfile\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/zipfile.py:1288\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1288\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1290\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/zipfile.py:1355\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[0;32m-> 1355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ef208",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = d2l.BERTModel(len(vocab), num_hiddens=128, ffn_num_hiddens=256, num_heads=2, num_blks=2, dropout=0.2)\n",
    "devices = d2l.try_all_gpus()\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n",
    "                         segments_X, valid_lens_x,\n",
    "                         pred_positions_X, mlm_weights_X,\n",
    "                         mlm_Y, nsp_y):\n",
    "    # Forward pass\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n",
    "                                  valid_lens_x.reshape(-1),\n",
    "                                  pred_positions_X)\n",
    "    # Compute masked language model loss\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n",
    "    mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    # Compute next sentence prediction loss\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l, nsp_l, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
    "    net(*next(iter(train_iter))[:4])\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    step, timer = 0, d2l.Timer()\n",
    "    animator = d2l.Animator(xlabel='step', ylabel='loss',\n",
    "                            xlim=[1, num_steps], legend=['mlm', 'nsp'])\n",
    "    # Sum of masked language modeling losses, sum of next sentence prediction\n",
    "    # losses, no. of sentence pairs, count\n",
    "    metric = d2l.Accumulator(4)\n",
    "    num_steps_reached = False\n",
    "    while step < num_steps and not num_steps_reached:\n",
    "        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n",
    "            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
    "            tokens_X = tokens_X.to(devices[0])\n",
    "            segments_X = segments_X.to(devices[0])\n",
    "            valid_lens_x = valid_lens_x.to(devices[0])\n",
    "            pred_positions_X = pred_positions_X.to(devices[0])\n",
    "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
    "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
    "            trainer.zero_grad()\n",
    "            timer.start()\n",
    "            mlm_l, nsp_l, l = _get_batch_loss_bert(\n",
    "                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
    "                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
    "            timer.stop()\n",
    "            animator.add(step + 1,\n",
    "                         (metric[0] / metric[3], metric[1] / metric[3]))\n",
    "            step += 1\n",
    "            if step == num_steps:\n",
    "                num_steps_reached = True\n",
    "                break\n",
    "\n",
    "    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n",
    "          f'NSP loss {metric[1] / metric[3]:.3f}')\n",
    "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49709c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert(train_iter, net, loss, len(vocab), devices, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
