{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af633e2",
   "metadata": {},
   "source": [
    "#### Deep Convolutional Neural Networks (AlexNet)\n",
    "\n",
    "#### Historical Context: The AI Winter Ends (2012)\n",
    "\n",
    "Although CNNs existed since LeNet (1995), they didn't dominate computer vision until **AlexNet won ImageNet 2012** by a large margin. Before this breakthrough:\n",
    "\n",
    "- **Traditional pipelines** dominated: Hand-crafted features (SIFT, SURF, HOG) → Linear classifiers\n",
    "- **Neural networks** were outperformed by kernel methods, ensemble methods, and SVMs\n",
    "- **Features were engineered**, not learned\n",
    "\n",
    "##### What Was Missing?\n",
    "\n",
    "| Missing Ingredient | Problem | Solution by 2012 |\n",
    "|-------------------|---------|------------------|\n",
    "| **Data** | Small datasets (thousands of images) | ImageNet: 1.2M images, 1000 classes |\n",
    "| **Compute** | CPUs too slow for deep networks | GPUs: 1000x speedup over CPUs |\n",
    "| **Algorithms** | Training instabilities | ReLU, Dropout, better initialization |\n",
    "\n",
    "#### Representation Learning\n",
    "\n",
    "The key insight: **features should be learned, not hand-designed**.\n",
    "\n",
    "- Traditional CV: pixels → SIFT/SURF → classifier\n",
    "- Deep Learning: pixels → learned features (hierarchical) → classifier\n",
    "\n",
    "AlexNet showed that learned features outperform hand-crafted ones when given enough data and compute.\n",
    "\n",
    "#### AlexNet Architecture (2012)\n",
    "\n",
    "AlexNet is an evolutionary improvement over LeNet with crucial differences:\n",
    "\n",
    "| Aspect | LeNet-5 (1998) | AlexNet (2012) |\n",
    "|--------|----------------|----------------|\n",
    "| **Depth** | 5 layers | 8 layers |\n",
    "| **Input size** | 28×28 | 224×224 |\n",
    "| **Activation** | Sigmoid | ReLU |\n",
    "| **Regularization** | None | Dropout (0.5) |\n",
    "| **Data augmentation** | Minimal | Extensive |\n",
    "| **Training hardware** | CPU | 2× NVIDIA GTX 580 GPUs |\n",
    "\n",
    "##### Architecture Details\n",
    "\n",
    "```\n",
    "Input: 224×224×3\n",
    "    ↓\n",
    "Conv1: 96 filters, 11×11, stride 4, ReLU → MaxPool 3×3, stride 2\n",
    "    ↓  [54×54×96 → 26×26×96]\n",
    "Conv2: 256 filters, 5×5, pad 2, ReLU → MaxPool 3×3, stride 2\n",
    "    ↓  [26×26×256 → 12×12×256]\n",
    "Conv3: 384 filters, 3×3, pad 1, ReLU\n",
    "    ↓  [12×12×384]\n",
    "Conv4: 384 filters, 3×3, pad 1, ReLU\n",
    "    ↓  [12×12×384]\n",
    "Conv5: 256 filters, 3×3, pad 1, ReLU → MaxPool 3×3, stride 2\n",
    "    ↓  [12×12×256 → 5×5×256]\n",
    "Flatten → 6400\n",
    "    ↓\n",
    "FC1: 4096, ReLU, Dropout(0.5)\n",
    "    ↓\n",
    "FC2: 4096, ReLU, Dropout(0.5)\n",
    "    ↓\n",
    "FC3: 1000 (classes)\n",
    "```\n",
    "\n",
    "#### Key Innovations\n",
    "\n",
    "##### 1. ReLU Activation\n",
    "- **Problem with Sigmoid**: Vanishing gradients when output ≈ 0 or 1\n",
    "- **ReLU solution**: $f(x) = \\max(0, x)$ — gradient is always 1 for positive inputs\n",
    "- **Benefit**: 6× faster training than tanh\n",
    "\n",
    "##### 2. Dropout Regularization\n",
    "- Randomly zero out 50% of neurons during training\n",
    "- Prevents co-adaptation of features\n",
    "- Acts like training an ensemble of networks\n",
    "\n",
    "##### 3. Data Augmentation\n",
    "- Random crops from 256×256 → 224×224\n",
    "- Horizontal flips\n",
    "- Color jittering (PCA-based)\n",
    "- Makes model robust to variations\n",
    "\n",
    "##### 4. GPU Training\n",
    "- Split model across 2 GPUs (3GB each)\n",
    "- Enabled training on large ImageNet dataset\n",
    "- Paradigm shift: Deep learning became GPU-bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "402c6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "269c06b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet Architecture:\n",
      "AlexNet(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (13): Flatten(start_dim=1, end_dim=-1)\n",
      "    (14): Linear(in_features=6400, out_features=4096, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Dropout(p=0.5, inplace=False)\n",
      "    (17): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (18): ReLU()\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "    (20): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# AlexNet Implementation\n",
    "class AlexNet(nn.Module):\n",
    "    \"\"\"AlexNet architecture for ImageNet-scale classification.\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Conv Block 1: Large kernel to capture global patterns\n",
    "            nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), #(26,26,96)\n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # (12,12,256)\n",
    "            # Conv Blocks 3-5: Smaller 3x3 kernels\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),                             # (12,12,384)\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),                             # (12,12,384)\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),               \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Classifier\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6400, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Initialize weights using Xavier initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "model = AlexNet()\n",
    "model.apply(init_weights)\n",
    "print(\"AlexNet Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3395703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet layer shapes for 224x224 single-channel input:\n",
      "\n",
      "Layer           Output Shape              Params    \n",
      "=======================================================\n",
      "Conv2d          torch.Size([1, 96, 54, 54]) 11,712\n",
      "ReLU            torch.Size([1, 96, 54, 54]) 0\n",
      "MaxPool2d       torch.Size([1, 96, 26, 26]) 0\n",
      "Conv2d          torch.Size([1, 256, 26, 26]) 614,656\n",
      "ReLU            torch.Size([1, 256, 26, 26]) 0\n",
      "MaxPool2d       torch.Size([1, 256, 12, 12]) 0\n",
      "Conv2d          torch.Size([1, 384, 12, 12]) 885,120\n",
      "ReLU            torch.Size([1, 384, 12, 12]) 0\n",
      "Conv2d          torch.Size([1, 384, 12, 12]) 1,327,488\n",
      "ReLU            torch.Size([1, 384, 12, 12]) 0\n",
      "Conv2d          torch.Size([1, 256, 12, 12]) 884,992\n",
      "ReLU            torch.Size([1, 256, 12, 12]) 0\n",
      "MaxPool2d       torch.Size([1, 256, 5, 5]) 0\n",
      "Flatten         torch.Size([1, 6400])     0\n",
      "Linear          torch.Size([1, 4096])     26,218,496\n",
      "ReLU            torch.Size([1, 4096])     0\n",
      "Dropout         torch.Size([1, 4096])     0\n",
      "Linear          torch.Size([1, 4096])     16,781,312\n",
      "ReLU            torch.Size([1, 4096])     0\n",
      "Dropout         torch.Size([1, 4096])     0\n",
      "Linear          torch.Size([1, 10])       40,970\n",
      "\n",
      "Total parameters: 46,764,746\n"
     ]
    }
   ],
   "source": [
    "# Inspect layer-by-layer output shapes\n",
    "def layer_summary(model, input_shape):\n",
    "    \"\"\"Print output shape at each layer.\"\"\"\n",
    "    X = torch.randn(*input_shape)\n",
    "    print(f\"{'Layer':<15} {'Output Shape':<25} {'Params':<10}\")\n",
    "    print(\"=\" * 55)\n",
    "    for layer in model.net:\n",
    "        X = layer(X)\n",
    "        params = sum(p.numel() for p in layer.parameters()) if hasattr(layer, 'parameters') else 0\n",
    "        print(f\"{layer.__class__.__name__:<15} {str(X.shape):<25} {params:,}\")\n",
    "\n",
    "# AlexNet expects 224x224 input, but we'll use 1 channel for Fashion-MNIST\n",
    "# Note: For ImageNet, input would be (1, 3, 224, 224)\n",
    "print(\"AlexNet layer shapes for 224x224 single-channel input:\\n\")\n",
    "layer_summary(model, (1, 1, 224, 224))\n",
    "\n",
    "# Total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287c544",
   "metadata": {},
   "source": [
    "### Training AlexNet on Fashion-MNIST\n",
    "\n",
    "Since ImageNet is large (~150GB), we'll train on Fashion-MNIST resized to 224×224. This demonstrates the architecture while being computationally feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0bf6de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 60000\n",
      "Test samples: 10000\n",
      "Image shape: torch.Size([1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Resize Fashion-MNIST to 224x224 (AlexNet's expected input size)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Smaller batch size due to larger images\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Image shape: {train_dataset[0][0].shape}\")  # Should be (1, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbd58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(X)\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        total += X.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += X.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "model = AlexNet(num_classes=10).to(device)\n",
    "model.apply(init_weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train (fewer epochs due to computational cost)\n",
    "num_epochs = 10\n",
    "print(f\"\\n{'Epoch':<8}{'Train Loss':<12}{'Train Acc':<12}{'Test Loss':<12}{'Test Acc':<12}\")\n",
    "print(\"=\" * 56)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"{epoch+1:<8}{train_loss:<12.4f}{train_acc:<12.4f}{test_loss:<12.4f}{test_acc:<12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5461e7a6",
   "metadata": {},
   "source": [
    "### Summary: AlexNet\n",
    "\n",
    "#### Why AlexNet Matters\n",
    "\n",
    "AlexNet (2012) marked the beginning of the deep learning revolution in computer vision:\n",
    "\n",
    "| Impact | Before AlexNet | After AlexNet |\n",
    "|--------|---------------|---------------|\n",
    "| **Features** | Hand-crafted (SIFT, HOG) | Learned automatically |\n",
    "| **ImageNet accuracy** | ~74% (top-5) | 84.7% (top-5) — 10% improvement! |\n",
    "| **Dominant approach** | SVMs, ensemble methods | Deep neural networks |\n",
    "| **Hardware** | CPUs | GPUs become essential |\n",
    "\n",
    "#### Key Architectural Innovations\n",
    "\n",
    "1. **Deeper network**: 8 layers (5 conv + 3 FC) vs LeNet's 5 layers\n",
    "2. **ReLU activation**: Faster training, no vanishing gradients\n",
    "3. **Dropout**: Regularization for large FC layers\n",
    "4. **Overlapping pooling**: 3×3 windows with stride 2\n",
    "5. **Local Response Normalization** (LRN): Rarely used today, replaced by BatchNorm\n",
    "\n",
    "#### Parameter Count Breakdown\n",
    "\n",
    "| Layer Type | Parameters | % of Total |\n",
    "|------------|------------|------------|\n",
    "| Conv layers | ~2.3M | 4% |\n",
    "| FC layers | ~58.6M | 96% |\n",
    "| **Total** | ~60.9M | 100% |\n",
    "\n",
    "The FC layers dominate! This motivated later architectures (NiN, GoogLeNet) to reduce FC parameters.\n",
    "\n",
    "#### Legacy\n",
    "\n",
    "AlexNet established the modern CNN template:\n",
    "- **Conv-ReLU-Pool** blocks for feature extraction\n",
    "- **FC layers** for classification\n",
    "- **Dropout** for regularization\n",
    "- **GPU training** as standard practice\n",
    "\n",
    "This paved the way for VGG, GoogLeNet, ResNet, and all modern architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fbba2",
   "metadata": {},
   "source": [
    "### Networks Using Blocks (VGG)\n",
    "\n",
    "#### The Block Design Philosophy\n",
    "\n",
    "While AlexNet showed deep CNNs work, it didn't provide a **template** for designing new networks. VGG (Visual Geometry Group, Oxford, 2014) introduced a key insight:\n",
    "\n",
    "> **Design networks using repeating blocks of layers, not individual layers.**\n",
    "\n",
    "This mirrors chip design evolution: transistors → logic gates → logic blocks. Similarly:\n",
    "- **Early CNNs**: Design each layer individually (AlexNet)\n",
    "- **VGG onwards**: Design blocks, then stack them\n",
    "- **Modern**: Use entire pretrained models (foundation models)\n",
    "\n",
    "#### Why 3×3 Convolutions?\n",
    "\n",
    "VGG's key finding: **Deep and narrow beats shallow and wide**.\n",
    "\n",
    "##### Receptive Field Equivalence\n",
    "\n",
    "Two 3×3 convolutions have the same receptive field as one 5×5 convolution:\n",
    "\n",
    "```\n",
    "Single 5×5:     Two 3×3 stacked:\n",
    "  □□□□□           □□□ → □□□\n",
    "  □□□□□           □□□   □□□\n",
    "  □□□□□           □□□   □□□\n",
    "  □□□□□\n",
    "  □□□□□\n",
    "```\n",
    "\n",
    "Both \"see\" a 5×5 region of the input!\n",
    "\n",
    "##### Parameter Comparison\n",
    "\n",
    "For $c$ input and output channels:\n",
    "\n",
    "| Configuration | Parameters | Relative |\n",
    "|---------------|------------|----------|\n",
    "| One 5×5 conv | $25c^2$ | 1.0× |\n",
    "| Two 3×3 convs | $2 \\times 9c^2 = 18c^2$ | 0.72× |\n",
    "| Three 3×3 convs (7×7 equiv) | $3 \\times 9c^2 = 27c^2$ | vs $49c^2$ |\n",
    "\n",
    "**Benefit**: More layers = more nonlinearities + fewer parameters!\n",
    "\n",
    "#### VGG Block Structure\n",
    "\n",
    "A VGG block consists of:\n",
    "1. Multiple 3×3 convolutions (with padding=1 to preserve size)\n",
    "2. ReLU after each convolution\n",
    "3. 2×2 max pooling with stride 2 (halves dimensions)\n",
    "\n",
    "```\n",
    "VGG Block(num_convs, out_channels):\n",
    "    for i in range(num_convs):\n",
    "        Conv2d(3×3, padding=1) → ReLU\n",
    "    MaxPool2d(2×2, stride=2)\n",
    "```\n",
    "\n",
    "#### VGG Network Variants\n",
    "\n",
    "VGG defines a **family** of networks by varying the architecture:\n",
    "\n",
    "| Model | Architecture | Conv Layers | Parameters |\n",
    "|-------|-------------|-------------|------------|\n",
    "| VGG-11 | (1,64)-(1,128)-(2,256)-(2,512)-(2,512) | 8 | 133M |\n",
    "| VGG-13 | (2,64)-(2,128)-(2,256)-(2,512)-(2,512) | 10 | 133M |\n",
    "| VGG-16 | (2,64)-(2,128)-(3,256)-(3,512)-(3,512) | 13 | 138M |\n",
    "| VGG-19 | (2,64)-(2,128)-(4,256)-(4,512)-(4,512) | 16 | 144M |\n",
    "\n",
    "Format: (num_convs, channels) per block\n",
    "\n",
    "#### VGG-11 Architecture Diagram\n",
    "\n",
    "```\n",
    "Input: 224×224×3\n",
    "    ↓\n",
    "Block 1: 1×Conv(64) + Pool → 112×112×64\n",
    "    ↓\n",
    "Block 2: 1×Conv(128) + Pool → 56×56×128\n",
    "    ↓\n",
    "Block 3: 2×Conv(256) + Pool → 28×28×256\n",
    "    ↓\n",
    "Block 4: 2×Conv(512) + Pool → 14×14×512\n",
    "    ↓\n",
    "Block 5: 2×Conv(512) + Pool → 7×7×512\n",
    "    ↓\n",
    "Flatten → 25088\n",
    "    ↓\n",
    "FC(4096) → ReLU → Dropout(0.5)\n",
    "    ↓\n",
    "FC(4096) → ReLU → Dropout(0.5)\n",
    "    ↓\n",
    "FC(1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd12a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "516a33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(d2l.Classifier):\n",
    "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        conv_blks = []\n",
    "        for (num_convs, out_channels) in arch:\n",
    "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
    "        self.net = nn.Sequential(\n",
    "            *conv_blks, nn.Flatten(),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(num_classes))\n",
    "        self.net.apply(d2l.init_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd88be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 25088])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary((1, 1, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6819e",
   "metadata": {},
   "source": [
    "### Training VGG on Fashion-MNIST\n",
    "\n",
    "VGG is more computationally demanding than AlexNet. For Fashion-MNIST, we use a **smaller version** with fewer channels to make training feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d407864",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)))\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78c71d",
   "metadata": {},
   "source": [
    "### Summary: VGG\n",
    "\n",
    "#### Key Contributions\n",
    "\n",
    "1. **Block-based design**: First to use repeating blocks of layers\n",
    "2. **3×3 convolutions everywhere**: Showed deep+narrow > shallow+wide\n",
    "3. **Network families**: VGG-11, 13, 16, 19 provide speed-accuracy tradeoffs\n",
    "4. **Simple and uniform**: Easy to implement, understand, and modify\n",
    "\n",
    "#### VGG vs AlexNet\n",
    "\n",
    "| Aspect | AlexNet | VGG-16 |\n",
    "|--------|---------|--------|\n",
    "| Design | Ad-hoc layers | Repeating blocks |\n",
    "| Conv kernels | 11×11, 5×5, 3×3 | Only 3×3 |\n",
    "| Depth | 8 layers | 16 layers |\n",
    "| Parameters | ~60M | ~138M |\n",
    "| Top-5 error | 15.3% | 7.3% |\n",
    "\n",
    "![\"AlexNet to VGG\"](./Images/8/AlexNetToVGG.png)\n",
    "\n",
    "#### The 3×3 Convolution Insight\n",
    "\n",
    "VGG proved that **stacking small filters is better** than using large filters:\n",
    "- Same receptive field\n",
    "- More nonlinearities (more expressive)\n",
    "- Fewer parameters\n",
    "- Faster GPU implementations\n",
    "\n",
    "This became the **standard** for all subsequent architectures (ResNet, Inception, etc.)\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "1. **Very large FC layers**: 7×7×512 = 25,088 → 4096 requires ~100M parameters\n",
    "2. **Computationally expensive**: Slow training and inference\n",
    "3. **No skip connections**: Harder to train very deep versions\n",
    "\n",
    "These limitations motivated NiN (no FC layers), GoogLeNet (inception modules), and ResNet (skip connections).\n",
    "\n",
    "#### VGG Architecture Pattern\n",
    "\n",
    "```python\n",
    "# The VGG pattern: blocks with increasing channels, decreasing spatial size\n",
    "arch = [\n",
    "    (num_convs_1, channels_1),  # Block 1: spatial / 2\n",
    "    (num_convs_2, channels_2),  # Block 2: spatial / 2\n",
    "    ...\n",
    "]\n",
    "# Channels typically: 64 → 128 → 256 → 512 → 512\n",
    "# Spatial: 224 → 112 → 56 → 28 → 14 → 7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ac581",
   "metadata": {},
   "source": [
    "### Network in Network (NiN)\n",
    "\n",
    "#### Core Idea\n",
    "\n",
    "AlexNet and VGG follow a pattern: **convolutional layers** (feature extraction) → **fully connected layers** (classification). The FC layers contain the vast majority of parameters (e.g., ~100M of VGG's 138M).\n",
    "\n",
    "**NiN's insight**: Replace the expensive FC layers entirely by using **1×1 convolutions** and **global average pooling**.\n",
    "\n",
    "#### The NiN Block\n",
    "\n",
    "Each NiN block consists of:\n",
    "1. A **standard convolution** (e.g., 5×5 or 3×3) for spatial feature extraction\n",
    "2. Two **1×1 convolutions** with ReLU — acting as a **per-pixel MLP**\n",
    "\n",
    "```\n",
    "Input → Conv(k×k) → ReLU → Conv(1×1) → ReLU → Conv(1×1) → ReLU → Output\n",
    "```\n",
    "\n",
    "**Why 1×1 convolutions?** They mix channel information at each spatial position independently — equivalent to applying a fully connected layer *per pixel* across the channel dimension.\n",
    "\n",
    "#### NiN Architecture\n",
    "\n",
    "```\n",
    "NiN Block 1: Conv 11×11, 96 channels, stride 4 + two 1×1 convs\n",
    "    ↓ MaxPool 3×3, stride 2\n",
    "NiN Block 2: Conv 5×5, 256 channels + two 1×1 convs\n",
    "    ↓ MaxPool 3×3, stride 2\n",
    "NiN Block 3: Conv 3×3, 384 channels + two 1×1 convs\n",
    "    ↓ MaxPool 3×3, stride 2\n",
    "NiN Block 4: Conv 3×3, 10 channels (= num_classes) + two 1×1 convs\n",
    "    ↓ Global Average Pooling → flatten → output (10,)\n",
    "```\n",
    "\n",
    "**No FC layers at all!** The final NiN block outputs `num_classes` channels, and global average pooling reduces each channel's spatial map to a single number.\n",
    "\n",
    "#### Why Global Average Pooling?\n",
    "\n",
    "| Aspect | FC Layers | Global Average Pooling |\n",
    "|--------|-----------|----------------------|\n",
    "| Parameters | Millions (e.g., 25088×4096) | **Zero** |\n",
    "| Overfitting | High risk, needs dropout | Much less prone |\n",
    "| Spatial info | Destroyed by flattening | Summarized naturally |\n",
    "| Input size | Fixed (must match dimensions) | **Any** spatial size works |\n",
    "\n",
    "#### Key Contributions\n",
    "\n",
    "1. **1×1 convolutions**: Adds per-pixel nonlinearity and channel mixing — became a fundamental building block (used in GoogLeNet, ResNet, etc.)\n",
    "2. **Global average pooling**: Eliminates FC layers, dramatically reducing parameters and overfitting\n",
    "3. **Fully convolutional**: The entire network is convolutional — no dense layers at all\n",
    "\n",
    "#### NiN vs VGG\n",
    "\n",
    "| Aspect | VGG-16 | NiN |\n",
    "|--------|--------|-----|\n",
    "| FC parameters | ~120M | **0** |\n",
    "| Total parameters | ~138M | **Much fewer** |\n",
    "| Classification head | 3 FC layers + dropout | 1×1 conv + global avg pool |\n",
    "| Overfitting risk | High | Lower |\n",
    "| Key innovation | Block-based design | 1×1 conv + global avg pool |\n",
    "\n",
    "![\"VGG-Nin\"](./Images/8/VGG-NiN.png)\n",
    "\n",
    "#### Legacy\n",
    "\n",
    "- **1×1 convolutions** became ubiquitous: GoogLeNet's Inception module, ResNet's bottleneck blocks, and squeeze-and-excitation networks all rely on them\n",
    "- **Global average pooling** replaced FC layers in almost all modern architectures\n",
    "- NiN showed that the classification head doesn't need to be the parameter bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a87f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2083328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(out_channels, kernel_size, strides, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.LazyConv2d(out_channels, kernel_size, strides, padding), nn.ReLU(),\n",
    "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),\n",
    "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiN(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nin_block(96, kernel_size=11, strides=4, padding=0),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nin_block(256, kernel_size=5, strides=1, padding=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nin_block(384, kernel_size=3, strides=1, padding=1),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.\n",
    "            ==\n",
    "(num_classes, kernel_size=3, strides=1, padding=1),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten())\n",
    "        self.net.apply(d2l.init_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "160e9c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Sequential output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 384, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Dropout output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Sequential output shape:\t torch.Size([1, 10, 5, 5])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "NiN().layer_summary((1, 1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NiN(lr=0.05)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6a281",
   "metadata": {},
   "source": [
    "### Multi-Branch Networks (GoogLeNet / Inception)\n",
    "\n",
    "#### The Problem: Which Convolution Size to Use?\n",
    "\n",
    "Previous architectures made a **fixed choice** of kernel size per layer (AlexNet: 11×11, 5×5, 3×3; VGG: all 3×3). But different features exist at different scales:\n",
    "- **Small patterns** (edges, textures) → small kernels (1×1, 3×3)\n",
    "- **Medium patterns** (parts, shapes) → medium kernels (5×5)\n",
    "- **Large patterns** (objects) → larger receptive fields\n",
    "\n",
    "**GoogLeNet's answer**: Don't choose — **use them all in parallel!**\n",
    "\n",
    "#### The Inception Block\n",
    "\n",
    "The core building block of GoogLeNet processes input through **4 parallel paths** and concatenates results along the channel dimension:\n",
    "\n",
    "```\n",
    "                        Input\n",
    "           ┌──────┬──────┼──────────┐\n",
    "           │      │      │          │\n",
    "        Path 1  Path 2  Path 3   Path 4\n",
    "        1×1     1×1     1×1     MaxPool\n",
    "        conv    conv    conv      3×3\n",
    "           │      │      │          │\n",
    "           │    3×3    5×5       1×1\n",
    "           │    conv   conv      conv\n",
    "           │      │      │          │\n",
    "           └──────┴──────┴──────────┘\n",
    "                        │\n",
    "                   Concatenate\n",
    "                  (along channels)\n",
    "```\n",
    "\n",
    "| Path | Operations | Purpose |\n",
    "|------|-----------|---------|\n",
    "| **Path 1** | 1×1 conv | Capture single-pixel features |\n",
    "| **Path 2** | 1×1 conv → 3×3 conv | Capture local spatial features |\n",
    "| **Path 3** | 1×1 conv → 5×5 conv | Capture wider spatial features |\n",
    "| **Path 4** | 3×3 MaxPool → 1×1 conv | Capture pooled features |\n",
    "\n",
    "##### Why 1×1 Convolutions Before 3×3 and 5×5?\n",
    "\n",
    "The 1×1 convolutions act as **channel-dimension bottlenecks** to reduce computation:\n",
    "\n",
    "| Without 1×1 bottleneck | With 1×1 bottleneck |\n",
    "|------------------------|---------------------|\n",
    "| Input: 192 channels | Input: 192 channels |\n",
    "| 5×5 conv → 32 channels | 1×1 conv → 16 channels → 5×5 conv → 32 channels |\n",
    "| FLOPs: $192 \\times 32 \\times 5^2 = 153{,}600$ per pixel | FLOPs: $192 \\times 16 + 16 \\times 32 \\times 25 = 15{,}872$ per pixel |\n",
    "| — | **~10× fewer operations!** |\n",
    "\n",
    "This is the **bottleneck design pattern** — compress channels first, then apply expensive operations.\n",
    "\n",
    "#### GoogLeNet Architecture\n",
    "\n",
    "GoogLeNet (22 layers deep) is organized into stages:\n",
    "\n",
    "```\n",
    "Input: 224×224×3\n",
    "    ↓\n",
    "Stage 1: Conv 7×7/2, MaxPool 3×3/2          → 56×56×64\n",
    "Stage 2: Conv 1×1, Conv 3×3, MaxPool 3×3/2  → 28×28×192\n",
    "    ↓\n",
    "Stage 3: Inception(3a) + Inception(3b) + MaxPool  → 14×14×480\n",
    "Stage 4: Inception(4a–4e) + MaxPool                → 7×7×832\n",
    "Stage 5: Inception(5a) + Inception(5b)             → 7×7×1024\n",
    "    ↓\n",
    "Global Average Pooling → 1024\n",
    "    ↓\n",
    "Linear → 1000 classes\n",
    "```\n",
    "\n",
    "**Key**: No FC layers except the final classifier (following NiN's insight)!\n",
    "\n",
    "![\"GoogLeNet\"](./Images/8/GoogLeNet.png)\n",
    "\n",
    "\n",
    "\n",
    "#### Inception Block Channel Configuration\n",
    "\n",
    "Each Inception block has carefully tuned channel counts:\n",
    "\n",
    "| Block | Path 1 (1×1) | Path 2 (1×1→3×3) | Path 3 (1×1→5×5) | Path 4 (Pool→1×1) | Output |\n",
    "|-------|-------------|-------------------|-------------------|-------------------|--------|\n",
    "| 3a | 64 | 96→128 | 16→32 | 32 | 256 |\n",
    "| 3b | 128 | 128→192 | 32→96 | 64 | 480 |\n",
    "| 4a | 192 | 96→208 | 16→48 | 64 | 512 |\n",
    "| 4e | 256 | 160→320 | 32→128 | 128 | 832 |\n",
    "| 5b | 384 | 192→384 | 48→128 | 128 | 1024 |\n",
    "\n",
    "Output channels = sum of all path outputs (concatenated).\n",
    "\n",
    "#### Key Innovations\n",
    "\n",
    "1. **Multi-scale feature extraction**: Parallel paths with different kernel sizes capture features at multiple scales simultaneously\n",
    "2. **1×1 bottleneck convolutions**: Dramatically reduce computation by compressing channels before expensive operations\n",
    "3. **Global average pooling**: Following NiN, eliminates expensive FC layers\n",
    "4. **22 layers deep**: Much deeper than VGG, yet fewer parameters (~5M vs VGG's 138M)\n",
    "5. **Auxiliary classifiers** (training only): Added at intermediate layers to combat vanishing gradients in the deep network\n",
    "\n",
    "#### GoogLeNet vs Previous Architectures\n",
    "\n",
    "| Aspect | AlexNet | VGG-16 | NiN | GoogLeNet |\n",
    "|--------|---------|--------|-----|-----------|\n",
    "| Depth | 8 | 16 | 12 | 22 |\n",
    "| Parameters | ~60M | ~138M | ~2M | **~5M** |\n",
    "| FC layers | 3 | 3 | 0 | 0 (+ 1 linear) |\n",
    "| Key idea | Deep CNN | Block design | 1×1 conv | Multi-branch |\n",
    "| Top-5 error | 15.3% | 7.3% | — | **6.7%** |\n",
    "| Kernel sizes | Mixed | Only 3×3 | Mixed | **All in parallel** |\n",
    "\n",
    "#### Inception Versions\n",
    "\n",
    "The original Inception module evolved through several versions:\n",
    "\n",
    "| Version | Paper | Key Change |\n",
    "|---------|-------|------------|\n",
    "| **v1** | GoogLeNet (2014) | Original 4-path design |\n",
    "| **v2** | Ioffe & Szegedy (2015) | Added Batch Normalization |\n",
    "| **v3** | Szegedy et al. (2016) | Factorized convolutions (e.g., 5×5 → two 3×3) |\n",
    "| **v4** | Szegedy et al. (2017) | Combined with residual connections |\n",
    "\n",
    "#### Legacy\n",
    "\n",
    "- **Multi-branch/parallel design** influenced many architectures (ResNeXt, Xception)\n",
    "- **1×1 bottleneck** became standard in ResNet and beyond\n",
    "- Proved that **smart architecture design** can achieve better results with far fewer parameters\n",
    "- Showed the value of **letting the network decide** which scale is important (by using all scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05260215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd17bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    # c1--c4 are the number of output channels for each branch\n",
    "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # Branch 1\n",
    "        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n",
    "        # Branch 2\n",
    "        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n",
    "        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
    "        # Branch 3\n",
    "        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n",
    "        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
    "        # Branch 4\n",
    "        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = F.relu(self.b1_1(x))\n",
    "        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\n",
    "        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\n",
    "        b4 = F.relu(self.b4_2(self.b4_1(x)))\n",
    "        return torch.cat((b1, b2, b3, b4), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f30c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleNet(d2l.Classifier):\n",
    "    def b1(self):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5076d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(GoogleNet)\n",
    "def b2(self):\n",
    "    return nn.Sequential(\n",
    "        nn.LazyConv2d(64, kernel_size=1), nn.ReLU(),\n",
    "        nn.LazyConv2d(192, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2e621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(GoogleNet)\n",
    "def b3(self):\n",
    "    return nn.Sequential(Inception(64, (96, 128), (16, 32), 32),\n",
    "                         Inception(128, (128, 192), (32, 96), 64),\n",
    "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb5c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(GoogleNet)\n",
    "def b4(self):\n",
    "    return nn.Sequential(Inception(192, (96, 208), (16, 48), 64),\n",
    "                         Inception(160, (112, 224), (24, 64), 64),\n",
    "                         Inception(128, (128, 256), (24, 64), 64),\n",
    "                         Inception(112, (144, 288), (32, 64), 64),\n",
    "                         Inception(256, (160, 320), (32, 128), 128),\n",
    "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16c66cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(GoogleNet)\n",
    "def b5(self):\n",
    "    return nn.Sequential(Inception(256, (160, 320), (32, 128), 128),\n",
    "                         Inception(384, (192, 384), (48, 128), 128),\n",
    "                         nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c34e2196",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(GoogleNet)\n",
    "def __init__(self, lr=0.1, num_classes=10):\n",
    "    super(GoogleNet, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = nn.Sequential(self.b1(), self.b2(), self.b3(), self.b4(),\n",
    "                             self.b5(), nn.LazyLinear(num_classes))\n",
    "    self.net.apply(d2l.init_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d77d112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
      "Sequential output shape:\t torch.Size([1, 192, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 480, 6, 6])\n",
      "Sequential output shape:\t torch.Size([1, 832, 3, 3])\n",
      "Sequential output shape:\t torch.Size([1, 1024])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/pytorch-d2l/.venv310/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = GoogleNet().layer_summary((1, 1, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7785209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GoogleNet(lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a3ae70",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "#### Why Batch Normalization?\n",
    "\n",
    "Training deep networks is hard — intermediate layer activations can drift in magnitude across layers, units, and over time (*internal covariate shift*). Batch normalization (Ioffe & Szegedy, 2015) addresses this by **normalizing activations within each minibatch**, providing three key benefits:\n",
    "\n",
    "1. **Preprocessing / Numerical Stability** — keeps intermediate values well-scaled, enabling more aggressive learning rates.\n",
    "2. **Faster Convergence** — actively centers and rescales activations back to a controlled mean and variance.\n",
    "3. **Regularization** — noise from minibatch statistics acts as implicit regularization (similar in spirit to dropout).\n",
    "\n",
    "#### The Batch Normalization Formula\n",
    "\n",
    "For a minibatch $\\mathcal{B}$ and input $\\mathbf{x} \\in \\mathcal{B}$:\n",
    "\n",
    "$$\\text{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\mu}}_\\mathcal{B} = \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x}, \\qquad \\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}^2 = \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B})^2 + \\epsilon$$\n",
    "\n",
    "- $\\boldsymbol{\\gamma}$ (scale) and $\\boldsymbol{\\beta}$ (shift) are **learnable parameters** that restore representational power after normalization.\n",
    "- $\\epsilon > 0$ prevents division by zero.\n",
    "\n",
    "#### Batch Norm in Different Layer Types\n",
    "\n",
    "| Layer Type | Normalization Dimension | Details |\n",
    "|---|---|---|\n",
    "| **Fully Connected** | Per-feature across the batch | Applied after affine transform, before activation: $\\mathbf{h} = \\phi(\\text{BN}(\\mathbf{Wx} + \\mathbf{b}))$ |\n",
    "| **Convolutional** | Per-channel across batch & spatial dims | Each channel gets its own $\\gamma$ and $\\beta$; normalization computed over batch × height × width |\n",
    "\n",
    "#### Training vs. Prediction\n",
    "\n",
    "- **Training mode:** Normalizes using **minibatch** mean and variance (introduces beneficial noise).\n",
    "- **Prediction mode:** Uses **running estimates** (exponential moving averages) of the full dataset mean and variance for deterministic output.\n",
    "\n",
    "#### Key Practical Notes\n",
    "\n",
    "- Batch size matters more with BN — moderate sizes (50–100) inject the \"right amount\" of noise for regularization.\n",
    "- Minibatch size of 1 is useless (mean subtraction zeros everything out).\n",
    "- BN allows higher learning rates and reduces sensitivity to initialization.\n",
    "- Combined with residual connections (ResNet), BN enabled training networks with 100+ layers.\n",
    "\n",
    "#### Controversy & Alternatives\n",
    "\n",
    "- The original \"internal covariate shift\" explanation is debated; BN may work primarily by **smoothing the loss landscape** (Santurkar et al., 2018).\n",
    "- **Layer Normalization** (Ba et al., 2016) normalizes across features instead of batch — useful for RNNs and Transformers where batch stats are unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34708351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaecfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # When using a fully connected layer, calculate the mean and\n",
    "            # variance on the feature dimension\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # When using a two-dimensional convolutional layer, calculate the\n",
    "            # mean and variance on the channel dimension (axis=1). Here we\n",
    "            # need to maintain the shape of X, so that the broadcasting\n",
    "            # operation can be carried out later\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # In training mode, the current mean and variance are used\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # Update the mean and variance using moving average\n",
    "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
    "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
    "\n",
    "    # gamma shape: (1, num_features) or (1, num_features(channels), 1, 1)\n",
    "    # beta shape: (1, num_features) or (1, num_features(channels), 1, 1)\n",
    "    # broadcasting will automatically expand the shape of gamma and beta\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_mean.data, moving_var.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a0c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1) # here num_features means channels, using a generic name\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # The variables that are not model parameters are initialized to 0 and 1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.1)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d36dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNLeNetScratch(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), BatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), BatchNorm(16, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120),\n",
    "            BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "model = BNLeNetScratch(lr=0.1)\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.net[1].gamma.reshape((-1,)), model.net[1].beta.reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bdddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concise implementation of batch normalization\n",
    "class BNLeNet(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), nn.LazyBat\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),\n",
    "            nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),\n",
    "            nn.Sigmoid(), nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74eb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "model = BNLeNet(lr=0.1)\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f60297d",
   "metadata": {},
   "source": [
    "### Residual Networks (ResNet) and ResNeXt\n",
    "\n",
    "#### 1. Motivation: Nested Function Classes\n",
    "\n",
    "- Let $\\mathcal{F}$ be the class of functions a network architecture can represent. We seek $f^*_\\mathcal{F} = \\arg\\min_{f \\in \\mathcal{F}} L(\\mathbf{X}, \\mathbf{y}, f)$.\n",
    "- If we move to a more powerful class $\\mathcal{F}'$, there is **no guarantee** $f^*_{\\mathcal{F}'}$ is better — unless $\\mathcal{F} \\subseteq \\mathcal{F}'$ (**nested** function classes).\n",
    "- **Key insight:** If every added layer can easily learn the **identity mapping** $f(\\mathbf{x}) = \\mathbf{x}$, the new model is *at least* as good as the old one — strictly nesting the function classes.\n",
    "\n",
    "#### 2. Residual Blocks\n",
    "\n",
    "- In a standard block the network must learn $f(\\mathbf{x})$ directly. In a **residual block** the network learns the *residual* $g(\\mathbf{x}) = f(\\mathbf{x}) - \\mathbf{x}$, then computes $f(\\mathbf{x}) = g(\\mathbf{x}) + \\mathbf{x}$.\n",
    "- Learning $g(\\mathbf{x}) = 0$ (identity) is easy — just push weights/biases toward zero.\n",
    "- The direct path $\\mathbf{x} \\to$ addition is called a **residual connection** (shortcut connection).\n",
    "\n",
    "**Architecture of a residual block (PyTorch):**\n",
    "\n",
    "| Component | Details |\n",
    "|---|---|\n",
    "| Conv → BN → ReLU | $3 \\times 3$ conv, same #channels |\n",
    "| Conv → BN | $3 \\times 3$ conv, same #channels |\n",
    "| Shortcut | Identity (or $1 \\times 1$ conv if channels/resolution change) |\n",
    "| Output | ReLU( shortcut + conv output ) |\n",
    "\n",
    "- When changing #channels or halving spatial resolution (stride = 2), a **$1 \\times 1$ convolution** on the shortcut aligns dimensions.\n",
    "\n",
    "![\"resNet block\"](./Images/8/ResNetBlock.png)\n",
    "\n",
    "#### 3. ResNet Model Architecture\n",
    "\n",
    "| Stage | Layers | Output Size |\n",
    "|---|---|---|\n",
    "| **Stem (b1)** | $7 \\times 7$ Conv (64), stride 2 → BN → ReLU → $3 \\times 3$ MaxPool, stride 2 | $56 \\times 56$ |\n",
    "| **Stage 2 (b2)** | 2 residual blocks, 64 channels | $56 \\times 56$ |\n",
    "| **Stage 3 (b3)** | 2 residual blocks, 128 channels (first block stride 2) | $28 \\times 28$ |\n",
    "| **Stage 4 (b4)** | 2 residual blocks, 256 channels (first block stride 2) | $14 \\times 14$ |\n",
    "| **Stage 5 (b5)** | 2 residual blocks, 512 channels (first block stride 2) | $7 \\times 7$ |\n",
    "| **Head** | Global Average Pool → Dense (num_classes) | $1 \\times 1$ |\n",
    "\n",
    "- Each stage doubles channels and halves spatial resolution (except b2).\n",
    "- This is the **ResNet-18** configuration (2 blocks × 4 stages + stem = 18 weight layers).\n",
    "\n",
    "![\"ResNet-18\"](./Images/8/ResNet-18.png)\n",
    "\n",
    "#### 4. ResNeXt: Grouped Convolutions\n",
    "\n",
    "ResNeXt extends ResNet by replacing the inner $3 \\times 3$ convolution with **grouped convolutions**:\n",
    "\n",
    "$$\\mathbf{y} = \\sum_{i=1}^{g} \\mathbf{W}_i \\mathbf{x}_i$$\n",
    "\n",
    "- Input channels are split into $g$ **groups**, each processed independently then concatenated.\n",
    "- A $1 \\times 1$ conv first reduces channels to `bot_channels`, a grouped $3 \\times 3$ conv follows, and another $1 \\times 1$ conv expands back — a **bottleneck** design.\n",
    "- This increases the number of *paths* (cardinality) through the block while keeping parameter count manageable.\n",
    "\n",
    "#### 5. Key Takeaways\n",
    "\n",
    "- **Residual connections** solve the degradation problem — deeper networks can be trained without performance loss.\n",
    "- Learning the **residual** $g(\\mathbf{x}) = f(\\mathbf{x}) - \\mathbf{x}$ is easier than learning $f(\\mathbf{x})$ directly, especially near the identity.\n",
    "- ResNet won **ImageNet 2015** and profoundly influenced subsequent architectures (Transformers, GNNs, etc.).\n",
    "- **ResNeXt** shows that increasing *cardinality* (number of groups) is more effective than increasing depth or width alone.\n",
    "- Batch normalization + residual connections together enabled training networks with **100+ layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cafd07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "327c5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):  #@save\n",
    "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26200f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/pytorch-d2l/.venv310/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3)\n",
    "X = torch.randn(4, 3, 6, 6)\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "619a9641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(6, use_1x1conv=True, strides=2)\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a61b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(d2l.Classifier):\n",
    "    def b1(self):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f441d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(ResNet)\n",
    "def block(self, num_residuals, num_channels, first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(ResNet)\n",
    "def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = nn.Sequential(self.b1())\n",
    "    for i, b in enumerate(arch):\n",
    "        self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
    "    self.net.add_module('last', nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "        nn.LazyLinear(num_classes)))\n",
    "    self.net.apply(d2l.init_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(ResNet):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n",
    "                       lr, num_classes)\n",
    "\n",
    "ResNet18().layer_summary((1, 1, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c222684",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18(lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4fadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeXtBlock(nn.Module):  #@save\n",
    "    \"\"\"The ResNeXt block.\"\"\"\n",
    "    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,\n",
    "                 strides=1):\n",
    "        super().__init__()\n",
    "        bot_channels = int(round(num_channels * bot_mul))\n",
    "        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)\n",
    "        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,\n",
    "                                   stride=strides, padding=1,\n",
    "                                   groups=bot_channels//groups)\n",
    "        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "        self.bn3 = nn.LazyBatchNorm2d()\n",
    "        if use_1x1conv:\n",
    "            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "            self.bn4 = nn.LazyBatchNorm2d()\n",
    "        else:\n",
    "            self.conv4 = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
    "        Y = self.bn3(self.conv3(Y))\n",
    "        if self.conv4:\n",
    "            X = self.bn4(self.conv4(X))\n",
    "        return F.relu(Y + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c16bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "blk = ResNeXtBlock(32, 16, 1)\n",
    "X = torch.randn(4, 32, 96, 96)\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bce71",
   "metadata": {},
   "source": [
    "### Densely Connected Networks (DenseNet)\n",
    "\n",
    "#### 1. From ResNet to DenseNet\n",
    "\n",
    "- **ResNet** decomposes functions as $f(\\mathbf{x}) = \\mathbf{x} + g(\\mathbf{x})$ — the input is **added** to the residual.\n",
    "- **DenseNet** goes further: instead of adding, it **concatenates** the outputs of all preceding layers:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\to \\left[\\mathbf{x},\\; f_1(\\mathbf{x}),\\; f_2([\\mathbf{x}, f_1(\\mathbf{x})]),\\; f_3([\\mathbf{x}, f_1(\\mathbf{x}), f_2(\\cdot)]),\\; \\ldots \\right]\n",
    "$$\n",
    "\n",
    "- This creates **dense connections**: every layer receives feature maps from *all* previous layers, encouraging **feature reuse** and strengthening gradient flow.\n",
    "- The name \"DenseNet\" reflects the dense dependency graph between layers.\n",
    "\n",
    "#### 2. Dense Blocks\n",
    "\n",
    "Each **dense block** contains multiple convolution blocks with structure: **BN → ReLU → 3×3 Conv**.\n",
    "\n",
    "- Each conv block produces a fixed number of output channels (the **growth rate**, e.g., 10).\n",
    "- Input and output of each block are **concatenated** along the channel dimension.\n",
    "- After $n$ conv blocks with growth rate $k$ and input channels $c_0$, the output has $c_0 + n \\cdot k$ channels.\n",
    "\n",
    "```python\n",
    "def conv_block(num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, num_channels):\n",
    "        super().__init__()\n",
    "        layer = []\n",
    "        for i in range(num_convs):\n",
    "            layer.append(conv_block(num_channels))\n",
    "        self.net = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            X = torch.cat((X, Y), dim=1)  # Concatenate along channels\n",
    "        return X\n",
    "```\n",
    "\n",
    "**Example:** 2 conv blocks with growth rate 10 on 3-channel input → output has $3 + 10 + 10 = 23$ channels.\n",
    "\n",
    "#### 3. Transition Layers\n",
    "\n",
    "Since dense blocks continuously **increase** the number of channels, **transition layers** are used to reduce complexity:\n",
    "\n",
    "| Component | Purpose |\n",
    "|---|---|\n",
    "| BN → ReLU | Normalization and activation |\n",
    "| $1 \\times 1$ Conv | Reduce number of channels |\n",
    "| $2 \\times 2$ AvgPool (stride 2) | Halve spatial resolution |\n",
    "\n",
    "```python\n",
    "def transition_block(num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "        nn.LazyConv2d(num_channels, kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "```\n",
    "\n",
    "#### 4. DenseNet Model Architecture\n",
    "\n",
    "DenseNet follows a structure similar to ResNet:\n",
    "\n",
    "| Stage | Details |\n",
    "|---|---|\n",
    "| **Stem** | $7 \\times 7$ Conv (64), stride 2 → BN → ReLU → $3 \\times 3$ MaxPool, stride 2 |\n",
    "| **Body** | Alternating **Dense Blocks** and **Transition Layers** (4 dense blocks with growth rate 32; block sizes: 4, 4, 4, 4 conv layers) |\n",
    "| **Head** | BN → ReLU → Global AvgPool → Flatten → Dense (num_classes) |\n",
    "\n",
    "- Each transition layer halves both channels and spatial resolution.\n",
    "- The initial number of channels is 64; growth rate is typically 32.\n",
    "\n",
    "#### 5. Key Takeaways\n",
    "\n",
    "- **Concatenation vs. Addition:** DenseNet concatenates features from all preceding layers rather than adding residuals, preserving and reusing earlier features.\n",
    "- **Feature reuse:** Each layer has direct access to gradients and feature maps from all previous layers, which encourages feature reuse and improves gradient flow.\n",
    "- **Parameter efficiency:** Despite dense connections, DenseNet can be more parameter-efficient than ResNet because each layer produces only a narrow set of feature maps (controlled by the growth rate).\n",
    "- **Transition layers** are essential to keep the model size manageable by compressing channels between dense blocks.\n",
    "- **Growth rate** $k$ controls how much new information each layer contributes — a key hyperparameter of DenseNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06de3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6032275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "638c89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, num_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        layer = []\n",
    "        for i in range(num_convs):\n",
    "            layer.append(conv_block(num_channels))\n",
    "        self.net = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # Concatenate input and output of each block along the channels\n",
    "            X = torch.cat((X, Y), dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "268ec3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = DenseBlock(2, 10)\n",
    "X = torch.randn(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd2e77be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "        nn.LazyConv2d(num_channels, kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33b9e457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(10)\n",
    "blk(Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(d2l.Classifier):\n",
    "    def b1(self):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fcc1a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(DenseNet)\n",
    "def __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4),\n",
    "             lr=0.1, num_classes=10):\n",
    "    super(DenseNet, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = nn.Sequential(self.b1())\n",
    "    for i, num_convs in enumerate(arch):\n",
    "        self.net.add_module(f'dense_blk{i+1}', DenseBlock(num_convs,\n",
    "                                                          growth_rate))\n",
    "        # The number of output channels in the previous dense block\n",
    "        num_channels += num_convs * growth_rate\n",
    "        # A transition layer that halves the number of channels is added\n",
    "        # between the dense blocks\n",
    "        if i != len(arch) - 1:\n",
    "            num_channels //= 2\n",
    "            self.net.add_module(f'tran_blk{i+1}', transition_block(\n",
    "                num_channels))\n",
    "    self.net.add_module('last', nn.Sequential(\n",
    "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "        nn.LazyLinear(num_classes)))\n",
    "    self.net.apply(d2l.init_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e285a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet(lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2101e66",
   "metadata": {},
   "source": [
    "### Designing Convolution Network Architectures\n",
    "\n",
    "#### 1. Motivation\n",
    "\n",
    "- Previous architectures (VGG, ResNet, DenseNet, etc.) were designed largely by **intuition and trial-and-error**.\n",
    "- The goal is to develop **systematic design principles** that can guide architecture construction — moving from hand-crafted designs to a principled **design space**.\n",
    "- The key reference is the **RegNet** approach (Radosavovic et al., 2020), which progressively narrows a broad design space using empirical analysis.\n",
    "\n",
    "#### 2. The AnyNet Design Space\n",
    "\n",
    "The starting point is a generic network template with three parts:\n",
    "\n",
    "| Component | Description |\n",
    "|---|---|\n",
    "| **Stem** | Initial processing: $3 \\times 3$ Conv (stride 2) to reduce resolution |\n",
    "| **Body** | $n$ stages, each with $d_i$ residual blocks of width $w_i$, group width $g_i$, bottleneck ratio $b_i$ |\n",
    "| **Head** | Global Average Pooling → Fully Connected layer for classification |\n",
    "\n",
    "- Each stage uses **ResNeXt-style blocks** (grouped convolutions with bottleneck).\n",
    "- The first block of each stage (except the first) uses stride 2 to halve resolution.\n",
    "- This is called **AnyNet** because each stage can have independent hyperparameters $(d_i, w_i, g_i, b_i)$.\n",
    "\n",
    "#### 3. Design Principles (Narrowing the Space)\n",
    "\n",
    "By training and evaluating many random configurations, several design constraints emerge:\n",
    "\n",
    "| Constraint | Insight | Resulting Design |\n",
    "|---|---|---|\n",
    "| **Shared bottleneck ratio** $b_i = b$ | Performance is roughly the same regardless of per-stage $b_i$ — fix $b = 1$ | AnyNetA → AnyNetB |\n",
    "| **Shared group width** $g_i = g$ | Per-stage group widths don't help — share a single $g$ | AnyNetB → AnyNetC |\n",
    "| **Increasing widths** $w_i \\leq w_{i+1}$ | Networks with non-decreasing widths across stages perform better | AnyNetC → AnyNetD |\n",
    "| **Increasing depths** $d_i \\leq d_{i+1}$ | Deeper later stages tend to improve performance | AnyNetD → AnyNetE |\n",
    "\n",
    "Each constraint **reduces the search space** without sacrificing (and often improving) the distribution of good models.\n",
    "\n",
    "#### 4. RegNet: Parameterizing the Design Space\n",
    "\n",
    "The final insight is that the **log-widths** $\\log(w_i)$ of good networks follow an approximately **linear** relationship with stage index $i$:\n",
    "\n",
    "$$w_i = w_0 \\cdot w_a^i$$\n",
    "\n",
    "where:\n",
    "- $w_0$ — initial width\n",
    "- $w_a$ — width slope (multiplicative growth factor)\n",
    "\n",
    "This reduces the entire body configuration to just **6 parameters**: $w_0$, $w_a$, depth $d$, group width $g$, bottleneck ratio $b$, and the number of stages.\n",
    "\n",
    "The resulting family of networks is called **RegNet**. By quantizing the continuous widths to valid channel counts, we recover a compact, high-performing design space.\n",
    "\n",
    "#### 5. Implementation Highlights\n",
    "\n",
    "```python\n",
    "class AnyNet(d2l.Classifier):\n",
    "    def stem(self, num_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(num_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU())\n",
    "```\n",
    "\n",
    "- Each **stage** is built from `ResNeXtBlock`s with specified depth, width, group width, and bottleneck ratio.\n",
    "- The **RegNet** subclass computes per-stage widths using the linear log-width formula and quantizes them.\n",
    "\n",
    "#### 6. Key Takeaways\n",
    "\n",
    "- **Design spaces over architectures:** Instead of searching for one optimal network, define a *space* of good networks and characterize it.\n",
    "- **Simplicity wins:** Sharing hyperparameters across stages (bottleneck ratio, group width) and enforcing monotonic widths/depths loses nothing but greatly simplifies the search.\n",
    "- **Linear parameterization:** The log-widths of well-performing networks are approximately linear across stages — a powerful inductive bias captured by RegNet.\n",
    "- **Efficiency:** RegNet models achieve competitive accuracy with fewer FLOPs and faster inference than many hand-designed architectures.\n",
    "- **Generalizable methodology:** The approach of progressively constraining a design space via empirical analysis can be applied beyond CNNs to other architecture families."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
