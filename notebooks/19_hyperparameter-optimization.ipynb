{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb1ba5a",
   "metadata": {},
   "source": [
    "=========================== What Is Hyperparameter Optimization ======================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae57d3",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "This section introduces **hyperparameter optimization** as the process of selecting good values for hyperparameters in machine learning models. Hyperparameters are settings that govern model training or structure but are *not* learned from the training data itself. Unlike model parameters (weights), hyperparameters must be tuned externally.\n",
    "\n",
    "#### What Are Hyperparameters?\n",
    "\n",
    "In machine learning, we distinguish between:\n",
    "\n",
    "- **Model parameters:** learned from data via optimization (e.g., weights in neural networks)\n",
    "- **Hyperparameters:** set before training and control aspects such as:\n",
    "  - model complexity (e.g., number of layers, number of hidden units)\n",
    "  - optimization behavior (e.g., learning rate, batch size)\n",
    "  - regularization (e.g., weight decay, dropout rate)\n",
    "\n",
    "These are often denoted as $\\lambda$.\n",
    "\n",
    "#### Hyperparameter Optimization as a Nested Problem\n",
    "\n",
    "Hyperparameter optimization can be formalized as a nested optimization problem, where we aim to find the hyperparameter setting $\\lambda^*$ that minimizes some evaluation metric on held-out (validation) data:\n",
    "\n",
    "$$\n",
    "\\lambda^*\n",
    "= \\arg\\min_{\\lambda\\in\\Lambda}\n",
    "\\mathcal{L}_{\\text{val}}\\Big(\n",
    "\\big\\{\\hat{\\mathbf{y}}_j(\\lambda)\\big\\}_{j=1}^m,\\,\\big\\{\\mathbf{y}_j\\big\\}_{j=1}^m\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "- $\\Lambda$: hyperparameter search space\n",
    "- $\\hat{\\mathbf{y}}_j(\\lambda)$: model predictions with hyperparameter $\\lambda$\n",
    "- $\\mathbf{y}_j$: true validation labels\n",
    "- $\\mathcal{L}_{\\text{val}}$: validation loss\n",
    "- The model is first trained on training data for each $\\lambda$, then evaluated on validation data.\n",
    "\n",
    "This nested structure makes hyperparameter tuning computationally expensive because each evaluation of the validation loss requires training a model.\n",
    "\n",
    "#### Role of Training and Validation\n",
    "\n",
    "In practice:\n",
    "- **Training set:** used to update model parameters given a specific $\\lambda$\n",
    "- **Validation set:** used to evaluate the performance of the resulting model\n",
    "- **Test set:** used only after hyperparameter tuning to estimate generalization performance\n",
    "\n",
    "Model selection (choosing $\\lambda$) is based on performance on validation data, not training data.\n",
    "\n",
    "#### Simple Optimization Methods\n",
    "\n",
    "##### 1. Grid Search\n",
    "\n",
    "- Define a discrete grid of possible hyperparameter values.\n",
    "- Evaluate every combination by training a model and computing validation loss.\n",
    "- Choose the combination with the best validation performance.\n",
    "\n",
    "Pros:\n",
    "- Simple to implement\n",
    "\n",
    "Cons:\n",
    "- Computationally inefficient\n",
    "- Scalability issues with many hyperparameters\n",
    "\n",
    "##### 2. Random Search\n",
    "\n",
    "- Sample hyperparameter configurations randomly from a distribution or range.\n",
    "- Evaluate validation performance for each sample.\n",
    "\n",
    "Advantages over grid search:\n",
    "- More efficient coverage of high-dimensional spaces\n",
    "- Often finds good settings with fewer evaluations\n",
    "\n",
    "#### Why Random Search Works Better\n",
    "\n",
    "Empirical observation:\n",
    "- Some hyperparameters matter more than others\n",
    "- Grid search wastes evaluations on unimportant dimensions\n",
    "- Random search allocates trials more uniformly\n",
    "\n",
    "Example: tuning learning rate vs. momentum â€” sampling randomly can find effective learning rates faster than a full grid over both.\n",
    "\n",
    "#### General Workflow\n",
    "\n",
    "A typical hyperparameter optimization loop:\n",
    "\n",
    "1. **Choose a search strategy:** grid, random, Bayesian, etc.\n",
    "2. **Sample hyperparameters** from the search space\n",
    "3. **Train the model** with these hyperparameters\n",
    "4. **Evaluate on validation data**\n",
    "5. **Record results**\n",
    "6. **Select the best configuration**\n",
    "\n",
    "This loop may be parallelized to utilize multiple computational resources.\n",
    "\n",
    "#### Challenges\n",
    "\n",
    "- **High cost:** each evaluation trains a full model\n",
    "- **Search space design:** ranges and distributions matter\n",
    "- **Interactions between hyperparameters:** complex dependencies\n",
    "- **Stochastic training dynamics:** results may vary across runs\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **Hyperparameter optimization** is choosing settings that minimize validation error.\n",
    "- It is a **nested optimization**, often expensive and time-consuming.\n",
    "- **Grid search** and **random search** are basic strategies.\n",
    "- Random search scales better in higher dimensions.\n",
    "- The search strategy and definition of the search space are critical to success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe2e24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966558a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class HPOTrainer(d2l.Trainer):  #@save\n",
    "    def validation_error(self):\n",
    "        self.model.eval()\n",
    "        accuracy = 0\n",
    "        val_batch_idx = 0\n",
    "        for batch in self.val_dataloader:\n",
    "            with torch.no_grad():\n",
    "                x, y = self.prepare_batch(batch)\n",
    "                y_hat = self.model(x)\n",
    "                accuracy += self.model.accuracy(y_hat, y)\n",
    "            val_batch_idx += 1\n",
    "        return 1 -  accuracy / val_batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe9964f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def hpo_objective_softmax_classification(config, max_epochs=8):\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    trainer = d2l.HPOTrainer(max_epochs=max_epochs)\n",
    "    data = d2l.FashionMNIST(batch_size=16)\n",
    "    model = d2l.SoftmaxRegression(num_outputs=10, lr=learning_rate)\n",
    "    trainer.fit(model=model, data=data)\n",
    "    return trainer.validation_error().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796e1ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "config_space = {\"learning_rate\": stats.loguniform(1e-4, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c40f21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "errors, values = [], []\n",
    "num_iterations = 5\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    learning_rate = config_space[\"learning_rate\"].rvs()\n",
    "    print(f\"Trial {i}: learning_rate = {learning_rate}\")\n",
    "    y = hpo_objective_softmax_classification({\"learning_rate\": learning_rate})\n",
    "    print(f\"    validation_error = {y}\")\n",
    "    values.append(learning_rate)\n",
    "    errors.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6dee7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "best_idx = np.argmin(errors)\n",
    "print(f\"optimal learning rate = {values[best_idx]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
