{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af633e2",
   "metadata": {},
   "source": [
    "#### Deep Convolutional Neural Networks (AlexNet)\n",
    "\n",
    "#### Historical Context: The AI Winter Ends (2012)\n",
    "\n",
    "Although CNNs existed since LeNet (1995), they didn't dominate computer vision until **AlexNet won ImageNet 2012** by a large margin. Before this breakthrough:\n",
    "\n",
    "- **Traditional pipelines** dominated: Hand-crafted features (SIFT, SURF, HOG) → Linear classifiers\n",
    "- **Neural networks** were outperformed by kernel methods, ensemble methods, and SVMs\n",
    "- **Features were engineered**, not learned\n",
    "\n",
    "##### What Was Missing?\n",
    "\n",
    "| Missing Ingredient | Problem | Solution by 2012 |\n",
    "|-------------------|---------|------------------|\n",
    "| **Data** | Small datasets (thousands of images) | ImageNet: 1.2M images, 1000 classes |\n",
    "| **Compute** | CPUs too slow for deep networks | GPUs: 1000x speedup over CPUs |\n",
    "| **Algorithms** | Training instabilities | ReLU, Dropout, better initialization |\n",
    "\n",
    "#### Representation Learning\n",
    "\n",
    "The key insight: **features should be learned, not hand-designed**.\n",
    "\n",
    "- Traditional CV: pixels → SIFT/SURF → classifier\n",
    "- Deep Learning: pixels → learned features (hierarchical) → classifier\n",
    "\n",
    "AlexNet showed that learned features outperform hand-crafted ones when given enough data and compute.\n",
    "\n",
    "#### AlexNet Architecture (2012)\n",
    "\n",
    "AlexNet is an evolutionary improvement over LeNet with crucial differences:\n",
    "\n",
    "| Aspect | LeNet-5 (1998) | AlexNet (2012) |\n",
    "|--------|----------------|----------------|\n",
    "| **Depth** | 5 layers | 8 layers |\n",
    "| **Input size** | 28×28 | 224×224 |\n",
    "| **Activation** | Sigmoid | ReLU |\n",
    "| **Regularization** | None | Dropout (0.5) |\n",
    "| **Data augmentation** | Minimal | Extensive |\n",
    "| **Training hardware** | CPU | 2× NVIDIA GTX 580 GPUs |\n",
    "\n",
    "##### Architecture Details\n",
    "\n",
    "```\n",
    "Input: 224×224×3\n",
    "    ↓\n",
    "Conv1: 96 filters, 11×11, stride 4, ReLU → MaxPool 3×3, stride 2\n",
    "    ↓  [54×54×96 → 26×26×96]\n",
    "Conv2: 256 filters, 5×5, pad 2, ReLU → MaxPool 3×3, stride 2\n",
    "    ↓  [26×26×256 → 12×12×256]\n",
    "Conv3: 384 filters, 3×3, pad 1, ReLU\n",
    "    ↓  [12×12×384]\n",
    "Conv4: 384 filters, 3×3, pad 1, ReLU\n",
    "    ↓  [12×12×384]\n",
    "Conv5: 256 filters, 3×3, pad 1, ReLU → MaxPool 3×3, stride 2\n",
    "    ↓  [12×12×256 → 5×5×256]\n",
    "Flatten → 6400\n",
    "    ↓\n",
    "FC1: 4096, ReLU, Dropout(0.5)\n",
    "    ↓\n",
    "FC2: 4096, ReLU, Dropout(0.5)\n",
    "    ↓\n",
    "FC3: 1000 (classes)\n",
    "```\n",
    "\n",
    "#### Key Innovations\n",
    "\n",
    "##### 1. ReLU Activation\n",
    "- **Problem with Sigmoid**: Vanishing gradients when output ≈ 0 or 1\n",
    "- **ReLU solution**: $f(x) = \\max(0, x)$ — gradient is always 1 for positive inputs\n",
    "- **Benefit**: 6× faster training than tanh\n",
    "\n",
    "##### 2. Dropout Regularization\n",
    "- Randomly zero out 50% of neurons during training\n",
    "- Prevents co-adaptation of features\n",
    "- Acts like training an ensemble of networks\n",
    "\n",
    "##### 3. Data Augmentation\n",
    "- Random crops from 256×256 → 224×224\n",
    "- Horizontal flips\n",
    "- Color jittering (PCA-based)\n",
    "- Makes model robust to variations\n",
    "\n",
    "##### 4. GPU Training\n",
    "- Split model across 2 GPUs (3GB each)\n",
    "- Enabled training on large ImageNet dataset\n",
    "- Paradigm shift: Deep learning became GPU-bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269c06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet Implementation\n",
    "class AlexNet(nn.Module):\n",
    "    \"\"\"AlexNet architecture for ImageNet-scale classification.\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Conv Block 1: Large kernel to capture global patterns\n",
    "            nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Conv Blocks 3-5: Smaller 3x3 kernels\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Classifier\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6400, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Initialize weights using Xavier initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "model = AlexNet()\n",
    "model.apply(init_weights)\n",
    "print(\"AlexNet Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3395703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect layer-by-layer output shapes\n",
    "def layer_summary(model, input_shape):\n",
    "    \"\"\"Print output shape at each layer.\"\"\"\n",
    "    X = torch.randn(*input_shape)\n",
    "    print(f\"{'Layer':<15} {'Output Shape':<25} {'Params':<10}\")\n",
    "    print(\"=\" * 55)\n",
    "    for layer in model.net:\n",
    "        X = layer(X)\n",
    "        params = sum(p.numel() for p in layer.parameters()) if hasattr(layer, 'parameters') else 0\n",
    "        print(f\"{layer.__class__.__name__:<15} {str(X.shape):<25} {params:,}\")\n",
    "\n",
    "# AlexNet expects 224x224 input, but we'll use 1 channel for Fashion-MNIST\n",
    "# Note: For ImageNet, input would be (1, 3, 224, 224)\n",
    "print(\"AlexNet layer shapes for 224x224 single-channel input:\\n\")\n",
    "layer_summary(model, (1, 1, 224, 224))\n",
    "\n",
    "# Total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287c544",
   "metadata": {},
   "source": [
    "### Training AlexNet on Fashion-MNIST\n",
    "\n",
    "Since ImageNet is large (~150GB), we'll train on Fashion-MNIST resized to 224×224. This demonstrates the architecture while being computationally feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Resize Fashion-MNIST to 224x224 (AlexNet's expected input size)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Smaller batch size due to larger images\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Image shape: {train_dataset[0][0].shape}\")  # Should be (1, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbd58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(X)\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        total += X.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += X.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "model = AlexNet(num_classes=10).to(device)\n",
    "model.apply(init_weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train (fewer epochs due to computational cost)\n",
    "num_epochs = 10\n",
    "print(f\"\\n{'Epoch':<8}{'Train Loss':<12}{'Train Acc':<12}{'Test Loss':<12}{'Test Acc':<12}\")\n",
    "print(\"=\" * 56)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"{epoch+1:<8}{train_loss:<12.4f}{train_acc:<12.4f}{test_loss:<12.4f}{test_acc:<12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5461e7a6",
   "metadata": {},
   "source": [
    "### Summary: AlexNet\n",
    "\n",
    "#### Why AlexNet Matters\n",
    "\n",
    "AlexNet (2012) marked the beginning of the deep learning revolution in computer vision:\n",
    "\n",
    "| Impact | Before AlexNet | After AlexNet |\n",
    "|--------|---------------|---------------|\n",
    "| **Features** | Hand-crafted (SIFT, HOG) | Learned automatically |\n",
    "| **ImageNet accuracy** | ~74% (top-5) | 84.7% (top-5) — 10% improvement! |\n",
    "| **Dominant approach** | SVMs, ensemble methods | Deep neural networks |\n",
    "| **Hardware** | CPUs | GPUs become essential |\n",
    "\n",
    "#### Key Architectural Innovations\n",
    "\n",
    "1. **Deeper network**: 8 layers (5 conv + 3 FC) vs LeNet's 5 layers\n",
    "2. **ReLU activation**: Faster training, no vanishing gradients\n",
    "3. **Dropout**: Regularization for large FC layers\n",
    "4. **Overlapping pooling**: 3×3 windows with stride 2\n",
    "5. **Local Response Normalization** (LRN): Rarely used today, replaced by BatchNorm\n",
    "\n",
    "#### Parameter Count Breakdown\n",
    "\n",
    "| Layer Type | Parameters | % of Total |\n",
    "|------------|------------|------------|\n",
    "| Conv layers | ~2.3M | 4% |\n",
    "| FC layers | ~58.6M | 96% |\n",
    "| **Total** | ~60.9M | 100% |\n",
    "\n",
    "The FC layers dominate! This motivated later architectures (NiN, GoogLeNet) to reduce FC parameters.\n",
    "\n",
    "#### Legacy\n",
    "\n",
    "AlexNet established the modern CNN template:\n",
    "- **Conv-ReLU-Pool** blocks for feature extraction\n",
    "- **FC layers** for classification\n",
    "- **Dropout** for regularization\n",
    "- **GPU training** as standard practice\n",
    "\n",
    "This paved the way for VGG, GoogLeNet, ResNet, and all modern architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fbba2",
   "metadata": {},
   "source": [
    "## 8.2 Networks Using Blocks (VGG)\n",
    "\n",
    "### The Block Design Philosophy\n",
    "\n",
    "While AlexNet showed deep CNNs work, it didn't provide a **template** for designing new networks. VGG (Visual Geometry Group, Oxford, 2014) introduced a key insight:\n",
    "\n",
    "> **Design networks using repeating blocks of layers, not individual layers.**\n",
    "\n",
    "This mirrors chip design evolution: transistors → logic gates → logic blocks. Similarly:\n",
    "- **Early CNNs**: Design each layer individually (AlexNet)\n",
    "- **VGG onwards**: Design blocks, then stack them\n",
    "- **Modern**: Use entire pretrained models (foundation models)\n",
    "\n",
    "### Why 3×3 Convolutions?\n",
    "\n",
    "VGG's key finding: **Deep and narrow beats shallow and wide**.\n",
    "\n",
    "#### Receptive Field Equivalence\n",
    "\n",
    "Two 3×3 convolutions have the same receptive field as one 5×5 convolution:\n",
    "\n",
    "```\n",
    "Single 5×5:     Two 3×3 stacked:\n",
    "  □□□□□           □□□ → □□□\n",
    "  □□□□□           □□□   □□□\n",
    "  □□□□□           □□□   □□□\n",
    "  □□□□□\n",
    "  □□□□□\n",
    "```\n",
    "\n",
    "Both \"see\" a 5×5 region of the input!\n",
    "\n",
    "#### Parameter Comparison\n",
    "\n",
    "For $c$ input and output channels:\n",
    "\n",
    "| Configuration | Parameters | Relative |\n",
    "|---------------|------------|----------|\n",
    "| One 5×5 conv | $25c^2$ | 1.0× |\n",
    "| Two 3×3 convs | $2 \\times 9c^2 = 18c^2$ | 0.72× |\n",
    "| Three 3×3 convs (7×7 equiv) | $3 \\times 9c^2 = 27c^2$ | vs $49c^2$ |\n",
    "\n",
    "**Benefit**: More layers = more nonlinearities + fewer parameters!\n",
    "\n",
    "### VGG Block Structure\n",
    "\n",
    "A VGG block consists of:\n",
    "1. Multiple 3×3 convolutions (with padding=1 to preserve size)\n",
    "2. ReLU after each convolution\n",
    "3. 2×2 max pooling with stride 2 (halves dimensions)\n",
    "\n",
    "```\n",
    "VGG Block(num_convs, out_channels):\n",
    "    for i in range(num_convs):\n",
    "        Conv2d(3×3, padding=1) → ReLU\n",
    "    MaxPool2d(2×2, stride=2)\n",
    "```\n",
    "\n",
    "### VGG Network Variants\n",
    "\n",
    "VGG defines a **family** of networks by varying the architecture:\n",
    "\n",
    "| Model | Architecture | Conv Layers | Parameters |\n",
    "|-------|-------------|-------------|------------|\n",
    "| VGG-11 | (1,64)-(1,128)-(2,256)-(2,512)-(2,512) | 8 | 133M |\n",
    "| VGG-13 | (2,64)-(2,128)-(2,256)-(2,512)-(2,512) | 10 | 133M |\n",
    "| VGG-16 | (2,64)-(2,128)-(3,256)-(3,512)-(3,512) | 13 | 138M |\n",
    "| VGG-19 | (2,64)-(2,128)-(4,256)-(4,512)-(4,512) | 16 | 144M |\n",
    "\n",
    "Format: (num_convs, channels) per block\n",
    "\n",
    "### VGG-11 Architecture Diagram\n",
    "\n",
    "```\n",
    "Input: 224×224×3\n",
    "    ↓\n",
    "Block 1: 1×Conv(64) + Pool → 112×112×64\n",
    "    ↓\n",
    "Block 2: 1×Conv(128) + Pool → 56×56×128\n",
    "    ↓\n",
    "Block 3: 2×Conv(256) + Pool → 28×28×256\n",
    "    ↓\n",
    "Block 4: 2×Conv(512) + Pool → 14×14×512\n",
    "    ↓\n",
    "Block 5: 2×Conv(512) + Pool → 7×7×512\n",
    "    ↓\n",
    "Flatten → 25088\n",
    "    ↓\n",
    "FC(4096) → ReLU → Dropout(0.5)\n",
    "    ↓\n",
    "FC(4096) → ReLU → Dropout(0.5)\n",
    "    ↓\n",
    "FC(1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd12a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG Block: the fundamental building unit\n",
    "def vgg_block(num_convs, out_channels):\n",
    "    \"\"\"\n",
    "    VGG block: multiple 3x3 convolutions followed by max pooling.\n",
    "    \n",
    "    Args:\n",
    "        num_convs: Number of convolutional layers in the block\n",
    "        out_channels: Number of output channels\n",
    "    \n",
    "    Returns:\n",
    "        nn.Sequential containing the block\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Test a single VGG block\n",
    "block = vgg_block(num_convs=2, out_channels=128)\n",
    "X = torch.randn(1, 64, 224, 224)\n",
    "print(f\"Input shape:  {X.shape}\")\n",
    "print(f\"Output shape: {block(X).shape}\")  # Height/width halved, channels changed to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG Network: stack VGG blocks + FC classifier\n",
    "class VGG(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG Network - configurable via architecture tuple.\n",
    "    \n",
    "    Args:\n",
    "        arch: Tuple of (num_convs, out_channels) for each block\n",
    "        num_classes: Number of output classes\n",
    "    \"\"\"\n",
    "    def __init__(self, arch, num_classes=10):\n",
    "        super().__init__()\n",
    "        conv_blks = []\n",
    "        for (num_convs, out_channels) in arch:\n",
    "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            *conv_blks,\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# VGG-11 architecture: (num_convs, channels) per block\n",
    "vgg11_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
    "\n",
    "model_vgg = VGG(arch=vgg11_arch)\n",
    "print(\"VGG-11 Architecture:\")\n",
    "print(model_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect VGG-11 layer shapes\n",
    "def vgg_layer_summary(model, input_shape):\n",
    "    \"\"\"Print output shape at each block/layer.\"\"\"\n",
    "    X = torch.randn(*input_shape)\n",
    "    print(f\"{'Block/Layer':<20} {'Output Shape':<25}\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, layer in enumerate(model.net):\n",
    "        X = layer(X)\n",
    "        name = f\"Block {i+1}\" if isinstance(layer, nn.Sequential) else layer.__class__.__name__\n",
    "        print(f\"{name:<20} {str(X.shape):<25}\")\n",
    "\n",
    "print(\"VGG-11 layer shapes for 224x224 input:\\n\")\n",
    "vgg_layer_summary(model_vgg, (1, 1, 224, 224))\n",
    "\n",
    "# Total parameters (note: much larger than AlexNet!)\n",
    "total_params = sum(p.numel() for p in model_vgg.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6819e",
   "metadata": {},
   "source": [
    "### Training VGG on Fashion-MNIST\n",
    "\n",
    "VGG is more computationally demanding than AlexNet. For Fashion-MNIST, we use a **smaller version** with fewer channels to make training feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d407864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller VGG for Fashion-MNIST (reduced channels)\n",
    "small_vgg_arch = ((1, 16), (1, 32), (2, 64), (2, 128), (2, 128))\n",
    "\n",
    "model_small_vgg = VGG(arch=small_vgg_arch, num_classes=10).to(device)\n",
    "model_small_vgg.apply(init_weights)\n",
    "\n",
    "print(\"Small VGG Architecture (for Fashion-MNIST):\")\n",
    "print(f\"Architecture: {small_vgg_arch}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Count parameters\n",
    "_ = model_small_vgg(torch.randn(1, 1, 224, 224).to(device))  # Initialize lazy modules\n",
    "total_params = sum(p.numel() for p in model_small_vgg.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70920ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train small VGG on Fashion-MNIST (reuse data loaders from AlexNet section)\n",
    "optimizer_vgg = torch.optim.SGD(model_small_vgg.parameters(), lr=0.05)\n",
    "\n",
    "print(\"Training Small VGG on Fashion-MNIST:\")\n",
    "print(f\"\\n{'Epoch':<8}{'Train Loss':<12}{'Train Acc':<12}{'Test Loss':<12}{'Test Acc':<12}\")\n",
    "print(\"=\" * 56)\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_epoch(model_small_vgg, train_loader, criterion, optimizer_vgg, device)\n",
    "    test_loss, test_acc = evaluate(model_small_vgg, test_loader, criterion, device)\n",
    "    print(f\"{epoch+1:<8}{train_loss:<12.4f}{train_acc:<12.4f}{test_loss:<12.4f}{test_acc:<12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78c71d",
   "metadata": {},
   "source": [
    "### Summary: VGG\n",
    "\n",
    "#### Key Contributions\n",
    "\n",
    "1. **Block-based design**: First to use repeating blocks of layers\n",
    "2. **3×3 convolutions everywhere**: Showed deep+narrow > shallow+wide\n",
    "3. **Network families**: VGG-11, 13, 16, 19 provide speed-accuracy tradeoffs\n",
    "4. **Simple and uniform**: Easy to implement, understand, and modify\n",
    "\n",
    "#### VGG vs AlexNet\n",
    "\n",
    "| Aspect | AlexNet | VGG-16 |\n",
    "|--------|---------|--------|\n",
    "| Design | Ad-hoc layers | Repeating blocks |\n",
    "| Conv kernels | 11×11, 5×5, 3×3 | Only 3×3 |\n",
    "| Depth | 8 layers | 16 layers |\n",
    "| Parameters | ~60M | ~138M |\n",
    "| Top-5 error | 15.3% | 7.3% |\n",
    "\n",
    "#### The 3×3 Convolution Insight\n",
    "\n",
    "VGG proved that **stacking small filters is better** than using large filters:\n",
    "- Same receptive field\n",
    "- More nonlinearities (more expressive)\n",
    "- Fewer parameters\n",
    "- Faster GPU implementations\n",
    "\n",
    "This became the **standard** for all subsequent architectures (ResNet, Inception, etc.)\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "1. **Very large FC layers**: 7×7×512 = 25,088 → 4096 requires ~100M parameters\n",
    "2. **Computationally expensive**: Slow training and inference\n",
    "3. **No skip connections**: Harder to train very deep versions\n",
    "\n",
    "These limitations motivated NiN (no FC layers), GoogLeNet (inception modules), and ResNet (skip connections).\n",
    "\n",
    "#### VGG Architecture Pattern\n",
    "\n",
    "```python\n",
    "# The VGG pattern: blocks with increasing channels, decreasing spatial size\n",
    "arch = [\n",
    "    (num_convs_1, channels_1),  # Block 1: spatial / 2\n",
    "    (num_convs_2, channels_2),  # Block 2: spatial / 2\n",
    "    ...\n",
    "]\n",
    "# Channels typically: 64 → 128 → 256 → 512 → 512\n",
    "# Spatial: 224 → 112 → 56 → 28 → 14 → 7\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
