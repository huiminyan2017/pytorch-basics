{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9fcd8c",
      "metadata": {
        "id": "df9fcd8c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7290f755",
      "metadata": {
        "id": "7290f755"
      },
      "outputs": [],
      "source": [
        "### Neurons, Layers, and Models\n",
        "\n",
        "### neuron network Module: nn.Module, a module can be a single layer, a component consisting of multiple layers, or the entire model itself\n",
        "# Individual layers can be modules. Many layers can comprise a module. Many modules can comprise a module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d89dd11",
      "metadata": {
        "id": "2d89dd11",
        "outputId": "525893e0-5a58-464b-b7ee-81148595319e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\huiya\\pytorch-basics\\.venv311\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# nn.Sequential defines a special kind of Module, It maintains an ordered list of constituent Modules.\n",
        "## LazyLinear class is also a subclass of Module.\n",
        "\n",
        "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
        "\n",
        "X = torch.rand(2, 20)\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "196c7e4b",
      "metadata": {
        "id": "196c7e4b"
      },
      "outputs": [],
      "source": [
        "# A custom Module\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        # Call the constructor of the parent class nn.Module to perform\n",
        "        # the necessary initialization\n",
        "        super().__init__()\n",
        "        self.hidden = nn.LazyLinear(256)\n",
        "        self.out = nn.LazyLinear(10)\n",
        "\n",
        "    # Define the forward propagation of the model, that is, how to return the\n",
        "    # required model output based on the input X\n",
        "    def forward(self, X):\n",
        "        return self.out(F.relu(self.hidden(X)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50ea8ba8",
      "metadata": {
        "id": "50ea8ba8",
        "outputId": "9e9ceb79-1d26-486f-e138-4edddfeae5c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = MLP()\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab41b0f",
      "metadata": {
        "id": "0ab41b0f"
      },
      "outputs": [],
      "source": [
        "# The Sequential Module\n",
        "class MySequential(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        for idx, module in enumerate(args):\n",
        "            self.add_module(str(idx), module)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for module in self.children():\n",
        "            X = module(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbcbc8f5",
      "metadata": {
        "id": "dbcbc8f5",
        "outputId": "281686ec-ec78-4e38-ab40-9346df9993ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb1f238e",
      "metadata": {
        "id": "fb1f238e",
        "outputId": "27a95419-de55-4cc2-f9db-5acc9f093533"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.0664, grad_fn=<SumBackward0>)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class FixedHiddenMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Random weight parameters that will not compute gradients and\n",
        "        # therefore keep constant during training\n",
        "        self.rand_weight = torch.rand((20, 20))\n",
        "        self.linear = nn.LazyLinear(20)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.linear(X)\n",
        "\n",
        "        # A hidden layer whose weights (self.rand_weight) are initialized randomly at instantiation and are thereafter constant.\n",
        "        X = F.relu(X @ self.rand_weight + 1)\n",
        "\n",
        "        # Reuse the fully connected layer. This is equivalent to sharing parameters with two fully connected layers\n",
        "        X = self.linear(X)\n",
        "        # Control flow\n",
        "        while X.abs().sum() > 1:\n",
        "            X /= 2\n",
        "        return X.sum()\n",
        "\n",
        "net = FixedHiddenMLP()\n",
        "net(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8935f87b",
      "metadata": {
        "id": "8935f87b",
        "outputId": "a07f73ce-ffb7-4916-84d9-054313f986cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.2779, grad_fn=<SumBackward0>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can mix and match various ways of assembling modules together.\n",
        "\n",
        "class NestMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
        "                                 nn.LazyLinear(32), nn.ReLU())\n",
        "        self.linear = nn.LazyLinear(16)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.linear(self.net(X))\n",
        "\n",
        "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
        "chimera(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c541f2",
      "metadata": {
        "id": "85c541f2"
      },
      "source": [
        "========Parameter Management================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ebf4e5b",
      "metadata": {
        "id": "1ebf4e5b",
        "outputId": "135b8ec9-8b65-4c72-fbe2-170ae0e30f17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 1])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = nn.Sequential(nn.LazyLinear(8),\n",
        "                    nn.ReLU(),\n",
        "                    nn.LazyLinear(1))\n",
        "\n",
        "X = torch.rand(size=(2, 4))\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faae2c8f",
      "metadata": {
        "id": "faae2c8f",
        "outputId": "2832c4d4-d45d-4116-9510-634361db1057"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[ 0.0980, -0.0327, -0.1445,  0.1823,  0.1344, -0.1969,  0.2621, -0.3496]])),\n",
              "             ('bias', tensor([0.1680]))])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# When a model is defined via the Sequential class, we can first access any layer by indexing into the model as though it were a list.\n",
        "\n",
        "net[2].state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00da128",
      "metadata": {
        "id": "b00da128",
        "outputId": "c8442249-d12b-4167-9b6a-24d2b9653f41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.nn.parameter.Parameter, tensor([0.1680]))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# each parameter is represented as an instance of the parameter class.\n",
        "\n",
        "type(net[2].bias), net[2].bias.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3837e8",
      "metadata": {
        "id": "bb3837e8",
        "outputId": "1e24e9ff-49b2-4396-fe06-4db296bbbf32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Parameters are complex objects, containing values, gradients, and additional information. That is why we need to request the value explicitly.\n",
        "\n",
        "net[2].weight.grad == None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a523c5aa",
      "metadata": {
        "id": "a523c5aa",
        "outputId": "4205cb40-4b54-4b54-8678-5e3dfd775045"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('0.weight', torch.Size([8, 4])),\n",
              " ('0.bias', torch.Size([8])),\n",
              " ('2.weight', torch.Size([1, 8])),\n",
              " ('2.bias', torch.Size([1]))]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[(name, param.shape) for name, param in net.named_parameters()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aabf47f",
      "metadata": {
        "id": "0aabf47f",
        "outputId": "94527566-4eca-4216-86d0-0c4dd74e4fd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([True, True, True, True, True, True, True, True])\n",
            "tensor([True, True, True, True, True, True, True, True])\n"
          ]
        }
      ],
      "source": [
        "# Tied/Shared Parameters\n",
        "\n",
        "# We need to give the shared layer a name so that we can refer to its parameters\n",
        "shared = nn.LazyLinear(8)\n",
        "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
        "                    shared, nn.ReLU(),\n",
        "                    shared, nn.ReLU(),\n",
        "                    nn.LazyLinear(1))\n",
        "\n",
        "net(X)\n",
        "# Check whether the parameters are the same\n",
        "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
        "net[2].weight.data[0, 0] = 100\n",
        "# Make sure that they are actually the same object rather than just having the same value\n",
        "print(net[2].weight.data[0] == net[4].weight.data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb68318f",
      "metadata": {
        "id": "fb68318f"
      },
      "source": [
        "=====================Parameter Initialization======================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b75271d",
      "metadata": {
        "id": "9b75271d",
        "outputId": "5bce7466-248c-4649-af12-079e230ad21e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 1])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\n",
        "X = torch.rand(size=(2, 4))\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b2f7a7",
      "metadata": {
        "id": "17b2f7a7",
        "outputId": "11b3c6ef-e7e3-4ba4-a95d-65125d68f3f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([ 0.0131,  0.0030, -0.0118,  0.0016]), tensor(0.))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Built-in Initialization\n",
        "\n",
        "def init_normal(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.normal_(module.weight, mean=0, std=0.01) #  initializes all weight parameters as Gaussian random variables with standard deviation 0.01\n",
        "        nn.init.zeros_(module.bias)\n",
        "\n",
        "net.apply(init_normal)\n",
        "net[0].weight.data[0], net[0].bias.data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cdd7af9",
      "metadata": {
        "id": "0cdd7af9",
        "outputId": "2a09fd26-3cf2-4e80-b8e7-5d504667f125"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([1., 1., 1., 1.]), tensor(0.))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def init_constant(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.constant_(module.weight, 1) # given constant value\n",
        "        nn.init.zeros_(module.bias)\n",
        "\n",
        "net.apply(init_constant)\n",
        "net[0].weight.data[0], net[0].bias.data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "750ff931",
      "metadata": {
        "id": "750ff931",
        "outputId": "fb788d5d-d383-41bf-9da3-1bfb885c5f50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.0509, -0.5759, -0.5115,  0.5238])\n",
            "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
          ]
        }
      ],
      "source": [
        "def init_xavier(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.xavier_uniform_(module.weight)\n",
        "\n",
        "def init_42(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.constant_(module.weight, 42)\n",
        "\n",
        "net[0].apply(init_xavier)\n",
        "net[2].apply(init_42)\n",
        "print(net[0].weight.data[0])\n",
        "print(net[2].weight.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55fded65",
      "metadata": {
        "id": "55fded65",
        "outputId": "16777cdc-a034-40b1-e8c7-c5ac2d7604ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Init weight torch.Size([8, 4])\n",
            "Init weight torch.Size([1, 8])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[ 8.5926,  0.0000, -8.8848,  7.0531],\n",
              "        [-0.0000, -7.3913,  5.3600,  8.4965]], grad_fn=<SliceBackward0>)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Custome initialization\n",
        "\n",
        "def my_init(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        print(\"Init\", *[(name, param.shape)\n",
        "                        for name, param in module.named_parameters()][0])\n",
        "        nn.init.uniform_(module.weight, -10, 10)\n",
        "        module.weight.data *= module.weight.data.abs() >= 5\n",
        "\n",
        "net.apply(my_init)\n",
        "net[0].weight[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "794dee5a",
      "metadata": {
        "id": "794dee5a",
        "outputId": "7be1316d-8cd6-41de-fac0-cc23d061b745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Init Hello World\n"
          ]
        }
      ],
      "source": [
        "print(\"Init\", *(\"Hello\", \"World\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "817ab1ba",
      "metadata": {
        "id": "817ab1ba"
      },
      "source": [
        "==================Lazy Initialization======================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c6e6dc7",
      "metadata": {
        "id": "1c6e6dc7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06306f05",
      "metadata": {
        "id": "06306f05",
        "outputId": "13bdfe3c-a749-407b-e7ac-a09175f6f100"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<UninitializedParameter>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
        "\n",
        "# At this point, the network cannot possibly know the dimensions of the input layer’s weights because the input dimension remains unknown.\n",
        "# Consequently the framework has not yet initialized any parameters. We confirm by attempting to access the parameters below.\n",
        "\n",
        "net[0].weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f313895",
      "metadata": {
        "id": "5f313895",
        "outputId": "980e7ce8-d926-466c-b563-a75969dc8386"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([256, 20])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = torch.rand(2, 20)\n",
        "net(X)\n",
        "\n",
        "net[0].weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d484b5a3",
      "metadata": {
        "id": "d484b5a3"
      },
      "outputs": [],
      "source": [
        "# The following method passes in dummy inputs through the network for a dry run to infer all parameter shapes and subsequently initializes the\n",
        "# parameters. It will be used later when default random initializations are not desired.\n",
        "\n",
        "\n",
        "@d2l.add_to_class(d2l.Module)  #@save\n",
        "def apply_init(self, inputs, init=None):\n",
        "    self.forward(*inputs)\n",
        "    if init is not None:\n",
        "        self.net.apply(init)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f15ba8dd",
      "metadata": {
        "id": "f15ba8dd"
      },
      "source": [
        "=================Custom Layers==================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1a2b063",
      "metadata": {
        "id": "b1a2b063"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "088d4365",
      "metadata": {
        "id": "088d4365",
        "outputId": "0bb6d50f-20f9-447c-ea70-caacaff0686a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-2., -1.,  0.,  1.,  2.])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Without Parameters\n",
        "\n",
        "\n",
        "class CenteredLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X - X.mean()\n",
        "\n",
        "layer = CenteredLayer()\n",
        "layer(torch.tensor([1.0, 2, 3, 4, 5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ca71f3",
      "metadata": {
        "id": "60ca71f3",
        "outputId": "770bc854-f0e2-45e3-aa2b-f6488705c4e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\huiya\\pytorch-basics\\.venv311\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor(2.7940e-09, grad_fn=<MeanBackward0>), torch.Size([4, 128]))"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = nn.Sequential(nn.LazyLinear(128), CenteredLayer())\n",
        "\n",
        "Y = net(torch.rand(4, 8))\n",
        "Y.mean(), Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f34b7e9d",
      "metadata": {
        "id": "f34b7e9d",
        "outputId": "09bed12e-96bc-49e6-bea6-688f3601892e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.5466,  1.1332,  1.1790],\n",
              "        [-1.2105, -0.5504, -0.6399],\n",
              "        [-1.1234,  0.0551,  0.0035],\n",
              "        [ 0.2254, -1.6338,  0.2674],\n",
              "        [-0.7772, -0.6681,  1.1908]], requires_grad=True)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# With Parameters\n",
        "\n",
        "class MyLinear(nn.Module):\n",
        "    def __init__(self, in_units, units):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
        "        self.bias = nn.Parameter(torch.randn(units,))\n",
        "\n",
        "    def forward(self, X):\n",
        "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
        "        return F.relu(linear)\n",
        "\n",
        "linear = MyLinear(5, 3)\n",
        "linear.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f90455c6",
      "metadata": {
        "id": "f90455c6",
        "outputId": "8db5cedf-c5d6-4c65-d6b7-ad7d670d9359"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0487, 0.6646, 0.5094],\n",
              "        [0.0000, 0.0000, 0.0000]])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear(torch.rand(2, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42c23d7f",
      "metadata": {
        "id": "42c23d7f",
        "outputId": "66f6c390-5346-45ff-830f-458ec546ca35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0000],\n",
              "        [0.0039]])"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
        "net(torch.rand(2, 64))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd610c03",
      "metadata": {
        "id": "fd610c03"
      },
      "source": [
        "================File I/O=============="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5a32af0",
      "metadata": {
        "id": "c5a32af0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458791d6",
      "metadata": {
        "id": "458791d6"
      },
      "outputs": [],
      "source": [
        "x = torch.arange(4)\n",
        "torch.save(x, '..\\\\Data\\\\x-file')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6f3b74",
      "metadata": {
        "id": "9b6f3b74",
        "outputId": "79eb1e4d-8696-4a6f-9919-1d31eafe9257"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x2 = torch.load('..\\\\Data\\\\x-file')\n",
        "x2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ead563",
      "metadata": {
        "id": "f9ead563",
        "outputId": "06416ac7-7327-4130-e4a2-4a194339c1c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = torch.zeros(4)\n",
        "torch.save([x, y],'..\\\\Data\\\\x-file')\n",
        "x2, y2 = torch.load('..\\\\Data\\\\x-file')\n",
        "(x2, y2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fac8ca5",
      "metadata": {
        "id": "9fac8ca5",
        "outputId": "87c0041f-78f1-49d0-c1e9-b4ae947cf0ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mydict = {'x': x, 'y': y}\n",
        "torch.save(mydict, 'mydict')\n",
        "mydict2 = torch.load('mydict')\n",
        "mydict2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "451ef55b",
      "metadata": {
        "id": "451ef55b"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.LazyLinear(256)\n",
        "        self.output = nn.LazyLinear(10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.output(F.relu(self.hidden(x)))\n",
        "\n",
        "net = MLP()\n",
        "X = torch.randn(size=(2, 20))\n",
        "Y = net(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4489ff8",
      "metadata": {
        "id": "e4489ff8"
      },
      "outputs": [],
      "source": [
        "torch.save(net.state_dict(), '..\\\\Data\\\\mlp.params')\n",
        "# net.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ae9394b",
      "metadata": {
        "id": "7ae9394b",
        "outputId": "f1da3ca0-e945-46a0-fc15-c3fa619f0fa1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (hidden): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "  (output): LazyLinear(in_features=0, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# To recover the model, we instantiate a clone of the original MLP model. Instead of randomly initializing the model parameters, we read the parameters stored in the file directly.\n",
        "\n",
        "clone = MLP()\n",
        "clone.load_state_dict(torch.load('..\\\\Data\\\\mlp.params'))\n",
        "clone.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10084261",
      "metadata": {
        "id": "10084261",
        "outputId": "075f3762-9aa0-4177-9d45-c452981ed841"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[False, False, False, False, False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False, False, False, False, False]])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_clone = clone(X)\n",
        "Y_clone == Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b689eac",
      "metadata": {
        "id": "7b689eac"
      },
      "outputs": [],
      "source": [
        "X2 = torch.randn(size=(5, 20))\n",
        "clone(X2) # works!\n",
        "\n",
        "X3 = torch.randn(size=(5, 21))\n",
        "# clone(X3) # Error:  mat1 and mat2 shapes cannot be multiplied (5x21 and 20x256)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d882882",
      "metadata": {
        "id": "4d882882"
      },
      "source": [
        "=========================GPUs========================="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Below runs under Google Colab (Pythion3.12)\n",
        "\n",
        "%pip install numpy==1.26.4\n",
        "# %pip install torch torchvision\n",
        "%pip install d2l==1.0.3 --no-deps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM6DFcYVanIt",
        "outputId": "242c8367-6e72-4fa7-8961-7be747555683"
      },
      "id": "GM6DFcYVanIt",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: d2l==1.0.3 in /usr/local/lib/python3.12/dist-packages (1.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d314bffa",
      "metadata": {
        "id": "d314bffa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1e267614",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e267614",
        "outputId": "98dc11e9-04d4-4538-fd2f-f33314774038"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cpu'),\n",
              " device(type='cuda', index=0),\n",
              " device(type='cuda', index=1))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# In PyTorch, every array has a device; we often refer it as a context. So far, by default, all variables and associated computation have been assigned to the CPU. Typically, other contexts might be various GPUs.\n",
        "\n",
        "#  the cpu device means all physical CPUs and memory. This means that PyTorch’s calculations will try to use all CPU cores\n",
        "# a gpu device only represents one card and the corresponding memory. If there are multiple GPUs, we use torch.device(f'cuda:{i}') to represent the GPU (starts at 0). Also, gpu:0 and gpu are equivalent.\n",
        "def cpu():\n",
        "    \"\"\"Get the CPU device.\"\"\"\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def gpu(i=0):\n",
        "    \"\"\"Get a GPU device.\"\"\"\n",
        "    return torch.device(f'cuda:{i}')\n",
        "\n",
        "cpu(), gpu(), gpu(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_gpus():\n",
        "    \"\"\"Get the number of available GPUs.\"\"\"\n",
        "    return torch.cuda.device_count()\n",
        "\n",
        "num_gpus()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLogUs8Gc0fR",
        "outputId": "d11aaa4e-63c4-41d0-e1cb-8760a3703c36"
      },
      "id": "qLogUs8Gc0fR",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU count:\", torch.cuda.device_count())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xycUK2YLd3Jw",
        "outputId": "a646a9b9-1682-4592-c849-d11bdf958349"
      },
      "id": "xycUK2YLd3Jw",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU count: 1\n",
            "GPU name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if num_gpus() >= i + 1:\n",
        "        return gpu(i)\n",
        "    return cpu()\n",
        "\n",
        "def try_all_gpus():\n",
        "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
        "    return [gpu(i) for i in range(num_gpus())]\n",
        "\n",
        "try_gpu(), try_gpu(10), try_all_gpus()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHTEwOmRd9iQ",
        "outputId": "d893478c-5c4f-4c4c-bd55-21a1bd7cad6c"
      },
      "id": "BHTEwOmRd9iQ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cuda', index=0),\n",
              " device(type='cpu'),\n",
              " [device(type='cuda', index=0)])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# By default, tensors are created on the CPU. We can query the device where the tensor is located.\n",
        "\n",
        "x = torch.tensor([1, 2, 3])\n",
        "x.device\n",
        "\n",
        "# It is important to note that whenever we want to operate on multiple terms, they need to be on the same device. For instance, if we sum two tensors, we need to make sure that both arguments live on the same device.\n",
        "# otherwise the framework would not know where to store the result or even how to decide where to perform the computation.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjYHGxnreWs3",
        "outputId": "c56c6c04-114f-470e-f172-60df69f58d55"
      },
      "id": "ZjYHGxnreWs3",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.ones(2, 3, device=try_gpu())\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53xZhOr1g3Ds",
        "outputId": "1a2061e8-db35-4a4b-9fa1-f1de929463ef"
      },
      "id": "53xZhOr1g3Ds",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to compute X + Y, we need to decide where to perform this operation. For instance, as shown in Fig. 6.7.1, we can transfer X to the second GPU and perform the operation there. Do not simply add X and Y, since this will result in an exception. The runtime engine would not know what to do: it cannot find data on the same device and it fails. Since Y lives on the second GPU, we need to move X there before we can add the two.\n",
        "\n",
        "![img](https://github.com/huiminyan2017/pytorch-basics/blob/main/notebooks/Images/Gpu1.png?raw=1)"
      ],
      "metadata": {
        "id": "rZkEFJnBf7q1"
      },
      "id": "rZkEFJnBf7q1"
    },
    {
      "cell_type": "code",
      "source": [
        "X.cuda(0) is X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUuFegQSgfPV",
        "outputId": "43dde6da-61cf-4556-8383-e33fbf585d0d"
      },
      "id": "pUuFegQSgfPV",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "People use GPUs to do machine learning because they expect them to be fast. But transferring variables between devices is slow: much slower than computation. So we want you to be 100% certain that you want to do something slow before we let you do it. If the deep learning framework just did the copy automatically without crashing then you might not realize that you had written some slow code.\n",
        "\n",
        "Transferring data is not only slow, it also makes parallelization a lot more difficult, since we have to wait for data to be sent (or rather to be received) before we can proceed with more operations. This is why copy operations should be taken with great care. As a rule of thumb, many small operations are much worse than one big operation. Moreover, several operations at a time are much better than many single operations interspersed in the code unless you know what you are doing. This is the case since such operations can block if one device has to wait for the other before it can do something else. It is a bit like ordering your coffee in a queue rather than pre-ordering it by phone and finding out that it is ready when you are.\n",
        "\n",
        "Last, when we print tensors or convert tensors to the NumPy format, if the data is not in the main memory, the framework will copy it to the main memory first, resulting in additional transmission overhead. Even worse, it is now subject to the dreaded global interpreter lock that makes everything wait for Python to complete."
      ],
      "metadata": {
        "id": "Ip1UuLVuh1Ib"
      },
      "id": "Ip1UuLVuh1Ib"
    },
    {
      "cell_type": "code",
      "source": [
        "# puts the model parameters on the GPU.\n",
        "net = nn.Sequential(nn.LazyLinear(1))\n",
        "net = net.to(device=try_gpu())"
      ],
      "metadata": {
        "id": "0G4hO6-Mfn7X"
      },
      "id": "0G4hO6-Mfn7X",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when the input is a tensor on the GPU, the model will calculate the result on the same GPU.\n",
        "net(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCLP262eiB-j",
        "outputId": "ed08b3ec-e2d0-4108-9ed6-d916b7306fc3"
      },
      "id": "jCLP262eiB-j",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6935],\n",
              "        [-0.6935]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the model parameters are stored on the same GPU.\n",
        "net[0].weight.data.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojtXO77xiOIT",
        "outputId": "1ddf039d-b0db-49fa-c861-b349b20d32c8"
      },
      "id": "ojtXO77xiOIT",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(d2l.Trainer)\n",
        "def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
        "    self.save_hyperparameters()\n",
        "    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n",
        "\n",
        "@d2l.add_to_class(d2l.Trainer)\n",
        "def prepare_batch(self, batch):\n",
        "    if self.gpus:\n",
        "        batch = [a.to(self.gpus[0]) for a in batch]\n",
        "    return batch\n",
        "\n",
        "@d2l.add_to_class(d2l.Trainer)\n",
        "def prepare_model(self, model):\n",
        "    model.trainer = self\n",
        "    model.board.xlim = [0, self.max_epochs]\n",
        "    if self.gpus:\n",
        "        model.to(self.gpus[0])\n",
        "    self.model = model\n",
        "\n",
        "# In short, as long as all data and parameters are on the same device, we can learn models efficiently."
      ],
      "metadata": {
        "id": "01yhZrgPiTGq"
      },
      "id": "01yhZrgPiTGq",
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}