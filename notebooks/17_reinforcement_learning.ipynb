{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def3220d",
   "metadata": {},
   "source": [
    "============================================= Markov Decision Process (MDP) ============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef575e2",
   "metadata": {},
   "source": [
    "#### Markov Decision Process (MDP)\n",
    "\n",
    "This section introduces the foundation of reinforcement learning: **Markov Decision Processes (MDPs)**. MDPs are mathematical models used to describe how systems evolve over time under a sequence of decisions. They form the basis for defining RL problems and later solution methods like Value Iteration and Q-Learning. \n",
    "\n",
    "##### Definition of an MDP\n",
    "\n",
    "A **Markov Decision Process (MDP)** is a tuple: $\\text{MDP}: (\\mathcal{S}, \\mathcal{A}, T, r)$\n",
    "\n",
    "Where:\n",
    "\n",
    "- **$\\mathcal{S}$** — set of *states* the agent can be in (e.g., positions in a gridworld).  \n",
    "- **$\\mathcal{A}$** — set of *actions* the agent can take at each state.  \n",
    "- **$T: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\to [0,1]$** — *transition function* (probability distribution): $$T(s, a, s') = P(s_{t+1}=s' \\mid s_t=s, a_t=a)$$\n",
    "  This encodes how likely it is to move to state $s'$ after taking action $a$ in state $s$.\n",
    "\n",
    "  So in an MDP, even when the current state $s$ and action $a$ are fixed, the next state $s'$ is generally **not deterministic**. Instead, the environment transitions according to above probability distribution.\n",
    "- **$r: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$** — *reward function*: immediate reward received when taking action $a$ in state $s$.  \n",
    "  Larger rewards indicate more desirable actions relative to the task.\n",
    "\n",
    "##### Return and Discount Factor\n",
    "\n",
    "A **trajectory** in an MDP is a sequence generated by interacting with the environment:\n",
    "\n",
    "$$\n",
    "\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, a_2, r_2, \\dots)\n",
    "$$\n",
    "\n",
    "- **Return** of a trajectory is the sum of rewards obtained along it:\n",
    "  $R(\\tau) = r_0 + r_1 + r_2 + \\cdots$\n",
    "\n",
    "- To handle potentially infinite trajectories and emphasize near-term rewards, we introduce a **discount factor** $\\gamma < 1$:\n",
    "\n",
    "  $\n",
    "  R(\\tau) = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\cdots = \\sum_{t=0}^\\infty \\gamma^t r_t\n",
    "  $\n",
    "\n",
    "  - Smaller $\\gamma$ biases the agent toward near-term rewards.  \n",
    "  - Larger $\\gamma$ (close to 1) encourages long-term planning.\n",
    "\n",
    "The **goal in reinforcement learning** is to *find the trajectory (policy) that maximizes expected return*.\n",
    "\n",
    "##### Discussion of the Markov Assumption\n",
    "\n",
    "The **Markov property** means the next state depends *only* on the current state and action — not on any earlier history.\n",
    "\n",
    "##### Why this matters\n",
    "\n",
    "- If the system has **memory** (e.g., velocity depending on previous state), it may *not* be Markovian.\n",
    "- However, we can often *augment* the state to include necessary history (e.g., current location + velocity) so that the Markov assumption holds.\n",
    "\n",
    "This assumption simplifies modeling and algorithms, and most RL techniques rely on it. \n",
    "\n",
    "##### Summary\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "- An MDP is a formal way to represent **sequential decision making under uncertainty**.  \n",
    "- Defined by states, actions, transition probabilities, and rewards.  \n",
    "- The objective is to choose actions that maximize *expected discounted return*.  \n",
    "- Markov property simplifies modeling by assuming the future depends only on the current state and action. \n",
    "\n",
    "##### Optional: Additional Notes\n",
    "\n",
    "- MDPs are the backbone of RL algorithms like Value Iteration and Q-Learning.\n",
    "- In full RL problems, the agent often **doesn’t know** the exact transition function; it must learn optimal actions from experience. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564146a6",
   "metadata": {},
   "source": [
    "==================================== Value Iteration ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a951a",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "This section introduces **Value Iteration**, a dynamic programming method for computing the **optimal value function** in a Markov Decision Process (MDP). It is one of the foundational algorithms in reinforcement learning when the environment model (transition and reward functions) is known.\n",
    "\n",
    "#### Stochastic Policy\n",
    "\n",
    "In an MDP, a **stochastic policy** $\\pi(a|s)$ defines a probability distribution over actions $a \\in \\mathcal{A}$ for each state $s \\in \\mathcal{S}$. The goal is to find a policy that maximizes cumulative rewards.\n",
    "\n",
    "#### Value Function\n",
    "\n",
    "The **value function** $V^\\pi(s)$ estimates the expected *discounted return* starting from state $s$ when following policy $\\pi$.\n",
    "\n",
    "#### Action-Value Function\n",
    "\n",
    "The **action-value function** $Q^\\pi(s,a)$ gives the expected return starting from state $s$, taking action $a$, and thereafter following policy $\\pi$.\n",
    "\n",
    "#### Optimal Stochastic Policy\n",
    "\n",
    "An **optimal policy** $\\pi^*$ maximizes the expected return for all states — and corresponds to the **optimal value function** $V^*(s)$.\n",
    "\n",
    "#### Principle of Dynamic Programming\n",
    "\n",
    "Value iteration uses the **Bellman Optimality Equation**:\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_{a\\in \\mathcal{A}} \\left\\{r(s,a) + \\gamma \\sum_{s'\\in \\mathcal{S}} P(s'|s,a)\\,V^*(s') \\right\\}\n",
    "$$\n",
    "\n",
    "This identity expresses that the best return from $s$ equals the best immediate action value plus the discounted value of successor states. It’s derived from the **principle of dynamic programming** — optimal substructure: *the remainder of an optimal trajectory is also optimal*.\n",
    "\n",
    "#### Value Iteration Algorithm\n",
    "\n",
    "##### Core Idea\n",
    "\n",
    "Value iteration treats the Bellman Optimality Equation as a set of constraints and iteratively updates the value function until convergence.\n",
    "##### Update Rule\n",
    "\n",
    "Initialize the value function $V_0(s)$ arbitrarily (e.g., zero for all $s$). Then at iteration $k$, update:\n",
    "\n",
    "$$\n",
    "V_{k+1}(s)\n",
    "= \\max_{a\\in \\mathcal{A}}\\Big\\{r(s,a)\n",
    "+ \\gamma \\sum_{s'\\in \\mathcal{S}}P(s'\\mid s,a)\\,V_k(s')\\Big\\},\\quad \\forall s\\in \\mathcal{S}.\n",
    "$$\n",
    "\n",
    "Continue until the values converge (changes between iterations are negligible).\n",
    "\n",
    "##### Convergence\n",
    "\n",
    "As $k\\rightarrow \\infty$, the sequence $\\{V_k\\}$ converges to the **optimal value function** $V^*$ regardless of initialization: $V^*(s) = \\lim_{k\\to\\infty} V_k(s)\\quad \\forall s.$\n",
    "\n",
    "Once $V^*$ is found, the corresponding **optimal policy** $\\pi^*$ can be extracted by choosing actions that maximize the value at each state:\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_{a\\in\\mathcal{A}} \\left\\{\n",
    "r(s,a) + \\gamma \\sum_{s'}P(s'|s,a)\\,V^*(s')\n",
    "\\right\\}.\n",
    "$$\n",
    "\n",
    "#### Policy Evaluation & Improvement (Implicit)\n",
    "\n",
    "Unlike policy iteration — which separates evaluation and improvement — value iteration implicitly performs both in a single update: the value function is both evaluated under the current estimate and improved by choosing the maximizing action.\n",
    "\n",
    "#### Implementation of Value Iteration\n",
    "\n",
    "Typical implementations loop over states and actions, computing the Bellman update (often with a convergence threshold $\\theta$ to stop early). Efficient implementations store temporary values and track the largest change to detect convergence.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **Value iteration** is a dynamic programming algorithm for computing the optimal value function and policy in an MDP when the model (transition and reward functions) is fully known.  \n",
    "- It iteratively applies the Bellman Optimality update until values stabilize.  \n",
    "- It guarantees convergence to the optimal value function as the number of iterations goes to infinity.  \n",
    "- Once converged, you can derive the optimal policy by selecting actions that maximize the value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9d73a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 16, Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from d2l import torch as d2l\n",
    "\n",
    "seed = 0  # Random number generator seed\n",
    "gamma = 0.95  # Discount factor\n",
    "num_iters = 10  # Number of iterations\n",
    "random.seed(seed)  # Set the random seed to ensure results can be reproduced\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Workaround for Gym 0.26+ API change (env.seed() was removed)\n",
    "def make_frozen_lake_env(seed):\n",
    "    \"\"\"Create FrozenLake environment compatible with Gym 0.26+\"\"\"\n",
    "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    \n",
    "    # Extract environment info (same structure as d2l.make_env)\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    # Build transition and reward matrices from env.P\n",
    "    P = np.zeros((num_states, num_actions, num_states))\n",
    "    R = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    for s in range(num_states):\n",
    "        for a in range(num_actions):\n",
    "            for prob, next_s, reward, done in env.P[s][a]:\n",
    "                P[s, a, next_s] += prob\n",
    "                R[s, a] += prob * reward\n",
    "    \n",
    "    return env, P, R, num_states, num_actions\n",
    "\n",
    "# Now set up the environment\n",
    "env, P, R, num_states, num_actions = make_frozen_lake_env(seed)\n",
    "print(f\"Number of states: {num_states}, Number of actions: {num_actions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
